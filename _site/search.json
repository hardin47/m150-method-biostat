[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nEnjoy the  notes / text\nAttend class, review the  warm-up and solutions if you have any questions after completing it during class. Read the weekly  article before class every Monday.\nComplete the HW assignment ( pdf & qmd linked below),  submit the assignment via  Gradescope from  GitHub accessed on the course Canvas site. HW is due weekly on Tuesday at 11:59pm.\nHomework assignments are found at the   GitHub course organization. Due dates are provided on each assignment (roughly every other week). See the course GitHub page for more information on how to find, complete, and submit assignments.\nThere will be six in class quizzes.\nThe textbook is Practicing Statistics, by Kuiper and Sklar.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndate\nagenda\nreadings\nhomework\narticle (Mon)\nworksheets\n\n\n\n\nWeek 1  1.22.25\n• t-tests + • SLR\nKuiper & Sklar: 2   introduction   t-test   t-test as SLR\n\nEmma Benn\n WS 1 - notation\n\n\nWeek 2  1.27.25\n• SLR  • contingency analysis\nKuiper & Sklar: 2 & 6   SLR   categorical analysis\n HW 1 - due 1.28.25\n A/B testing at Netflix   Rafael Irizarry\n WS 2 - CIs   WS 3 - tech conds\n\n\nWeek 3  2.3.25\n• contingency analysis\nKuiper & Sklar: 6   types of studies   RR and CI   OR and CI\n HW 2 - due 2.4.25   Quiz 1 on 2.5.25\n what is efficacy?   efficacy   Desi Small-Rodriguez\n WS 4 - sampling distributions\n\n\nWeek 4  2.10.25\n• logistic regression\nKuiper & Sklar: 7   logistic regression   MLE\n HW 3 - due 2.11.25\n Obesity in Children Plummets   JAMA article here (see Table 6)   growth curves   Robert Santos\n WS 5 - log-linear model   WS 6 - MLE\n\n\nWeek 5  2.17.25\n• logistic regression\nKuiper & Sklar: 7   inference, logistic regression   multiple logistic regression\n HW 4 - due 2.18.25   Quiz 2 on 2.19.25\n intersectional data   Arianna Wright Rosenbluth\n WS 7 - interaction\n\n\nWeek 6  2.24.25\n• logistic regression\nKuiper & Sklar: 7   model process   cross validation   ROC\n HW 5 - due 2.25.25\n many models  (Tables 3 & 4, Fig 2)   DuBois Boman\n WS 8 - drop-in-deviance test     WS 9 - cross validation       tidymodels   Simpson's Paradox\n\n\nWeek 7  3.3.25\n• modeling\n model building\n HW 6 - due 3.4.25   Quiz 3 on 3.5.25\n health data taken offline   Jacqueline Hughes-Oliver\n WS 10 - ROC      ROC + one variable building  variable handout\n\n\nWeek 8  3.10.25\n• survival analysis\nKuiper & Sklar: 9   time to event    KM curves\n\n placebo effect (read abstract and Fig 3)   Regina Nuzzo\n WS 11 - forward selection     WS 12 - censored observations   \n\n\n3.17.25\n• Spring Break\n\n\n\n\n\n\nWeek 9  3.24.25\n• survival analysis\nKuiper & Sklar: 9   KM curves   KM CI\n HW 7 - due 3.25.25\n Alzheimer's   Adrian Coles\n  WS 13 - KM curve   WS 14 - log-rank test\n\n\nWeek 10  3.31.25\n• survival analysis\nKuiper & Sklar: 9   log rank tests    hazard functions\n HW 8 - due 4.1.25   Quiz 4 on 4.2.25\n the million veteran program   Abigail Echo-Hawk\n  WS 15 - hazard functions\n\n\nWeek 11  4.7.25\n• survival analysis\nKuiper & Sklar: 9   Cox PH model    multiple Cox PH\n HW 9 - due 4.8.25\n race-conscious medicine   Kim Sellers\nCHD & HR    WS 16 - Cox PH: linearity   WS 17 - Cox PH: interaction\n\n\nWeek 12  4.14.25\n• survival analysis\nKuiper & Sklar: 9   assessing PH    most published research\n HW 10 - due 4.15.25   Quiz 5 on 4.16.25\n most published research   science problems   abuse of power   \n WS 18 - Ionnidis\n\n\nWeek 13  4.21.25\n• multiple comparisons\n multiple comparisons    false discovery rate   interim analyses\n Project: EDA due 4.23.25\n pausing trials   Dionne Price\n WS 19 - Rejecting null hypotheses   WS 20 - FWER\n\n\nWeek 14  4.28.25\n• multiple comparisons\n multiple comparisons    false discovery rate   interim analyses\n HW 11 - due 4.29.25   Quiz 6 on 4.30.25\n missing data   Sharon Lane-Getaz\n WS 21 - interim analyses\n\n\nWeek 15  5.5.25\n• Poisson regression\n Poisson regression \n  Project: Graphic in class 5.7.25\n non-comparative trials   David Blackwell\n\n\n\nWeek 16  5.16.25\n\n\n  Project: Final due 5.16.25\n\n\n\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Clicker Q",
    "section": "",
    "text": "to go with Practicing Statistics by Kuiper & Sklar. Math 150 - Methods in Biostatistics.\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatistics, you should know at least a little bit (hopefully a lotta bit) about the following topics.\n\nHypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-squared test.1\n\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics\n\nInteraction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.2\n\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\nR / R Studio / Quarto3\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “R”?\n\n\n\n\nGit / GitHub4\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “Git”?\n\n\n\n\nWhere can I get feedback on my HW assignments / quizzes?5\n\nprof will return paper versions\non Gradescope\non Canvas\non GitHub\n\n\n\n\nWhich of the following includes talking to the remote version of GitHub?6\n\nchanging your name (updating the YAML)\ncommitting the file(s)\npushing the file(s)\nsome of the above\nall of the above\n\n\n\n\nThe Central Limit Theorem (CLT) says:7\n\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\nThe p-value is the probability:8\n\n\nthat the null hypothesis is true given the observed data.\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\nWhy do we use a t distribution (instead of a z / normal distribution) in the t-test?9\n\n\nthe technical conditions don’t hold\nthe means are quite variable\nwe like the letter t\nwe have two samples\nwe don’t know the true standard deviation parameter\n\n\n\nWhat happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?10\n\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\nthe software won’t give a p-value as output\nthe rejection region needs to be calculated in the opposite direction\nthe world blows up\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_{i=1}^n x_i?\\)11\n\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_{i=1}^n x_iy_i?\\)12\n\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\nThe regression technical conditions include:13\n\nThe Y variable is normally distributed\nThe X variable is normally distributed\nThe residuals are normally distributed\nThe slope coefficient is normally distributed\nThe intercept coefficient is normally distributed\n\n\n\n\nWe need the technical conditions to hold in order to calculate \\(b_0\\) and \\(b_1.\\)14\n\nTRUE\nFALSE\nIt depends\n\n\n\n\nWhy do we check technical conditions?15\n\nso that the inference is valid\nso that the estimates are valid\nso that the p-value is more likely to be small\nso that the confidence level is right\nfor fun\n\n\n\n\nWhen writing the regression equation, why is there a hat \\((\\ \\hat{} \\ )\\) on the response variable?16\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\nWith a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?17\n\nhappiness causes longer lives\nlonger lives cause happiness\nhappiness and longer life are correlated\nhappiness and longer life are perfectly predictive\nhappiness and longer life are unrelated\n\n\n\n\nIf there is no relationship in the population (true correlation = 0), then r = 0.18\n\nTRUE\nFALSE\n\n\n\n\nIf there is no relationship in the population (true slope \\(\\beta_1 = 0\\)), then \\(b_1=0\\).19\n\nTRUE\nFALSE\n\n\n\n\nSmaller variability around the regression line \\((\\sigma):\\)20\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nSmaller variability in the explanatory variable (SD(X) = \\(s_X):\\)21\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nA smaller sample size \\((n):\\)22\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nWe transform our variables…23\n\n… to find the highest \\(r^2\\) value.\n… when the X variable is not normally distributed.\n… to make the model easier to interpret.\n… so that the technical conditions are met.\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?24\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\n“Observed data or more extreme” is:25\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?26\n\n0\n9\n5.3\n11\n15\n\n\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?27\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?28\n\nSubstantially greater than 50%\nSubstantially less than 50%\nClose to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions, but I didn’t happen to this week.\nI can’t make the scheduled office hours / mentor sessions\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, please contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA. What is measurable?29\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?30\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA. What is measurable?31\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\nFrom the NYT, March 21, 2023, https://www.nytimes.com/2023/03/21/sports/basketball/tall-basketball-march-madness.html\n\nAmerican men who are between 6 feet and 6-2 — significantly taller than the 5-9 average — have about a five in a million chance of making the N.B.A., according to “The Sports Gene,” a 2013 book by David Epstein about the science of athletic performance. But if you hit the genetic lottery and happen to be 7 feet tall, your chances of landing in the N.B.A. are roughly one in six. (There are 38 players on active rosters who are 7 feet or taller, according to N.B.A. Advanced Stats; the average height of an N.B.A. player is 6 feet 6.5 inches.)\n\nhttps://davidepstein.com/david-epstein-the-sports-gene/\n\n\nCalcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.32\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nIt is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.33\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nWhen we select individuals based on the explanatory variable, we cannot accurately measure34\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nRelative Risk is35\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nThe odds ratio is “invariant to which variable is explanatory and which is response” means:36\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nIn finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?37\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\nBecause taking the natural log of the RR makes the distribution approximately normal.\nBecause the natural log compresses values that are bigger than 1 and spreads values that are smaller than 1.\nBecause we can get exact p-values using Fisher’s Exact Test.\n\n\n\n\nIn order to find a CI for the true OR, our steps are:38\n\n\nfind \\(\\widehat{\\ln(\\mbox{OR})}\\)\nadd \\(\\pm \\ z^* \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\\)\ntake exp of the endpoints\n\n\nbecause the sampling distribution of \\(\\widehat{\\mbox{OR}}\\) is normal\nbecause OR is typically greater than 1\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\nI know where to find: the solutions to the worksheets, the clicker questions (with solutions), and the HW solutions39\n\nTRUE\nFALSE\n\n\n\n\nAt the value \\(x = -\\beta_0 / \\beta_1\\), the probability of success is:40\n\n0\n0.5\n1\ndepends on \\(\\beta_0\\)\ndepends on \\(\\beta_1\\)\n\n\n\n\nThe logistic model gives probability of failure:41\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of success:42\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of failure:43\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nWith a logistic regression model, the relative risk of success (for a one unit increase in X) is:44\n\n\\(- \\beta_0/\\beta_1\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\na non-linear function of X (which depends on X )\n\n\n\n\nIf we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:45\n\nlinear\nlogistic\ncomplementary log-log\nlog-linear\n\n\n\n\nYou take a sample of size 4 from a binary population and get: FSFF. (failure, success, failure, failure) What is your guess for p = P(success)?46\n\n0.05\n0.15\n0.25\n0.5\n0.75\n\n\n\n\nIn a logistic regression model, the variability is given by47\n\nNormal Y given X\nBinomial Y given X\nBernoulli Y given X\nPoisson Y given X\n\n\n\n\nWhen trying to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\), we maximize the likelihood. \\[\\prod_{i=1}^n \\bigg(\\frac{e^{\\beta_0+ \\beta_1 x_i}}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{y_i}\\bigg(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{1 - y_i}\\] Take the derivative with respect to which variable(s):48\n\nX\nY\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\nMaximum likelihood estimation seeks to:49\n\nFind the data which are most likely under the model.\nFind the parameters which are most likely under the model.\nFind the parameters which make the data most likely under the model.\nFind the data which make the parameters most likely under the model.\n\n\n\n\nWe use maximum likelihood estimation because:50\n\nIt gives an principled approach for estimating the parameters.\nThe estimates are asymptotically normally distributed.\nThe estimates are always easy to compute.\nAll of the above.\nSome of the above.\n\n\n\n\nWe know that for a given data set (with MLEs of \\(b_0\\),\\(b_1\\)):51\n\n\\(L(b_0,b_1)&lt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1)&gt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\leq L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\nIn a logistic regresion if \\(H_0\\) is true, what is the probability of success?52\n\n\n\\(p_0\\)\n\\(\\frac{e^{b_0}}{1 + e^{b_0}}\\)\n\\(\\frac{e^{b_1}}{1 + e^{b_1}}\\)\n\\(e^{b_0}\\)\n\\(\\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\\)\n\n\n\nWhich is the correct logistic regression model to predict disease status based on snoring (never, occasionally, often, always): \\(X_1 = 1\\) for occasionally; \\(X_2 = 1\\) for often; \\(X_3 = 1\\) for always.53\n\n\nlogit\\((p) = \\beta_0\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 (X_1 + X_2 + X_3)\\)\n\n\n\nHow many parameters did we estimate in the HERS worksheet with the additive model?54\n\n1\n3\n4\n2757\n2761\n\n\n\n\nHow many parameters did we estimate in the HERS worksheet with the interaction model?55\n\n3\n4\n6\n7\n12\n\n\n\n\nWhat are the df for the LRT addressing whether interaction is needed in the HERS worksheet?56\n\n2\n3\n2760\n2754\n2757\n\n\n\n\n(Bird nest example) How many parameters do we estimate when considering Length as a categorical variable? (the only variable)57\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Length (as a categorical variable) belongs in the model?58\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Incubate and Color belong in the model (given Length is determined to be in the model)?59\n\n0\n1\n2\n3\n4\n\n\n\n\nAn interaction term in a multiple logistic regression model may be used when:60\n\nthe model fit is poor.\nthere is a quadratic relationship between the response and explanatory variables.\nneither one of two explanatory variables contribute significantly to the regression model.\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n\n\n\nThe interpretations of the main effects (on their own) make sense only when the interaction component is not significant.61\n\nTRUE\nFALSE\n\n\n\n\nIf the interaction is significant but the main effects aren’t:62\n\nreport on the significance of the main effects\nremove the main effects from the model\navoid talking about main effects on their own\ntest whether the main effects are significant without interaction in the model\n\n\n\n\nWith two variables of interest, what should you test first?63\n\nVariable 1.\nVariable 2.\nThe interaction between variables 1 and 2.\nNone of the above.\n\n\n\n\nConsider variable 1 is continuous and variable 2 has 4 levels. How many degrees of freedom are associated with the drop in deviance test (LRT) of their overall interaction?64\n\n1\n2\n3\n4\n5\n\n\n\n\nWhen selecting variables, it is important that65\n\nThe model predicts training data well\nThe model predicts test data well\nThe coefficients on the variables are all significant\nThe relationships between the variables make sense\n\n\n\n\nTo get a sense of the true accuracy of the model, the test data should be assessed (for accuracy)66\n\non the first model only.\non the last model only.\non every model in the process.\n\n\n\n\nIf I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on testing set, what should I look out for?67\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nIf I am picking and choosing between features of my dataset and I achieve 30% accuracy on my training set, and ~30% on testing set, what should I look out for?68\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nCross validating will guarantee that the model does not overfit.69\n\nTRUE\nFALSE\n\n\n\n\nSuppose we want to compute 10-Fold Cross-Validation error on 200 training examples. We need to compute a model error rate N1 times, and the Cross-Validation error is the average of the errors. To compute each error, we need to train a model with data of size N2, and test the model on the data of size N3. What are the numbers for N1, N2, N3?70\n\nN1 = 1, N2 = 180, N3 = 20\nN1 = 10, N2 = 180, N3 = 20\nN1 = 10, N2 = 200, N3 = 20\nN1 = 10, N2 = 200, N3 = 200\nN1 = 20, N2 = 180, N3 = 20\n\n\n\n\nYou are reviewing papers for Fancy Conference, and you see submissions with the following claims. Which ones would you consider accepting?71\n\nMy method achieves a training error lower than all previous methods!\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min test error.)\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\nMy method achieves a CV error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n\n\n\nWhich model is better (according to ROC)?72\n\npink because it goes closer to (1,1)\npink because it is closer to y=x\nblue because it is farther from y=x\nblue because it is steeper\nneither\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn ROC curve, the x-axis measures73\n\nTrue Pos Rate which we want high\nFalse Pos Rate which we want low\nTrue Neg Rate which we want high\nFalse Neg Rate which we want low\n\n\n\n\nQuiz on 11 topics (you know nothing). Your friends know topics:\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\nWho should you choose to help you answer the questions?74\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nWho do you want to choose next?75\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nIf you can pick two people, who do you pick?76\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA, B\nA, C\nA, D\nC, B\nC, D\n\n\n\n\nWhich variable should I put in first for the forward model process?77\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I put in second for the forward model process?78\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I remove first for the backward model process?79\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I remove second for the backward model process?80\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.81\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.82\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.83\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.84\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by best-subsets selection are a subset of the variables in the (k+1)-variable model identified by best-subsets selection.85\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nIn a drop-in-deviance test (LRT), the reduced model corresponds to the null hypothesis being true.86\n\nTRUE\nFALSE\n\n\n\n\nIn a drop-in-deviance test (LRT), the full model corresponds to the alternative hypothesis being true.87\n\nTRUE\nFALSE\n\n\n\n\nWith model building:88\n\nThere are many ways to find a good model.\nThere is always one right answer.\nThere is no end to the fun.\nCan we take a pure math class yet?\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins, the coefficient on number of coins is:89\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of low coins, the coefficient on number of low coins is:90\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of coins is:91\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of low coins is:92\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nIf we consider the censored times to be event times, the empirical survival curve will (on average)93\n\nunderestimate the parameter\noverestimate the parameter\nsometimes under and sometimes overestimate the parameter\n\n\n\n\nIf we remove all the censored observations, the empirical survival curve will (on average)94\n\nunderestimate the parameter\noverestimate the parameter\nsometimes under and sometimes overestimate the parameter\n\n\n\n\n\\(n_i - d_i = n_{i+1}\\) when:95\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\n\\(\\frac{(n_i - d_i)}{n_i} = 1\\) when:96\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\nProp survive &gt; 50 days, treated (turquoise line)97\n\n~0.65\n~0.35\n~0.45\nwe only know it’s bigger than red\nwe only know it’s smaller than red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaplan Meier curves (Log-Rank p-value),98\n\nblue is clearly better\nred is clearly better\ncan’t tell because they cross\ncan’t tell because the p-value is big\ncan’t tell because the p-value is small\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the log-rank test, why is it okay to consider only one cell of the 2x2 table at time \\(t_j\\)?99\n\nBecause the row totals are fixed.\nBecause the column totals are fixed.\nBecause the row and column totals are fixed.\nBecause the total number of observations is fixed.\n\n\n\n\nWhat does it mean for the log rank test to be more powerful than the Wilcoxon test?100\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n\n\n\nThe hazard at time \\(t\\) represents:101\n\nthe probability of the event\nthe instantaneous rate of the event\nthe relative risk of the event\nthe odds ratio of the event\n\n\n\n\nThe last entry in the table for the h(t) column is NA because:102\n\nthe last observation was a death\nthe last observation was censored\nthe time interval is too big\nthe time interval is too small\n\n\n\n\n\n\n\nTable 9.6 [@KuiperSklar]\n\n\n\n\n\n\nCensored observations are\\(\\ldots\\)?103\n\nMore important than non-censored ones in survival analysis\nAre assumed to be normally distributed over time\nAre assumed to have the same survival chances as uncensored observations\nAre essential to allow calculation of the Kaplan Meier plot\nAre allocated to the baseline survival curve\n\n\n\n\nSurvival Analysis: for a one unit change of an explanatory variable, the corresponding coefficient \\(e^\\beta\\) represents:104\n\nbaseline survival\nsurvival ratio\nbaseline hazard\nhazard ratio\n\n\n\n\nIn survival analysis, the closest interpretation of the value \\(e^\\beta\\) is:105\n\nodds\nprobability\ntime to event\nrelative risk\nodds ratio\n\n\n\n\nLet the event be death. If larger values of the explanatory variable are associated with higher likelihood of survival, the coefficient \\((\\beta)\\) should be 106\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nLet the event be death. If larger values of the variable are NOT associated with higher (or lower) likelihood of survival, the coefficient \\((\\beta)\\) should be107\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nBP violates the “linear HR” condition if:108\n\nthe ln ratio of the hazard curves is not linear with respect to BP\nthe ln ratio of the survival curves is not linear with respect to BP\nthe effect of BP is to increase the hazard\nthe effect of BP is to decrease the hazard\nthere is no effect due to BP\n\n\n\n\nA Cox regression analysis:109\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time.\nCan only be used when there are censored data\nAssumes that the relative hazard for a particular variable is always constant\nUses the logrank statistic to compare two survival curves\nRelies on the condition that the explanatory variables (covariates) in the model are normally distributed.\n\n\n\n\nThe effect of weight could violate PH if:110\n\npeople of different weights are in control vs treatment group\npeople tend to weigh less over time\nthe hazard function for weight is not monotonic\nthe hazard function changes as a function of weight which is also changing over time\n\n\n\n\nThe effect of treatment could violate PH if:111\n\nthe treatment has no effect\nthe treatment produces short term benefits only\nthe treatment effect interacts with a different variable, like gender\nthere is more than one treatment group\n\n\n\n\nAIC, BIC, model validation, and stepwise regression are methods for112\n\nparameter estimation\nvariable selection\n\n\n\n\nIf \\(\\alpha = 0.05\\), I would expect 5% of all hypotheses to be rejected.113\n\nTRUE\nFALSE\n\n\n\n\nPower is:114\n\nP(type I error)\nP(type II error)\n1 – P(type I error)\n1 – P(type II error)\n\n\ntype I = \\(H_0\\) true, but we reject\ntype II = \\(H_0\\) false, but we fail to reject\npower = P(rejecting when \\(H_0\\) false)\n\n\nThe p-value is115\n\nP(\\(H_0\\) is true | data)\nP(\\(H_a\\) is true | data)\nP(data | \\(H_0\\) is true)\nP(data | \\(H_a\\) is true)\n1 – P(data | \\(H_0\\) is true)\n\n\n\nRA Fisher (1929) &gt;“… An observation is judged significant, if it would rarely have been produced, in the absence of a real cause of the kind we are seeking. It is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained. He should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation.”\n\n\nFor hypothesis testing, the problem of multiple comparisons (also known as the multiple testing problem) results from the increase in ________ that occurs when statistical tests are used repeatedly.116\n\nType I errors\nType II errors\nNull hypothesis\nStatistical hypothesis testing\n\n\n\n\nIf \\(H_0\\) is true, the p-values should be distributed:117\n\nUniformly (equal prob) on 0 to 1\nUniformly on -1 to 1\nUnimodal on 0 to 1\nSkewed left on 0 to 1\nSkewed right on 0 to 1\n\n\n\n\nGiven many many tests (presumably some are null and some are “true”), a good estimate of the number of null tests is:118\n\n(# p-values &gt; 0.5) / 2\n(# p-values &gt; 0.5) * 2\n(# p-values &lt; 0.5) / 2\n(# p-values &lt; 0.5) * 2\n\n\n\n\nWhat do I do if the adjusted p-value is bigger than 1?119\n\nLeave it unadjusted\nAssign the value of the previous (“smaller”) p-value\nRound it to 1\nDivide by 2\n\n\n\n\nWith Holm’s method, what do I do if the (m+1)^th adjusted p-value is smaller than the m^th adjusted p-value?120\n\nLeave it unadjusted\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\nRound it to 1\nDivide by 2\n\n\n\n\nThe false discovery rate represents121\n\nthe proportion of true discoveries out of the total tests\nthe proportion of true discoveries out of the total discoveries\nthe ratio of the number of true discoveries divided by the number of null discoveries\nthe number of null discoveries out of the total tests\nthe number of null discoveries out of the total discoveries\n\n\n\n\nFDR and FWER differ in that122\n\nFDR is a rate and FWER is a probability\nFDR controls the rate of false positives\nFWER controls the probability of getting a false positive\nsome of the above\nall of the above\n\n\n\n\nWhich multiple comparisons adjustment gives the highest power?123\n\nBonferonni\nHolm\nBenjamini-Hochberg\nStorey (q-values)\n\n\n\n\nWhich stopping criteria is most aggressive with respect to stopping early (i.e., is most likely to stop early)?124\n\nBonferonni\nPocock\nPeto\nO’Brien-Fleming\n\n\n\n\nWhy do we want to stop early?125\n\nget out positive results sooner\nget out negative results sooner\nuse fewer observations (people)\nall of the above\ncontrol the type I error"
  },
  {
    "objectID": "clicker_study.html#footnotes",
    "href": "clicker_study.html#footnotes",
    "title": "Clicker Q",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npreferably d or e. maybe c on some of them.↩︎\nthese are the topics we will be covering. Would be nice if you have heard of them.↩︎\nwherever you are, make sure you are communicating with me when you have questions!↩︎\nwherever you are, make sure you are communicating with me when you have questions!↩︎\n\non Gradescope\n\n↩︎\n\npushing the file(s)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n↩︎\n\nwe don’t know the true standard deviation parameter\n\n↩︎\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\n\n↩︎\n\n\\(n_2\\)\n\n↩︎\n\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n↩︎\n\nThe residuals are normally distributed (which induces a., d., and e.). There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).\n\n↩︎\nFALSE. We can always minimize the sums of squares, regardless of whether or not the model is any good.↩︎\n\nso that the inference is valid (and also for fun). Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the “inference” paradigm.\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nhappiness and longer life are correlated\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\nso that the technical conditions are met.\n\n↩︎\n\nrandom allocation\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\ncase-control (they selected based on people who had died or not)\n\n↩︎\n\ncross-classification (they selected all uses of catheters)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category (tbh, we can’t measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\n\n↩︎\n\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\n\n↩︎\nThe worksheet solutions and clicker questions are on the main course website. The HW solutions are on Canvas under Files.↩︎\n\n0.5\n\n↩︎\n\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\n↩︎\n\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\n↩︎\n\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\n↩︎\n\na non-linear function of X (which depends on X )\n\n↩︎\n\nlog-linear\n\n↩︎\n\n0.25\n\n↩︎\n\nBernoulli Y given X\n\n↩︎\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n↩︎\n\nFind the parameters which make the data most likely under the model.\n\n↩︎\n\nSome of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)\n\n↩︎\n\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n↩︎\n\n\\(\\frac{e^{b_0}}{1 + e^{b_0}}\\)\n\n↩︎\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4\\)\n\n↩︎\n\n4 parameter estimates: \\(b_0, b_1, b_2, b_3\\)\n\n↩︎\n\n7 parameter estimates: \\(b_0, b_1, b_2, b_3, b_4, b_5, b_6\\)\n\n↩︎\n\n3 (7 - 4 = 3)\n\n↩︎\n\n34\n\n↩︎\n\n33 (34 - 1 = 33)\n\n↩︎\n\n2 (4 - 2 = 2)\n\n↩︎\n\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n↩︎\n\nTRUE\n\n↩︎\n\navoid talking about main effects on their own\n\n↩︎\n\nThe interaction between variables 1 and 2. (probably… although there are many schools of thought on how to build models)\n\n↩︎\n\n3 (1 * (4-1) = 3)\n\n↩︎\n\nThe model predicts test data well\n\n↩︎\n\non the last model only.\n\n↩︎\n\noverfitting\n\n↩︎\n\nunderfitting\n\n↩︎\n\nFALSE. CV reduces the effect of overfitting, but at the end of the day, you are still building a model on the dataset at hand, and it is possible that you will overfit that dataet.\n\n↩︎\n\nN1 = 10, N2 = 180, N3 = 20\n\n↩︎\n\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n↩︎\n\nblue because it is farther from the line y=x\n\n↩︎\n\nFalse Pos Rate which we want low\n\n↩︎\n\nA\n\n↩︎\n\nB\n\n↩︎\n\nC and D\n\n↩︎\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n↩︎\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n↩︎\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n↩︎\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n↩︎\n\nTRUE\n\n↩︎\n\nTRUE\n\n↩︎\n\nFALSE\n\n↩︎\n\nFALSE\n\n↩︎\n\nFALSE\n\n↩︎\n\nTRUE (the coefficient values are forced to be zero)\n\n↩︎\n\nFALSE (the null model can exist within the full model because there is flexibility in the values of the coefficients)\n\n↩︎\n\nThere are many ways to find a good model. Also, c. there is no end to the fun.\n\n↩︎\n\npositive\n\n↩︎\n\npositive\n\n↩︎\n\npositive\n\n↩︎\n\nnegative\n\n↩︎\n\nunderestimate the parameter\n\n↩︎\n\nsometimes under and sometimes overestimate the parameter. Because censoring and survival time are independent, there isn’t any reason why the censored observations would be different from the other observations. However, removing censored observations isn’t ideal because you lose information.\n\n↩︎\n\nthere is no censoring at time \\(t_i\\)\n\n↩︎\n\nthere are no deaths at time \\(t_i\\)\n\n↩︎\n\n~0.65\n\n↩︎\n\ncan’t tell because they cross (and also because d. the p-value is big)\n\n↩︎\n\nBecause the row and column totals are fixed.\n\n↩︎\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n↩︎\n\nthe instantaneous rate of the event\n\n↩︎\n\nthe last observation was censored. The reason the hazard is zero is because the width of the time interval is unknown, that is we don’t know when the last event time is.\n\n↩︎\n\nAre assumed to have the same survival chances as uncensored observations\n\n↩︎\n\nhazard ratio\n\n↩︎\n\nrelative risk\n\n↩︎\n\nnegative\n\n↩︎\n\nzero\n\n↩︎\n\nthe ln ratio of the hazard curves is not linear with respect to BP\n\n↩︎\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time. and c. Assumes that the relative hazard for a particular variable is always constant\n\n↩︎\n\nthe hazard function changes as a function of weight which is also changing over time\n\n↩︎\n\nthe treatment produces short term benefits only\n\n↩︎\n\nvariable selection\n\n↩︎\n\nFALSE, we’d expect 5% of all null hypotheses to be rejected\n\n↩︎\n\n1 – P(type II error)\n\n↩︎\n\nP(data | \\(H_0\\) is true)\n\n↩︎\n\nType I errors\n\n↩︎\n\nUniformly (equal prob) on 0 to 1\n\n↩︎\n\n(# p-values &gt; 0.5) * 2\n\n↩︎\n\nRound it to 1\n\n↩︎\n\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\n\n↩︎\n\nthe number of null discoveries out of the total discoveries\n\n↩︎\n\nall of the above\n\n↩︎\n\nStorey (q-values)\n\n↩︎\n\nPocock\n\n↩︎\n\nall of the above. We restrict our analysis to a situation of controling the type I error. However, that isn’t why we want to stop early. We want to stop early for the other reasons listed.\n\n↩︎"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Class: Mondays & Wednesday, 1:15-2:30pm\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMonday: 2:30-4:30pm\nTuesday: 9-11am\nThursday: 1:15-3pm\n\n\nTuesday 6-8pm\nEstella 2113\n\n\n\n\n\nArtwork by @allison_horst.\n\n\n\n\n\n\n\n\n\nMethods in Biostatistics is a second course in biostatistics, designed to follow either an Introduction to Statistics or Introduction to Biostatistics course. No biology background is needed, but examples and methods will be focused on those found in the life sciences. In particular, the main statistical topics covered include a logistic regression, survival analysis, and methods to ameliorate multiple comparison issues.\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!\n\n\n\n\n\nBy the end of the semester, students will be able to do the following:\n\nevaluate quantitative information with regards to clinical and biological data. We’ll be sure to keep in mind:\n\nCareful presentation of data\nConsideration of variability\nMeaningful comparisons\n\ncritically evaluate the medical literature with respect to design, analysis, and interpretation of results.\nunderstand the role of inherent variability and keep it in perspective when inferring results to a population.\ncritically evaluate medical results given in the mainstream media.\nread published studies with skepticism. Some people (in all fields!) wrongly believe that all studies published in a peer review publication must be 100% accurate and/or well designed studies. In this course, you will learn the tools to recognize, interpret, and critique statistical results in medical literature.\n\n\n\n\nIn an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates.\n\n\n\n\n\nPracticing Statistics, by Kuiper & Sklar – see Canvas for information on how to access the textbook and the relevant datasets (for HW assignments).\n\n\n\n\n\n\nDates\n\n\n\nQuizzes on: Feb 5, Feb 19, Mar 5, April 2, April 16, April 30\nProject:\nEDA due April 23\nPresentations on May 7\nFinal project due May 16\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using Quarto. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)\n\n\n\n\n\n\nThe prerequisites for this class are Introductory Statistics (Math 58 or equivalent) and completion of one semester of calculus. We rely heavily on these prerequisites, and students with no background in statistics or very light mathematics background will find themselves trying to catch up throughout the semester. You should be familiar with topics such as probability, confidence intervals, hypothesis testing, p-values, linear regression.\n\n\nHomework will be assigned from the text and due every Tuesday at 11:59pm. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\nHomework assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n[5] All problems completed with detailed solutions provided and 75% or more of the problems are fully correct. Additionally, there are no extraneous messages, warnings, or printed lists of numbers.\n[4] All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. Or all problems are completed and there are extraneous messages, warnings, or printed lists of numbers.\n[3] Close to all problems completed with less than 75% correct. OR an assignment that didn’t make it all the way to Canvas as the correctly rendered pdf.\n[2] More than half but fewer than all problems completed and &gt; 75% correct.\n[1] More than half but fewer than all problems completed and &lt; 75% correct; OR less than half of problems completed.\n[0] No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions. You will get a zero if your file is not compiled and submitted on GitHub.\n\n\n\n\nThere will be one project at the end of the semester based primarily on the survival analysis material. You will be able to work in pairs or alone. More information to come on the project.\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n30% Homework\n30% Quizzes\n30% Final Project\n10% Class Participation"
  },
  {
    "objectID": "syllabus.html#math-150-spring-2023",
    "href": "syllabus.html#math-150-spring-2023",
    "title": "syllabus",
    "section": "",
    "text": "Class: Tuesdays & Thursdays, 1:15-2:30pm\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMondays 1:30-3pm\nTuesdays 2:30-3:30pm\nWednesday 9-11am\nThursday 3-4pm\nor by appointment\n\n\nMonday 6-8pm\nWednesday 8-10pm\nEstella 2131\n\n\n\n\n\nArtwork by @allison_horst."
  },
  {
    "objectID": "syllabus.html#the-course",
    "href": "syllabus.html#the-course",
    "title": "syllabus",
    "section": "",
    "text": "Methods in Biostatistics is a second course in biostatistics, designed to follow either an Introduction to Statistics or Introduction to Biostatistics course. No biology background is needed, but examples and methods will be focused on those found in the life sciences. In particular, the main statistical topics covered include a logistic regression, survival analysis, and methods to ameliorate multiple comparison issues.\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!"
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "syllabus",
    "section": "",
    "text": "By the end of the semester, students will be able to do the following:\n\nevaluate quantitative information with regards to clinical and biological data. We’ll be sure to keep in mind:\n\nCareful presentation of data\nConsideration of variability\nMeaningful comparisons\n\ncritically evaluate the medical literature with respect to design, analysis, and interpretation of results.\nunderstand the role of inherent variability and keep it in perspective when inferring results to a population.\ncritically evaluate medical results given in the mainstream media.\nread published studies with skepticism. Some people (in all fields!) wrongly believe that all studies published in a peer review publication must be 100% accurate and/or well designed studies. In this course, you will learn the tools to recognize, interpret, and critique statistical results in medical literature."
  },
  {
    "objectID": "syllabus.html#inclusion-goals",
    "href": "syllabus.html#inclusion-goals",
    "title": "syllabus",
    "section": "",
    "text": "In an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates."
  },
  {
    "objectID": "syllabus.html#technical-details",
    "href": "syllabus.html#technical-details",
    "title": "syllabus",
    "section": "",
    "text": "Practicing Statistics, by Kuiper & Sklar – see Canvas for information on how to access the textbook and the relevant datasets (for HW assignments).\n\n\n\n\n\n\nDates\n\n\n\nQuizzes on: Feb 5, Feb 19, Mar 5, April 2, April 16, April 30\nProject:\nEDA due April 23\nPresentations on May 7\nFinal project due May 16\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using Quarto. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)"
  },
  {
    "objectID": "syllabus.html#important-features",
    "href": "syllabus.html#important-features",
    "title": "syllabus",
    "section": "",
    "text": "The prerequisites for this class are Introductory Statistics (Math 58 or equivalent) and completion of one semester of calculus. We rely heavily on these prerequisites, and students with no background in statistics or very light mathematics background will find themselves trying to catch up throughout the semester. You should be familiar with topics such as probability, confidence intervals, hypothesis testing, p-values, linear regression.\n\n\nHomework will be assigned from the text and due every Tuesday at 11:59pm. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\nHomework assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading.\n[5] All problems completed with detailed solutions provided and 75% or more of the problems are fully correct. Additionally, there are no extraneous messages, warnings, or printed lists of numbers.\n[4] All problems completed with detailed solutions and 50-75% correct; OR close to all problems completed and 75%-100% correct. Or all problems are completed and there are extraneous messages, warnings, or printed lists of numbers.\n[3] Close to all problems completed with less than 75% correct. OR an assignment that didn’t make it all the way to Canvas as the correctly rendered pdf.\n[2] More than half but fewer than all problems completed and &gt; 75% correct.\n[1] More than half but fewer than all problems completed and &lt; 75% correct; OR less than half of problems completed.\n[0] No work submitted, OR half or less than half of the problems submitted and without any detail/work shown to explain the solutions. You will get a zero if your file is not compiled and submitted on GitHub.\n\n\n\n\nThere will be one project at the end of the semester based primarily on the survival analysis material. You will be able to work in pairs or alone. More information to come on the project.\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n30% Homework\n30% Quizzes\n30% Final Project\n10% Class Participation"
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nadapted from Monica Linden, Brown University↩︎"
  },
  {
    "objectID": "handout/HW10_m150_s23.html",
    "href": "handout/HW10_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 10",
    "section": "",
    "text": "building Cox PH models\nchecking proportional hazards\ninterpreting coefficients in the Cox PH model"
  },
  {
    "objectID": "handout/HW10_m150_s23.html#important",
    "href": "handout/HW10_m150_s23.html#important",
    "title": "Math 150 - Methods in Biostatistics - Homework 10",
    "section": "Important",
    "text": "Important\nThe data are in the files tab on Canvas (in a folder called “data”).\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 9, E11\nRead text for description of data.\n\ncheck proportional hazard assumptions for treatment KM curves. Use fun=\"cloglog\" inside the ggsurvplot. (We will cover checking PH in Tuesday’s class.)\nuse all explanatory variables and likelihood ratio test to come up with the “best” model. [Note that the structure of the likelihood ratio test here is identical to the one we used in logistic regression! See: http://st47s.com/Math150/Notes/survival-analysis.html#multcoxph]\n\n\nAfter you pipe the coxph() model into glance() you will see a column called logLik.\nThe test statistics is \\(G = 2*(logLik_{bigger model} - logLik_{smaller model})\\)\nType and Partners are related and probably should not be thought of as numeric (instead they should be factor variables). That said, there are not 9 unique groups (3x3) so to use both, the six groups need to be defined:\n\n\nfruitfly &lt;- read_csv(\"https://pomona.box.com/shared/static/qnsl0sp0twdutz6azidxb5yt37boee7v\", na=\"*\") %&gt;%\n  mutate(Type_Partners = case_when(\n    Partners == 0 ~ \"0zero\",\n    Partners == 1 & Type == 0 ~ \"1P0T\",\n    Partners == 8 & Type == 0 ~ \"8P0T\",\n    Partners == 1 & Type == 1 ~ \"1P1T\",\n    Partners == 8 & Type == 1 ~ \"8P1T\"\n  ))\n\n\nThe p-value will be calculated using a chisq distribution where the degrees of freedom are the number of extra coefficients which were estimated in the bigger model.\n\n\nusing the final model, interpret each of the coefficients (in terms of hazard ratios). Don’t forget that when a model has multiple variables, the coefficient estimate will be interpreted while keeping all other variables constant.\n\n\n\nQ3. Chp 9, E12\nRead text for description of data.\n\ncheck proportional hazard assumptions for treatment KM curves. Use fun=\"cloglog\" inside the ggsurvplot. And/or use cox.zph. (We will cover checking PH in Tuesday’s class.)\nuse all explanatory variables and likelihood ratio test to come up with the “best” model.\nusing the final model, interpret each of the coefficients (in terms of hazard ratios). Don’t forget that when a model has multiple variables, the coefficient estimate will be interpreted while keeping all other variables constant.\n\n\nVAlung &lt;- read_csv(\"https://pomona.box.com/shared/static/r6hoo1gawopkt0526xvwwze5fl3245de\",\n                     na=\"*\")\n\n\npraise()\n\n[1] \"You are shining!\""
  },
  {
    "objectID": "handout/HW11_m150_s23.html",
    "href": "handout/HW11_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 10",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nunderstanding PPV\ntype I and type II errors\nadjusting p-values to control for multiple comparisons\n\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Positive Predictive Value\nUsing the Ioannidis paper, explain the details of PPV for the model with multiple researchers. That is, derive the entire PPV equation (you may need to derive most of Table 3).\n\n\nQ3. Evaluating errors part 1\nConsider the following claim (which may or may not be true):\n\nIf a null hypothesis is NOT rejected in multiple studies, then we have good evidence that the null is likely to be true.\n\nFive datasets are created to study the same phenomenon. For each dataset there is a treatment group and a control group. Do the two groups differ, on average, with respect to the continuous response variable? There are pairs of columns representing each of the 5 studies. Your task is to:\n\nAscertain whether the response variable is different across the treatment and control. Look at p-values and confidence intervals. Consider the datasets separately, and consider the datasets combined into one analysis.\nRespond to the claim above.\n\nYou might consider R code like the following:\n\nerrordata &lt;- read_delim(\"https://pomona.box.com/shared/static/acsduo30e4yzrc05mi2nnr56vj8qn3pb\", \n                        delim= \"\\t\")\nt.test(resp1~group1, data=errordata) %&gt;% tidy()\n\nresponse &lt;- c(errordata$resp1, errordata$resp2, errordata$resp3, errordata$resp4, errordata$resp5)\ngroup &lt;- c(errordata$group1, errordata$group2, errordata$group3, errordata$group4, errordata$group5)\nt.test(response~group) %&gt;% tidy()\n\nSolution:\n\n\nQ4. Evaluating errors part 2\nConsider a large randomized controlled trial designed to investigate problem drinking in Australian university students (Kypri et al., Randomized controlled trial of proactive web-based alcohol screening and brief intervention for university students., 2009). They specified 7 outcomes in advance, 3 were primary and 4 were secondary. No adjustments for multiple comparisons were made, and the p-values were reported to be 0.001, 0.02, 0.001 (primary endpoints), 0.59, 0.87, 0.22, 0.001 (secondary endpoints).\n\nAdjust the p-values using Bonferroni, Holm, and Benjamini-Hochberg. Do all 3 methods give the same conclusions with respect to significance? Explain.\nNote that the Bonferroni and Holm adjusted p-values report the smallest familywise error under which each of the tests would reject the null hypothesis. Benjamini-Hochberg report the experiment wide FDR if all tests below a critical value are rejected. Explain why (within each of the methods) some of the adjusted p-values are repeated.\nExplain how adding 100 more tests (all of whose unadjusted p-values are greater than 0.1) would change each of the adjusted p-values (and corresponding conclusions).\n\n\npraise()\n\n[1] \"You are prime!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW1_m150_s23.html",
    "href": "handout/HW1_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "",
    "text": "Practice using R to run t-tests and linear models\nProvide details about what the models mean\n\n\n\nAt the bottom of this assignment, I’ve given an R Tutorial of sorts with some of the main ideas that we’ll be using this semester. You might want to read over them and try out some of the commands yourself.\nPlease ask questions as you go along! Asking questions about the R code (and about the course in general) is your key to success.\n\nLook carefully at the line of code in the R chunk immediately below. Notice that a dataset from the textbook is being loaded in so that it can be analyzed. The datasets for the textbook are all provided to you on Canvas. You’ll need to get the data from Canvas (the file for this HW is called C2 Games1.csv), and copy it either to your own computer or to the file system on the Pomona R Server (if you are using the server). Then you need to point to the location of that dataset.\nIf it is difficult to find the location, look at the upper right side of the R Studio window for the icon that says Import Dataset. By clicking through, import the dataset into R. But wait, you aren’t done! After you import the data, you’ll see the correct path in the Console. That path must be written into the R chunk which imports the data.\nYou will know that you have successfully imported the data if you can knit this file to a pdf without any errors. After you’ve imported the data (and before you move on to the rest of the assignment), try to knit to pdf. Does it work? Great! Does it not work? Read the paragraph above again (then ask questions on Slack).\n\ngames1 &lt;- readr::read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 02/CSV Files/C2 Games1.csv\")\n\n\n\n\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\n\nUse R to calculate a two-sample test statistic (assuming equal variances) and find the p-value corresponding to this statistic. In addition, calculate a 95% confidence interval for the difference between the two means (\\(\\mu_1 - \\mu_2\\)). The end of chapter exercises will provide details on conducting this calculation by hand. If \\(H_0: \\mu_1 = \\mu_2\\) is true, the p-value states how likely that just random sampling variability would create a difference between two sample means (\\(\\overline{y}_1 - \\overline{y}_2\\)) at least as large as we observed. Based on the p-value, what can you conclude about these two types of games?\ngames1 %&gt;%\n    t.test(Time ~ Type, data=., var.equal = TRUE)\n\ngames1 %&gt;%\n    t.test(Time ~ Type, data=., var.equal = TRUE) %&gt;%\n    tidy()\n\n\n\nTo fit a linear model, the Type variable will need to be binary. Fit a linear model in R using lm() and notice which level of Type gets set to 0 and which gets set to 1. How can you tell?\nDevelop a regression model using Time as the response and the indicator on Type as the explanatory variable.\nCreate a linear model (lm()) and then tidy() the model. The following example code might help.\ngames1 %&gt;%\n  lm(Time ~ Type, data = .) %&gt;%\n  summary()\n\ngames1 %&gt;%\n  lm(Time ~ Type, data = .) %&gt;%\n  tidy()\n\n\n\nUse R to calculate the t-statistic and p-value for the hypothesis test \\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\ne 0\\). In addition, construct at 95% confidence interval for \\(\\beta_1\\). Based on these statistics, can you conclude that the coefficient \\(\\beta_1\\) is significantly different from zero? How is the test of \\(\\beta_1\\) equivalent to the t.test() in the earlier question?\nThe argument conf.int = TRUE inside tidy() on the linear model will find confidence intervals for the coefficients.\n\n\n\nAssume you are conducting a t-test to determine if there is a difference between two means. You have the following summary statistics: \\(\\overline{x}_1 = 10, \\overline{x}_2 = 20\\) and \\(s_1=s_2=10\\). Without completing the hypothesis test, explain why \\(n_1=n_2=100\\) would result in a smaller p-value than \\(n_1=n_2=16\\).\n\n\n\nIf the hypothesis test \\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\ne 0\\) results in a small p-value, can we be confident that the regression model provides a good estimate of the response value for a given value of \\(x_i\\)? Provide an explanation for your answer.\n\n\n\nWhat model technical conditions (if any) need to be satisfied in order to calculate \\(b_0\\) and \\(b_1\\) in a simple linear regression model?\n\n\n\nExplain why the model notation \\(y_i = \\beta_0 + \\beta_1 x_i\\) is not appropriate, but \\(\\hat{y}_i = b_0 + b_1 x_i\\) is appropriate.\n\n\nHW assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading."
  },
  {
    "objectID": "handout/HW1_m150_s23.html#getting-started",
    "href": "handout/HW1_m150_s23.html#getting-started",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nHere we will explore the data using functions that can be found in the tidyverse package. The data can be found in the package nycflights13.\nLet’s load the packages.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nThe data\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes available transportation data, such as the flights data we will be working with here.\nWe begin by loading the flights data frame. Run the following command by clicking on the green triangle:\n\ndata(flights)\n\nThe data set flights that shows up in your workspace is a data matrix, with each row representing an observation and each column representing a variable. R calls this data format a data frame, which is a term that will be used throughout the course. For this data set, each observation is a single flight.\nTo view the names of the variables, run the command\n\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\n\nThis returns the names of the variables in this data frame. The codebook (description of the variables) can be accessed by pulling up the help file (run this line with the green triangle and then look at the box on the bottom right of the RStudio screen):\n\n?flights\n\nOne of the variables refers to the carrier (i.e. airline) of the flight, which is coded according to the following system.\n\ncarrier: Two letter carrier abbreviation.\n\n9E: Endeavor Air Inc.\nAA: American Airlines Inc.\nAS: Alaska Airlines Inc.\nB6: JetBlue Airways\nDL: Delta Air Lines Inc.\nEV: ExpressJet Airlines Inc.\nF9: Frontier Airlines Inc.\nFL: AirTran Airways Corporation\nHA: Hawaiian Airlines Inc.\nMQ: Envoy Air\nOO: SkyWest Airlines Inc.\nUA: United Air Lines Inc.\nUS: US Airways Inc.\nVX: Virgin America\nWN: Southwest Airlines Co.\nYV: Mesa Airlines Inc.\n\n\nA very useful function for taking a quick peek at your data frame and viewing its dimensions and data types is glimpse().\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nThe flights data frame is a massive trove of information (336,776 observations!!!). Notice also that the glimpse() function let’s you know how the variables are stored: integer, double (a fancy way to say decimal number), character string, date/time, etc. Let’s think about some questions we might want to answer with these data:\n\nHow delayed were flights that were headed to Los Angeles?\nHow do departure delays vary over months?\nWhich of the three major NYC airports has a better on time percentage for departing flights?\n\n\n\nTidy Structure of Data\nFor plotting, analyses, model building, etc., the data should be structured according to certain principles.\n\nTidy Data: rows (cases/observational units) and columns (variables).\nThe key is that every row is a case and every column is a variable.\nNo exceptions.\nCreating tidy data is often not trivial.\n\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language.\nSome things to consider:\n\nobject_name &lt;- anything is a way of assigning anything to the new object_name.\nobject_name &lt;- function_name(data_frame, arguments) is a way of using a function to create a new object.\nobject_name &lt;- data_frame %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions.\nIn chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side.\n\nobject_name &lt;- data_frame %&gt;%\n                    function_name(arguments) %&gt;% \n                    another_function_name(other_arguments)\nis extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%.\n\n\nUsing the pipe to chain\nThe pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function.\nFor example:\nx %&gt;% f(y) is the same as f(x, y)\ny %&gt;% f(x, ., z) is the same as f(x,y,z)\nPipes are used commonly with functions in the dplyr package and they allow us to sequentially build data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations."
  },
  {
    "objectID": "handout/HW1_m150_s23.html#analysis",
    "href": "handout/HW1_m150_s23.html#analysis",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "Analysis",
    "text": "Analysis\n\nDeparture delays\nLet’s start by examining the distribution of departure delays of all flights using the summary function. The first item (on the left) is the data set. Subsequently, two functions are applied, (1) select() to get only the dep_delay variables, and (2) summary() to produce the numerical summary.\n\nflights %&gt;% \n  dplyr::select(dep_delay) %&gt;% \n  summary()\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\n\n\nfilter (create a smaller dataset)\nIf we want to focus only on departure delays of flights headed to Los Angeles, we need to first filter() the data for flights with that destination (dest == \"LAX\"). The departure delay for the LAX flights can then be summarized.\n\nlax_flights &lt;- flights %&gt;%\n  dplyr::filter(dest == \"LAX\")\n\nlax_flights %&gt;% \n  dplyr::select(dep_delay) %&gt;% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\nLet’s decipher these two commands (It’s common to add a break to a new line after %&gt;% to help readability).\n\nCommand 1: Take the flights data frame, filter() for flights headed to LAX, and save the result as a new data frame called lax_flights.\n\n== means “if it is equal to”. (notice that there are TWO equals signs)\nLAX is in quotation marks since it is a character string.\n\nCommand 2: Basically the same call for summarizing the departure delay.\n\nNotice that if we only want the summary of dep_delay for the LAX flights (and we don’t need to keep a copy of the dataset), we can perform the above tasks by combining them into fewer steps:\n\nflights %&gt;%\n  filter(dest == \"LAX\") %&gt;%\n  dplyr::select(dep_delay) %&gt;% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\n\nLogical operators:  Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so we use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n&gt; or &lt; means “greater than” or “less than”\n&gt;= or &lt;= means “greater than or equal to” or “less than or equal to”\n\n\n\n\nsummarize (calculate statistics)\nWe can also obtain numerical summaries for these flights:\n\nlax_flights %&gt;%\n  summarize(mean_dd = mean(dep_delay, na.rm=TRUE), \n            median_dd = median(dep_delay, na.rm=TRUE), n_dd = n())\n\n# A tibble: 1 × 3\n  mean_dd median_dd  n_dd\n    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1    9.40        -1 16174\n\n\nNote that in the summarize() function we created a list of three different numerical summaries that we were interested in. The names of these elements are user defined, like mean_dd, median_dd, n_dd, and you could customize these names as you like (just don’t use spaces in your names). Calculating these summary statistics also require that you know the function calls. Note that n() reports the sample size.\n\nSummary statistics:  Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean\nmedian\nsd\nIQR\nmin\nmax\n\nNote that each of these functions take a single vector as an argument, and returns a single value.\n\nFunctions you may not be familiar with (and that we will see in more detail in coming weeks) include: \\[\\mbox{sd} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\\] \\[\\mbox{IQR} = 75\\% - 25\\%\\]\nWe can also filter based on multiple criteria. Suppose we are interested in flights headed to San Francisco (SFO) in February:\n\nsfo_feb_flights &lt;- flights %&gt;%\n  filter(dest == \"SFO\", month == 2)\n\nNote that we can separate the conditions using commas if we want flights that are both headed to SFO and in February. If we are interested in either flights headed to SFO or in February we can use the | instead of the comma.\n\nPractice\nCreate a new data frame that includes flights headed to SFO in February, and save this data frame as sfo_feb_flights. How many flights meet these criteria?\n\nsfo_feb_flights %&gt;%\n  summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1   791\n\n\nDescribe the distribution of the arrival delays of these flights using summary and/or appropriate summary statistics.\n\nsfo_feb_flights %&gt;%\n  summarize(arrdelmean = mean(arr_delay, na.rm=TRUE), \n            arrdelsd = sd(arr_delay, na.rm=TRUE),\n            arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  arrdelmean arrdelsd arrdelmed arrdeliqr\n       &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      -9.14     31.4       -13        29\n\n\n\n\n\ngroup_by (group before summarizing)\nAnother useful technique is quickly calculating summary statistics for various groups in your data frame. For example, we can modify the above command using the group_by() function to get the same summary stats for each origin airport:\n\nsfo_feb_flights %&gt;%\n  group_by(origin) %&gt;%\n  summarize(median_dd = median(dep_delay, na.rm=TRUE), \n            iqr_dd = IQR(dep_delay, na.rm=TRUE), n_flights = n())\n\n# A tibble: 2 × 4\n  origin median_dd iqr_dd n_flights\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;     &lt;int&gt;\n1 EWR            0      9       194\n2 JFK           -2      8       597\n\n\nHere, we first grouped the data by origin, and then calculated the summary statistics.\n\nPractice\nCalculate the median and interquartile range for arr_delay of flights in in the sfo_feb_flights data frame, grouped by carrier. Which carrier has the most variable arrival delays (as measured by IQR)?\n\nsfo_feb_flights %&gt;% \n  group_by(carrier) %&gt;%\n  summarize(arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE)) %&gt;%\n  arrange(desc(arrdeliqr))\n\n# A tibble: 5 × 3\n  carrier arrdelmed arrdeliqr\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 AA             -7      35  \n2 DL            -24      27.5\n3 UA             -9      27  \n4 B6            -11      25.5\n5 VX            -20      23  \n\n\n\n\n\n\narrange() departure delays over months\n\nPractice\nWhich month would you expect to have the highest average delay departing from an NYC airport?\nLet’s think about how we would answer this question:\n\nFirst, calculate monthly averages for departure delays. With the new language we are learning, we need to\n\ngroup_by() months, then\nsummarize() mean departure delays.\n\nThen, we need to arrange() these average delays in desc()ending order\n\n\nflights %&gt;%\n  group_by(month) %&gt;%\n  summarize(mean_dd = mean(dep_delay, na.rm=TRUE)) %&gt;%\n  arrange(desc(mean_dd))\n\n# A tibble: 12 × 2\n   month mean_dd\n   &lt;int&gt;   &lt;dbl&gt;\n 1     7   21.7 \n 2     6   20.8 \n 3    12   16.6 \n 4     4   13.9 \n 5     3   13.2 \n 6     5   13.0 \n 7     8   12.6 \n 8     2   10.8 \n 9     1   10.0 \n10     9    6.72\n11    10    6.24\n12    11    5.44\n\n\n\n\nOn time departure rate for NYC airports\nSuppose you will be flying out of NYC and want to know which of the three major NYC airports has the best on time departure rate of departing flights. Suppose also that for you a flight that is delayed for less than 5 minutes is basically “on time”. You consider any flight delayed for 5 minutes of more to be “delayed”.\nIn order to determine which airport has the best on time departure rate, we need to\n\nfirst classify each flight as “on time” or “delayed”,\nthen group flights by origin airport,\nthen calculate on time departure rates for each origin airport,\nand finally arrange the airports in descending order for on time departure percentage.\n\n\n\nmutate() (create a new variable)\n\nPractice\nLet’s start with classifying each flight as “on time” or “delayed” by creating a new variable with the mutate() function.\n\nflights &lt;- flights %&gt;%\n  mutate(dep_type = ifelse(dep_delay &lt; 5, \"on time\", \"delayed\"))\n\nThe first argument in the mutate() function is the name of the new variable we want to create, in this case dep_type. Then if dep_delay &lt; 5 we classify the flight as \"on time\" and \"delayed\" if not, i.e. if the flight is delayed for 5 or more minutes.\nNote that we are also overwriting the flights data frame with the new version of this data frame that includes the new dep_type variable.\nWe can handle all the remaining steps in one code chunk:\n\nflights %&gt;%\n  group_by(origin) %&gt;%\n  summarize(ot_dep_rate = mean(dep_type == \"on time\", na.rm=TRUE)) %&gt;%\n  arrange(desc(ot_dep_rate))\n\n# A tibble: 3 × 2\n  origin ot_dep_rate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 LGA          0.728\n2 JFK          0.691\n3 EWR          0.639\n\n\n\n\nPractice\nIf you were selecting an airport (of the three NYC airports in the dataset) simply based on on time departure percentage, which NYC airport would you choose to fly out of? (How did you define “on time”? 0 min? 5 min? Something else?)\nLGA seems to have the highest on time departure percentage, so that’s the airport I would choose."
  },
  {
    "objectID": "handout/HW7_m150_s23.html",
    "href": "handout/HW7_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 7",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nbuilding multiple regression models one variable at a time\nfluent use of the multiple logistic model for prediction and for coefficient interpretation\npractice using ggplot() so that visualizations can inform the larger analysis (e.g., ROC curves)\n\nNote that if you don’t know the R code either check my notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax.\nAlso, note that you’ll need to get the data from Canvas and use it for this analysis. Look back to your own HW1 file to see the line of code you used to import the games1.csv dataset. Ask me if it isn’t obvious to you after you look at your own HW1. And just like in HW4, you’ll need to deal with the missing variables coded as “*“.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Reflecting\nMaybe this question would be better after you work on the modeling for a bit, but I didn’t want it to get lost at the end of the assignment.\n\nDo we need to CV or test / training to model fit with LRT? Said differently, does LRT keep the model from overfitting?\nDo we need CV or test / training to understand predictions? Said differently, does LRT supply independent predictions which will provide unbiased ideas of the accuracy of the model in the wild?\n\n\n\nQ3. And the Winner Is… take 3\nThe data this week come from kaggle as a compilation form Academy Award winners since the award began. https://www.kaggle.com/datasets/unanimad/the-oscar-award\nThe data wrangling below are an effort to consolidate the labels so that the variables measure the same over basic quality over the last 95 years. https://www.verdict.co.uk/oscars-90-whats-changed-years-oscars-records/\nThe dataset is in the files tab on Canvas (in a folder called “data”).\n\n# the only thing you should change in this R chunk is \n# the path of the dataset\nthe_oscar &lt;- readr::read_csv(\"~/Dropbox/teaching/MA150/the_oscar_award.csv\") %&gt;%\n  rename(year = year_ceremony) %&gt;%\n  filter(year != 2022) %&gt;%\n  select(-year_film, -ceremony, -name) %&gt;%\n  drop_na() %&gt;%\n  mutate(winner = case_when(\n    winner == TRUE ~ 1,\n    winner == FALSE ~ 0\n  )) %&gt;%\n  distinct(category, film, .keep_all = TRUE) %&gt;%\n  mutate(category = case_when(\n    category == \"OUTSTANDING PICTURE\" ~ \"BEST PICTURE\",\n    category == \"OUTSTANDING MOTION PICTURE\" ~ \"BEST PICTURE\",\n    category == \"OUTSTANDING PRODUCTION\" ~ \"BEST PICTURE\",\n    category == \"BEST MOTION PICTURE\" ~ \"BEST PICTURE\",\n    category == \"ACTOR\" ~ \"ACTOR IN A LEADING ROLE\",\n    category == \"ACTRESS\" ~ \"ACTRESS IN A LEADING ROLE\",\n    category == \"INTERNATIONAL FEATURE FILM\" ~ \"FOREIGN LANGUAGE FILM\",\n    str_detect(category, \"DIRECTING\") ~ \"DIRECTING\",\n    str_detect(category, \"WRITING\") ~ \"WRITING\",\n    str_detect(category, \"VISUAL EFFECTS\") ~ \"VISUAL EFFECTS\",\n    str_detect(category, \"SOUND\") ~ \"SOUND\",\n    str_detect(category, \"SHORT SUBJECT\") ~ \"SHORT SUBJECT\",\n    str_detect(category, \"SHORT FILM\") ~ \"SHORT FILM\",\n    str_detect(category, \"MUSIC\") ~ \"MUSIC\",\n    str_detect(category, \"MAKEUP\") ~ \"MAKEUP\",\n    str_detect(category, \"DOCUMENTARY\") ~ \"DOCUMENTARY\",\n    str_detect(category, \"COSTUME DESIGN\") ~ \"COSTUME DESIGN\",\n    str_detect(category, \"CINEMATOGRAPHY\") ~ \"CINEMATOGRAPHY\",\n    str_detect(category, \"ART DIRECTION\") ~ \"ART DIRECTION\",\n    str_detect(category, \"PRODUCTION\") ~ \"PRODUCTION\",\n    TRUE ~ category)) %&gt;%\n  filter(!(category %in% c(\"OUTSTANDING PICUTRE\",\n                         \"UNIQUE AND ARTISTIC PICTURE\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Effects)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Visual Effects)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Effects Editing)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Editing)\",\n                         \"ENGINEERING EFFECTS\",\n                         \"DANCE DIRECTION\",\n                         \"ASSISTANT DIRECTOR\"\n                         ))) %&gt;%\n  group_by(film, category) %&gt;%\n  arrange(desc(winner)) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(\"year\", \"film\"),\n              names_from = category, values_from = winner,\n              values_fill = 0) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(best_picture = as.factor(ifelse(best_picture == 1, \"win\", \"not win\"))) %&gt;%\n  select(-film)  # note that I took out the film title\n\n\nCreate a logistic regression model using all 22 explanatory variables. Which variables appear to be most significant? (Feel free to use only the glm() function without all of the tidymodels scaffolding. I took out the film title above.)\nCreate and compare a few different multiple logistic regression models. Submit the model with the fewest number of terms that best estimates the probability of winning the Best Picture award. You might start with no variables and add one at a time (see the add1() function.) Or you might start with all the variables and drop one at a time (see the `drop1() function).\n\nHere are the variables written out in a way to make model building easier for you:\nyear + actor_in_a_leading_role + actress_in_a_leading_role + art_direction + cinematography + directing + writing + sound + short_subject + film_editing + music + actor_in_a_supporting_role + actress_in_a_supporting_role + special_effects + documentary + costume_design + foreign_language_film + visual_effects + short_film + makeup + animated_feature_film + production\n\nProvide ROC curves (ideally on the same plot) for your two best models. Comment on your graph.\n\nRecall that there were two ways of model building:\n\nIf you used tidymodels then you could predict using something like: predict(my_fit, data = the_oscar, type = \"prob\")\nIf you used straight glm(), then you have to use a slightly different version of predict: predict(my_glm, data = the_oscar, type = \"response\") or you can use: augment(my_glm, data = the_oscar, type.predict = \"response\")\n\n\npraise()\n\n[1] \"You are praiseworthy!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW5_m150_s23.html",
    "href": "handout/HW5_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 5",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nfluent use of the multiple logistic model for prediction and for coefficient interpretation\nworking with variables that interact and variables which are multicollinear\npractice using ggplot() so that visualizations can inform the larger analysis\n\nNote that if you don’t know the R code either check my notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax.\nAlso, note that you’ll need to get the data from Sakai and use it for this analysis. Look back to your own HW1 file to see the line of code you used to import the games1.csv dataset. Ask me if it isn’t obvious to you after you look at your own HW1. And just like in HW4, you’ll need to deal with the missing variables coded as “*“.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 7, E1 Bird Nest study\nThe file Birdnest contains data for 99 species of North American passerine birds. Passerine are “perching birds” and include many families of familiar small birds (e.g., sparrows and warblers), as well as some larger species like crows and ravens, but do not include hawks, owls, water fowl, wading birds, and woodpeckers. One hypothesis of interest was about the relationship of body size to type of nest. Body size was measured as average length of the species. Although nests come in a variety of types (see the Nesttype variable), in this data set nest type was categorized into either closed or open. “Closed” refers to nests with only a small opening to the outside, such as the tree cavity nest of many woodpeckers or the pendant-style nest of an oriole. “Open” nests include the cup-shaped nest of the American robin. (Note: Closed? = 1 for closed nests; Closed? = 0 for open nests.)\n\nbirdnest &lt;- read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Birdnest.csv\",\n                     na=\"*\")\nglm(`Closed?` ~ Length, data=birdnest, family=\"binomial\") %&gt;% tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   0.457     0.753      0.607   0.544\n2 Length       -0.0677    0.0425    -1.59    0.112\n\n\n\nCreate a logistic regression model using bird length (Length) to estimate the probability that a bird species has a closed net type. Interpret the model in terms of the odds ratio.\nUse the Wald statistic to create a 95% confidence interval for the odds ratio. (Wald just means normal distribution, use Z. You can do it by hand using the standard output, or you can find the CI for the slope, using tidy(conf.int = TRUE), and exponentiate. You could do it both ways and check to make sure you get the same answer!)\nTest \\(H_0: \\beta_1 = 0\\) vs. \\(H_a: \\beta_1 \\ne 0\\) using both Wald’s test (from tidy() output) and the likelihood ratio test (from glance() output). State your conclusions based on these tests.\n\nFor the LRT in R, the first thing you’ll need to do is pipe the glm into glance(). That is: glm(...) %&gt;% glance(). The “deviance” value with the MLEs is called deviance, the “deviance” value with the null value of \\(\\beta_1 = 0\\) is called null.deviance. You have to use R as a calculator to subtract the values. Then use pchisq() to find the p-value. See page 223 in your book.\nskip (d), (e), (f)\n\n\nQ3. Chp 7, E9 (no (d)) Donner Party\nIn 1846, a group of 87 people (called the Donner Party) were heading west from Springfield, Illinois, for California. The leaders attempted a new route through the Sierra Nevada and were stranded there throughout the winter. The harsh weather conditions and lack of food resulted in the death of many people within the group. Social scientists have used the data to study the theory that females are better able than men to survive harsh conditions.\n\nCreate a logistic regression model using Gender and Age to estimate the probability of survival. Create a plot of the data plus the estimated probability of survival using Age as the explanatory variable and grouping the data by Gender. Use the plot and the model to interpret the coefficients in terms of the odds ratios.\n\n\ndonner &lt;- read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Donner.csv\",\n                     na=\"*\")\n\nnames(donner) &lt;- c(\"name\", \"gender\", \"age\", \"survived\", \"familysize\", \"X6\", \"X7\",\n                   \"X8\", \"X9\", \"adultname\", \"adultgender\", \"adultage\",\n                   \"adultsurvived\", \"adultfamilysize\")\n\nThe code will look something like this. Fill in the blanks. And run the code one line at a time so that you know exactly what each line is doing. [Recall: what is the difference between the output of tidy(), glance(), and augment()? That is, all are data frames. And they give output that is of dimension 1, p (p is the number of variables), and n (n is the number of observations). Which is which?]\nglm(___ ~ ___ + ___, data = ___, family=\"___\") %&gt;%\n  ___()\n  \n  \nglm(___ ~ ___ + ___, data = ___, family=\"___\") %&gt;% \n  ___(type.predict = \"response\") %&gt;%\n  arrange(age) %&gt;%\n  ggplot() +\n  geom_point(aes(x = ___, y = ___)) + \n  geom_line(aes(x = ___, y = .fitted, group = ___, color = as.factor(___)))\n\nCreate and interpret a logistic regression model using Gender, Age, and Gender*Age to estimate the probability of survival. Plot the observations, add lines representing the estimated probability of survival using Age as the explanatory variable and grouping the data by Gender. [Code from above almost identical.]\nExplain any key differences between the plots created in parts (a) and (b). Discuss how adding the interaction term Gender*Age impacts the model.\n\n\n\nQ4. Chp 7, E10 Variable Selection Techniques and Multicollinearity\nWolberg and Mangasarian developed a technique to accurate diagnose breast masses using only visual characteristics of the cells within the tumor (PNAS 1990). A sample is placed on a slide, and characteristics of the cellular nuclei within the tumor, such as size, shape, and texture are examined under a microscope to determine with the cancer cells are benign or malignant. Benign tumors are scar tissue or abnormal growths that do not spread and are typically harmless. Malignant (or invasive) cancer cells are cells that can travel, typically through the bloodstream or lymph nodes, and begin to replace normal cells in other parts of the body. If a tumor is malignant, it is essential to remove or destroy all cancerous cells in order to keep them from spreading. If a tumor is benign, surgery is not needed and the harmless tumor can remain.\n\nCrate a logistic regression model using Radius, Concave, and Radius*Radius, and Radius*Concave as explanatory variables to estimate the probability that a mass is malignant. Submit the logistic regression model and the likelihood ratio test results (testing whether any variables at all are significant), including the log-likelihood (or deviance) values and a statement of the null hypothesis. [Note that you need to create the Radius*Radius variable before running the glm.]\n\n\ncancer &lt;- read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Cancer2.csv\",\n                     na=\"*\")\ncancer &lt;- cancer %&gt;%\n  mutate(Radius2 = Radius*Radius)\n\nglm(`Malignant?` ~ Radius*Concave + Radius2, data=cancer, family=\"binomial\") %&gt;%\n  tidy()\n\n# A tibble: 5 × 5\n  term           estimate std.error statistic p.value\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)      -9.36      7.78    -1.20    0.229 \n2 Radius            0.346     3.78     0.0915  0.927 \n3 Concave           6.54      3.03     2.16    0.0310\n4 Radius2           0.351     0.465    0.757   0.449 \n5 Radius:Concave   -0.806     0.749   -1.08    0.282 \n\n\n\nEven though in part (a) Wald’s test shows the highest p-value for Radius, it is typically best to attempt to keep the simplest terms in the model. Generally, keeping simpler terms in the model makes the model easier to interpret. Thus, we suggest as a first attempt keeping Radius in the model and eliminating the variable with the next highest p-value. Create a logistic regression model using Radius, Concave, and Radius*Concave as explanatory variables to estimate the probability that a mass is malignant. Submit the logistic regression model and the likelihood ratio test results, including the log-likelihood (or deviance) values. Conduct the drop-in-deviance test (aka LRT) to determine if Radius*Radius should be included in the model, state the null hypothesis for your test. [Use glm(...) %&gt;% glance() on models with and without the squared term.]\nUse a scatterplot to compare Radius to Radius*Radius and calculate the correlation between these two terms. Are the two variables highly correlated?\nChapter 3 discusses multicollinearity (highly correlated explanatory variables). Explain whether you believe Radius is important in the logistic regression model. Why is the p-value for Radius so large in part (a) but very small\nCreate a logistic regression model using Radius and Concave as explanatory variables to estimate the probability that a mass is malignant. Submit the logistic regression model and the likelihood ratio test results, including the log-likelihood (or deviance) values. Conduct the drop-in-deviance test to determine if Radius*Concave should be included in the model, state the null hypothesis for your test.\nCreate a logistic regression model using only Concave as an explanatory variable to estimate the probability that a mass is malignant. Submit the logistic regression model and the likelihood ratio test results, including the log-likelihood (or deviance) values. Conduct the drop-in-deviance to test to determine if Radius should be included in the model.\nSubmit a final model and provide a justification for choosing that model.\n\n\npraise()\n\n[1] \"You are bee's knees!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW3_m150_s23.html",
    "href": "handout/HW3_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 3",
    "section": "",
    "text": "knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=4, fig.width=6.5, \n                      fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(praise)\n\n\nAssignment Summary (Goals)\n\nUnderstanding null hypotheses with respect to odds and proportions\nTesting via z-stat, Fisher, Chi-sq (note: if you did all for all scenarios, you’d almost always end up with the same conclusion each time!)\nMaking conclusions about different study types\n(Decided against the Chi Square test, you are not responsible for it.)\n\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 6, A23\nShow that the null hypothesis \\(H_0: p_1 = p_2\\) is mathematically equivalent to the null hypothesis \\(H_0: \\theta_1 / \\theta_2 = 1\\) where \\(p\\) represents the proportion successful and \\(\\theta\\) represents the odds of success for any two groups (labeled 1 and 2).\n\n\nQ3. Chp 6, E7 Cancer Cells: Testing for Homogeneity of Odds\nUse the data from Table 6.1 and define a benign cell as a success. Conduct a hypothesis test for the homogeneity of odds.\n\nEnter the data, tabulate it, and create a barplot (see the R code in the class notes).\nState the null and alternative hypotheses.\nCalculate the odds ratio and the test statistic (the Z statistic!). See pgs 191-192 in your book.\nProvide the p-value and state your conclusions within the context of the study.\n\n\n\nQ4. Chp 6, E12 The Pill Scare: understanding relative risk reduction\nIn October 1995, the United Kingdom Committee on Safety of Medicines (CSM) issued a warning to 190,000 general practitioners, pharmacists, and directors of public health about oral contraceptive pills containing gestodene or desogestrel. The warning, based on three unpublished epidemiological research studies, stated &gt; “It is well known that the pill may rarely produce thrombosis (blood clots) involving veins of the legs. New evidence has become available indicating that the chance of thrombosis occurring in a vein increases about two-fold for some types of pills compared to others.”\nTable 6.15 provides data from one of the studies.\nSince the occurrence of venous thrombosis is very rare (1 in 7000 for people using the second generation pill), 259 subjects were selected who had thrombosis and 651 similar subjects (from hospitals and community) who did not have thrombosis. Then these subjects were classified by the type of contraceptive they used.\n\nWas either the explanatory (row) or the response (column) variable fixed before the study was conducted?\nIs this an example of an experiment or an observational study?\nIs this a cross-classification, cohort, or case-control study?\nCreate a segmented bar chart for the data.\nUse a two-sided hypothesis and Fisher’s exact test to determine if the type of contraceptive impacts the likelihood of thrombosis. Do you expect the researchers took care to collect a simple random sample of subjects? What conclusions can be drawn?\n\nThe warning contained no numerical information other than the fact that the chance of blood clots was likely to double when birth control pills contained gestodene or desogestrel. This warning was widely publicized throughout the press, and evidence suggests that, as a result of this warning, many women ceased contraception altogether. Evidence shows a strong association between the warning and an increase in the number of unintended pregnancies and abortions (especially in women younger than 20 years old). This resulted in an estimated increase in cost of \\(\\textsterling\\) 21 million for maternity care and \\(\\textsterling\\) 4 to \\(\\textsterling\\) 6 million for abortion provision.\n\nRemember that the actual occurrence of venous thrombosis is only 1 in 7000.\nIf third generation pills double the chances of venous thrombosis, the likelihood of occurrence is still only 2 in 7000. Explain the difference between absolute risk reduction and relative risk reduction in this study.\nDeath from venous thrombosis related to third generation pills is estimated to be 1 in 11 million, much lower than the probability of death resulting from pregnancy. In 2005, the lifetime risk of maternal death in developed countries was 1 in 7300. The CSM warning did suggest that patients see a doctor before altering their contraceptives; however, it appears that many women simply stopped taking any contraceptives. Write a brief statement (just 1-2 sentences!) to the press, general practitioners, pharmacists, and directors of public health about this study.\n\n\npraise()\n\n[1] \"You are wonderful!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/variables.html",
    "href": "handout/variables.html",
    "title": "Variables: one at a time",
    "section": "",
    "text": "More metrics\n\nAIC: Akaike’s Information Criteria = \\(-2\\ln L + 2p\\)\nBIC: Bayesian Information Criteria = \\(-2 \\ln L + p \\ln(n)\\)\n\n\nchoose a model with the smallest AIC or BIC (i.e., the biggest likelihood)\n\n\n\nVariable Selection\n\nForward: start with the “best” variable and add variables one at a time\n\n\nBackward: start with the full model and remove variables one at a time\n\n\nBest: find the “best” combo of variables (check them all!) for a specified number of variables\n\n\n\nTools\n\ntidymodels does not make it easy to add or drop 1 variable at a time.\nadd1() and drop1() functions do not make it easy to work with dozens of predictors and missing data.\n\nTherefore, we’ll go back to the bird data from HW 5.\n\nnests %&gt;% select(Location) %&gt;% table()\n\nLocation\n  bank  conif  decid ground  shrub   snag   wall \n     3     14     25     19     17      4      4 \n\n\n\n\nForward +1\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ 1\n         Df Deviance     AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       108.533 110.533                      \nLength    1  105.296 109.296  3.2373 0.0719792 .  \nLocation  6   77.065  91.065 31.4684 2.063e-05 ***\nNo.eggs   1   90.951  94.951 17.5816 2.752e-05 ***\nColor     1  108.087 112.087  0.4463 0.5041175    \nIncubate  1  108.267 112.267  0.2658 0.6061875    \nNestling  1   93.825  97.825 14.7078 0.0001255 ***\nTotcare   1   98.964 102.964  9.5688 0.0019791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nForward +2\n\nglm(`Closed?` ~ Location, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ Location\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        77.065 91.065                      \nLength    1   71.704 87.704  5.3605    0.0206 *  \nNo.eggs   1   61.211 77.211 15.8530 6.846e-05 ***\nColor     1   74.758 90.758  2.3070    0.1288    \nIncubate  1   74.829 90.829  2.2355    0.1349    \nNestling  1   74.722 90.722  2.3425    0.1259    \nTotcare   1   76.635 92.635  0.4300    0.5120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nForward +3\n\nglm(`Closed?` ~ No.eggs + Location, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ No.eggs + Location\n         Df Deviance    AIC     LRT Pr(&gt;Chi)  \n&lt;none&gt;        61.211 77.211                   \nLength    1   58.229 76.229 2.98230  0.08418 .\nColor     1   59.925 77.925 1.28650  0.25669  \nIncubate  1   59.891 77.891 1.32019  0.25056  \nNestling  1   59.247 77.247 1.96461  0.16102  \nTotcare   1   60.751 78.751 0.46084  0.49723  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBackward -1\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling + Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)   \n&lt;none&gt;        46.252 70.252                    \nLength    1   52.812 74.812  6.5600 0.010430 * \nLocation  6   66.017 78.017 19.7648 0.003049 **\nNo.eggs   1   56.049 78.049  9.7973 0.001748 **\nColor     1   46.997 68.997  0.7457 0.387857   \nIncubate  0   46.252 70.252  0.0000            \nNestling  0   46.252 70.252  0.0000            \nTotcare   0   46.252 70.252  0.0000            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBackward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate  + Totcare, data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)   \n&lt;none&gt;        46.252 70.252                    \nLength    1   52.812 74.812  6.5600 0.010430 * \nLocation  6   66.017 78.017 19.7648 0.003049 **\nNo.eggs   1   56.049 78.049  9.7973 0.001748 **\nColor     1   46.997 68.997  0.7457 0.387857   \nIncubate  1   49.031 71.031  2.7796 0.095472 . \nTotcare   1   56.989 78.989 10.7368 0.001050 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBackward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Incubate  + Totcare, \n    data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Incubate + Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;        46.997 68.997                     \nLength    1   53.878 73.878  6.8809 0.008712 ** \nLocation  6   66.664 76.664 19.6663 0.003175 ** \nNo.eggs   1   57.418 77.418 10.4201 0.001247 ** \nIncubate  1   49.839 69.839  2.8416 0.091854 .  \nTotcare   1   58.227 78.227 11.2297 0.000805 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nBackward -3\n\nglm(`Closed?` ~ Length + Location + No.eggs + Totcare, \n    data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Totcare\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        49.839 69.839                      \nLength    1   60.751 78.751 10.9116 0.0009556 ***\nLocation  6   69.236 77.236 19.3974 0.0035425 ** \nNo.eggs   1   61.940 79.940 12.1013 0.0005039 ***\nTotcare   1   58.229 76.229  8.3902 0.0037725 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nAutomatic: Forward w AIC\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") %&gt;%\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, \n         direction = \"forward\", k = 2)\n\nStart:  AIC=110.53\n`Closed?` ~ 1\n\n           Df Deviance     AIC\n+ Location  6   77.065  91.065\n+ No.eggs   1   90.951  94.951\n+ Nestling  1   93.825  97.825\n+ Totcare   1   98.964 102.964\n+ Length    1  105.296 109.296\n&lt;none&gt;         108.533 110.533\n+ Color     1  108.087 112.087\n+ Incubate  1  108.267 112.267\n\nStep:  AIC=91.06\n`Closed?` ~ Location\n\n           Df Deviance    AIC\n+ No.eggs   1   61.211 77.211\n+ Length    1   71.704 87.704\n+ Nestling  1   74.722 90.722\n+ Color     1   74.758 90.758\n+ Incubate  1   74.829 90.829\n&lt;none&gt;          77.065 91.065\n+ Totcare   1   76.635 92.635\n\nStep:  AIC=77.21\n`Closed?` ~ Location + No.eggs\n\n           Df Deviance    AIC\n+ Length    1   58.229 76.229\n&lt;none&gt;          61.211 77.211\n+ Nestling  1   59.247 77.247\n+ Incubate  1   59.891 77.891\n+ Color     1   59.925 77.925\n+ Totcare   1   60.751 78.751\n\nStep:  AIC=76.23\n`Closed?` ~ Location + No.eggs + Length\n\n           Df Deviance    AIC\n+ Nestling  1   47.292 67.292\n+ Totcare   1   49.839 69.839\n&lt;none&gt;          58.229 76.229\n+ Color     1   56.989 76.989\n+ Incubate  1   58.227 78.227\n\nStep:  AIC=67.29\n`Closed?` ~ Location + No.eggs + Length + Nestling\n\n           Df Deviance    AIC\n&lt;none&gt;          47.292 67.292\n+ Color     1   46.580 68.580\n+ Incubate  1   46.997 68.997\n+ Totcare   1   46.997 68.997\n\n\n\nCall:  glm(formula = `Closed?` ~ Location + No.eggs + Length + Nestling, \n    family = \"binomial\", data = nests)\n\nCoefficients:\n   (Intercept)   Locationconif   Locationdecid  Locationground   Locationshrub  \n       11.1085        -19.2865        -16.8603        -20.5222        -18.6448  \n  Locationsnag    Locationwall         No.eggs          Length        Nestling  \n        0.6949        -18.3127          0.7950         -0.2194          0.3983  \n\nDegrees of Freedom: 85 Total (i.e. Null);  76 Residual\nNull Deviance:      108.5 \nResidual Deviance: 47.29    AIC: 67.29\n\n\n\n\nFinal Forward AIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Nestling,\n    data = nests, family=\"binomial\") %&gt;% tidy()\n\n# A tibble: 10 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      11.1   3328.      0.00334  0.997  \n 2 Length           -0.219    0.0754 -2.91     0.00364\n 3 Locationconif   -19.3   3328.     -0.00580  0.995  \n 4 Locationdecid   -16.9   3328.     -0.00507  0.996  \n 5 Locationground  -20.5   3328.     -0.00617  0.995  \n 6 Locationshrub   -18.6   3328.     -0.00560  0.996  \n 7 Locationsnag      0.695 4313.      0.000161 1.00   \n 8 Locationwall    -18.3   3328.     -0.00550  0.996  \n 9 No.eggs           0.795    0.262   3.04     0.00238\n10 Nestling          0.398    0.144   2.76     0.00577\n\n\n\n\nAutomatic: Backward w BIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare,\n    data = nests, family=\"binomial\") %&gt;%\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, \n         direction = \"backward\", k = log(86))\n\nStart:  AIC=99.7\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling + Totcare\n\n\nStep:  AIC=99.7\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling\n\n           Df Deviance     AIC\n- Location  6   66.017  92.743\n- Incubate  1   46.580  95.577\n- Color     1   46.997  95.995\n&lt;none&gt;          46.252  99.704\n- Length    1   52.812 101.810\n- No.eggs   1   56.049 105.047\n- Nestling  1   56.989 105.986\n\nStep:  AIC=92.74\n`Closed?` ~ Length + No.eggs + Color + Incubate + Nestling\n\n           Df Deviance     AIC\n- Incubate  1   66.175  88.447\n- Color     1   66.664  88.935\n&lt;none&gt;          66.017  92.743\n- No.eggs   1   74.635  96.907\n- Length    1   75.000  97.272\n- Nestling  1   85.891 108.163\n\nStep:  AIC=88.45\n`Closed?` ~ Length + No.eggs + Color + Nestling\n\n           Df Deviance     AIC\n- Color     1   66.762  84.579\n&lt;none&gt;          66.175  88.447\n- No.eggs   1   75.577  93.395\n- Length    1   79.115  96.932\n- Nestling  1   89.064 106.881\n\nStep:  AIC=84.58\n`Closed?` ~ Length + No.eggs + Nestling\n\n           Df Deviance     AIC\n&lt;none&gt;          66.762  84.579\n- No.eggs   1   76.704  90.067\n- Length    1   79.162  92.525\n- Nestling  1   90.053 103.416\n\n\n\nCall:  glm(formula = `Closed?` ~ Length + No.eggs + Nestling, family = \"binomial\", \n    data = nests)\n\nCoefficients:\n(Intercept)       Length      No.eggs     Nestling  \n    -6.7711      -0.1871       0.6476       0.4062  \n\nDegrees of Freedom: 85 Total (i.e. Null);  82 Residual\nNull Deviance:      108.5 \nResidual Deviance: 66.76    AIC: 74.76\n\n\n\n\nFinal Backward BIC\n\nglm(`Closed?` ~ Length + No.eggs + Nestling,\n    data = nests, family=\"binomial\") %&gt;% tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -6.77     1.73       -3.90 0.0000946\n2 Length        -0.187    0.0598     -3.13 0.00177  \n3 No.eggs        0.648    0.245       2.65 0.00815  \n4 Nestling       0.406    0.107       3.78 0.000156 \n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "clicker_slides.html",
    "href": "clicker_slides.html",
    "title": "Clicker Q",
    "section": "",
    "text": "to go with Practicing Statistics by Kuiper & Sklar. Math 150 - Methods in Biostatistics.\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatistics, you should know at least a little bit (hopefully a lotta bit) about the following topics.\n\nHypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-square test.1\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics\n\nInteraction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.2\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\n\nThe Central Limit Theorem (CLT) says:3\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nThe p-value is the probability:4\n\nthat the null hypothesis is true given the observed data.\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\n\nWhy do we use a t distribution (instead of a z / normal distribution) in the t-test?5\n\nthe technical conditions don’t hold\nthe means are quite variable\nwe like the letter t\nwe have two samples\nwe don’t know the true standard deviation parameter\n\n\n\n\nWhat happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?6\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\nthe software won’t give a p-value as output\nthe rejection region needs to be calculated in the opposite direction\nthe world blows up\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_i\\)?7\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_iy_i\\)?8\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nThe regression technical conditions include:9\n\nThe Y variable is normally distributed\nThe X variable is normally distributed\nThe residuals are normally distributed\nThe slope coefficient is normally distributed\nThe intercept coefficient is normally distributed\n\n\n\n\nWe need the technical conditions to hold in order to calculate \\(b_0\\) and \\(b_1.\\)10\n\nTRUE\nFALSE\nIt depends\n\n\n\n\nWhy do we check technical conditions?11\n\nso that the inference is valid\nso that the estimates are valid\nso that the p-value is more likely to be small\nso that the confidence level is right\nfor fun\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?12\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\nWith a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?13\n\nhappiness causes longer lives\nlonger lives cause happiness\nhappiness and longer life are correlated\nhappiness and longer life are perfectly predictive\nhappiness and longer life are unrelated\n\n\n\n\nIf there is no relationship in the population (true correlation = 0), then r = 0.14\n\nTRUE\nFALSE\n\n\n\n\nIf there is no relationship in the population (true slope \\(\\beta_1 = 0\\)), then \\(b_1=0\\).15\n\nTRUE\nFALSE\n\n\n\n\nSmaller variability around the regression line (\\(\\sigma\\)):16\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nSmaller variability in the explanatory variable (SD(X) = \\(s_X\\)):17\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nA smaller sample size (\\(n\\)):18\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nWe transform our variables…19\n\n… to find the highest \\(r^2\\) value.\n… when the X variable is not normally distributed.\n… to make the model easier to interpret.\n… so that the technical conditions are met.\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?20\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\n“Observed data or more extreme” is:21\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?22\n\n0\n9\n5.3\n11\n15\n\n\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?23\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?24\n\nSubstantially greater than 50%\nSubstantially less than 50%\nClose to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions, but I didn’t happen to this week.\nI can’t make the scheduled office hours / mentor sessions\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA. What is measurable?25\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?26\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA. What is measurable?27\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\nFrom the NYT, March 21, 2023, https://www.nytimes.com/2023/03/21/sports/basketball/tall-basketball-march-madness.html\n\nThe average W.N.B.A. player, at a shade taller than 6 feet, towers over the average American woman (5 feet 3.5 inches). American men who are between 6 feet and 6-2 — significantly taller than the 5-9 average — have about a five in a million chance of making the N.B.A., according to “The Sports Gene,” a 2013 book by David Epstein about the science of athletic performance. But if you hit the genetic lottery and happen to be 7 feet tall, your chances of landing in the N.B.A. are roughly one in six. (There are 38 players on active rosters who are 7 feet or taller, according to N.B.A. Advanced Stats; the average height of an N.B.A. player is 6 feet 6.5 inches.)\n\nhttps://davidepstein.com/david-epstein-the-sports-gene/\n\n\nCalcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.28\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nIt is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.29\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nWhen we select individuals based on the explanatory variable, we cannot accurately measure30\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nRelative Risk is31\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nThe odds ratio is “invariant to which variable is explanatory and which is response” means:32\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nIn finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?33\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\nBecause taking the natural log of the RR makes the distribution approximately normal.\nBecause the natural log compresses values that are bigger than 1 and spreads values that are smaller than 1.\nBecause we can get exact p-values using Fisher’s Exact Test.\n\n\n\n\nIn order to find a CI for the true OR, our steps are:34\n\n\nfind \\(\\widehat{\\ln(\\mbox{OR})}\\)\nadd \\(\\pm \\ z^* \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\\)\ntake exp of the endpoints\n\n\nbecause the sampling distribution of \\(\\widehat{\\mbox{OR}}\\) is normal\nbecause OR is typically greater than 1\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\nI know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW solutions35\n\nTRUE\nFALSE\n\n\n\n\nAt the value \\(x = -\\beta_0 / \\beta_1\\), the probability of success is:36\n\n0\n0.5\n1\ndepends on \\(\\beta_0\\)\ndepends on \\(\\beta_1\\)\n\n\n\n\nThe logistic model gives probability of failure:37\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of success:38\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of failure:39\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nWith a logistic regression model, the relative risk of success (for a one unit increase in X) is:40\n\n\\(- \\beta_0/\\beta_1\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\na non-linear function of X (which depends on X )\n\n\n\n\nIf we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:41\n\nlinear\nlogistic\ncomplementary log-log\nlog-linear\n\n\n\n\nYou take a sample of size 4 from a binary population and get: FSFF. (failure, success, failure, failure) What is your guess for p = P(success)?42\n\n0.05\n0.15\n0.25\n0.5\n0.75\n\n\n\n\nIn a logistic regression model, the variability is given by43\n\nNormal Y given X\nBinomial Y given X\nBernoulli Y given X\nPoisson Y given X\n\n\n\n\nWhen trying to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\), we maximize the likelihood. \\[\\prod_{i=1}^n \\bigg(\\frac{e^{\\beta_0+ \\beta_1 x_i}}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{y_i}\\bigg(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{1 - y_i}\\] Take the derivative with respect to which variable(s):44\n\nX\nY\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\nMaximum likelihood estimation seeks to:45\n\nFind the data which are most likely under the model.\nFind the parameters which are most likely under the model.\nFind the parameters which make the data most likely under the model.\nFind the data which make the parameters most likely under the model.\n\n\n\n\nWe use maximum likelihood estimation because:46\n\nIt gives an principled approach for estimating the parameters.\nThe estimates are asymptotically normally distributed.\nThe estimates are always easy to compute.\nAll of the above.\nSome of the above.\n\n\n\n\nWe know that for a given data set (with MLEs of \\(b_0\\),\\(b_1\\)):47\n\n\\(L(b_0,b_1)&lt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1)&gt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\leq L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\nHow many parameters did we estimate in the HERS warm-up with the additive model?48\n\n1\n3\n4\n2757\n2761\n\n\n\n\nHow many parameters did we estimate in the HERS warm-up with the interaction model?49\n\n3\n4\n6\n7\n12\n\n\n\n\nWhat are the df for the LRT addressing whether interaction is needed in the HERS warm-up?50\n\n2\n3\n2760\n2754\n2757\n\n\n\n\n(Bird nest example) How many parameters do we estimate when considering Length as a categorical variable? (the only variable)51\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Length (as a categorical variable) belongs in the model?52\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Incubate and Color belong in the model (given Length is determined to be in the model)?53\n\n0\n1\n2\n3\n4\n\n\n\n\nAn interaction term in a multiple logistic regression model may be used when:54\n\nthe model fit is poor.\nthere is a quadratic relationship between the response and explanatory variables.\nneither one of two explanatory variables contribute significantly to the regression model.\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n\n\n\nThe interpretations of the main effects (on their own) make sense only when the interaction component is not significant.55\n\nTRUE\nFALSE\n\n\n\n\nIf the interaction is significant but the main effects aren’t:56\n\nreport on the significance of the main effects\nremove the main effects from the model\navoid talking about main effects on their own\ntest whether the main effects are significant without interaction in the model\n\n\n\n\nWith two variables of interest, what should you test first?57\n\nVariable 1.\nVariable 2.\nThe interaction between variables 1 and 2.\nNone of the above.\n\n\n\n\nConsider variable 1 is continuous and variable 2 has 4 levels. How many degrees of freedom are associated with the drop in deviance test (LRT) of their overall interaction?58\n\n1\n2\n3\n4\n5\n\n\n\n\nWhen selecting variables, it is important that59\n\nThe model predicts training data well\nThe model predicts test data well\nThe coefficients on the variables are all significant\nThe relationships between the variables make sense\n\n\n\n\nTo get a sense of the true accuracy of the model, the test data should be assessed (for accuracy)60\n\non the first model only.\non the last model only.\non every model in the process.\n\n\n\n\nIf I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on testing set, what should I look out for?61\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nIf I am picking and choosing between features of my dataset and I achieve 30% accuracy on my training set, and ~30% on testing set, what should I look out for?62\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nCross validating will guarantee that the model does not overfit.63\n\nTRUE\nFALSE\n\n\n\n\nSuppose we want to compute 10-Fold Cross-Validation error on 200 training examples. We need to compute a model error rate N1 times, and the Cross-Validation error is the average of the errors. To compute each error, we need to train a model with data of size N2, and test the model on the data of size N3. What are the numbers for N1, N2, N3?64\n\nN1 = 1, N2 = 180, N3 = 20\nN1 = 10, N2 = 180, N3 = 20\nN1 = 10, N2 = 200, N3 = 20\nN1 = 10, N2 = 200, N3 = 200\nN1 = 20, N2 = 180, N3 = 20\n\n\n\n\nYou are reviewing papers for Fancy Conference, and you see submissions with the following claims. Which ones would you consider accepting?65\n\nMy method achieves a training error lower than all previous methods!\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min test error.)\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\nMy method achieves a CV error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n\n\n\nWhich model is better (according to ROC)?66\n\npink because it goes closer to (1,1)\npink because it is closer to y=x\nblue because it is farther from y=x\nblue because it is steeper\nneither\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn ROC curve, the x-axis measures67\n\nTrue Pos Rate which we want high\nFalse Pos Rate which we want low\nTrue Neg Rate which we want high\nFalse Neg Rate which we want low\n\n\n\n\nQuiz on 11 topics (you know nothing). Your friends know topics:\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\nWho should you choose to help you answer the questions?68\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nWho do you want to choose next?69\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nIf you can pick two people, who do you pick?70\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA, B\nA, C\nA, D\nC, B\nC, D\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.71\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.72\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.73\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.74\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by best-subsets selection are a subset of the variables in the (k+1)-variable model identified by best-subsets selection.75\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nIn a drop-in-deviance test (LRT), the reduced model corresponds to the null hypothesis being true.76\n\nTRUE\nFALSE\n\n\n\n\nIn a drop-in-deviance test (LRT), the full model corresponds to the alternative hypothesis being true.77\n\nTRUE\nFALSE\n\n\n\n\nWith model building:78\n\nThere are many ways to find a good model.\nThere is always one right answer.\nThere is no end to the fun.\nCan we take a pure math class yet?\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins, the coefficient on number of coins is:79\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of low coins, the coefficient on number of low coins is:80\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of coins is:81\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of low coins is:82\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nIf we consider the censored times to be event times, the empirical survival curve will (on average)83\n\nunderestimate the parameter\noverestimate the parameter\nsometimes under and sometimes overestimate the parameter\n\n\n\n\n\\(n_i - d_i = n_{i+1}\\) when:84\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\n\\(\\frac{(n_i - d_i)}{n_i} = 1\\) when:85\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\nProp survive &gt; 50 days, treated (turquoise line)86\n\n~0.65\n~0.35\n~0.45\nwe only know it’s bigger than red\nwe only know it’s smaller than red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaplan Meier curves (Log-Rank p-value),87\n\nblue is clearly better\nred is clearly better\ncan’t tell because they cross\ncan’t tell because the p-value is big\ncan’t tell because the p-value is small\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the log-rank test, why is it okay to consider only one cell of the 2x2 table at time \\(t_j\\)?88\n\nBecause the row totals are fixed.\nBecause the column totals are fixed.\nBecause the row and column totals are fixed.\nBecause the total number of observations is fixed.\n\n\n\n\nWhat does it mean for the log rank test to be more powerful than the Wilcoxon test?89\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n\n\n\nThe hazard at time \\(t\\) represents:90\n\nthe probability of the event\nthe instantaneous rate of the event\nthe relative risk of the event\nthe odds ratio of the event\n\n\n\n\nThe last entry in the table for the h(t) column is NA because:91\n\nthe last observation was a death\nthe last observation was censored\nthe time interval is too big\nthe time interval is too small\n\n\n\n\n\n\n\nTable 9.6 [@KuiperSklar]\n\n\n\n\n\n\nCensored observations are\\(\\ldots\\)?92\n\nMore important than non-censored ones in survival analysis\nAre assumed to be normally distributed over time\nAre assumed to have the same survival chances as uncensored observations\nAre essential to allow calculation of the Kaplan Meier plot\nAre allocated to the baseline survival curve\n\n\n\n\nSurvival Analysis: for a one unit change of an explanatory variable, the corresponding coefficient \\(e^\\beta\\) represents:93\n\nbaseline survival\nsurvival ratio\nbaseline hazard\nhazard ratio\n\n\n\n\nIn survival analysis, the closest interpretation of the value \\(e^\\beta\\) is:94\n\nodds\nprobability\ntime to event\nrelative risk\nodds ratio\n\n\n\n\nLet the event be death. If larger values of the explanatory variable are associated with higher likelihood of survival, the coefficient \\((\\beta)\\) should be 95\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nLet the event be death. If larger values of the variable are NOT associated with higher (or lower) likelihood of survival, the coefficient \\((\\beta)\\) should be96\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nBP violates the “linear HR” condition if:97\n\nthe ln ratio of the hazard curves is not linear with respect to BP\nthe ln ratio of the survival curves is not linear with respect to BP\nthe effect of BP is to increase the hazard\nthe effect of BP is to decrease the hazard\nthere is no effect due to BP\n\n\n\n\nA Cox regression analysis:98\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time.\nCan only be used when there are censored data\nAssumes that the relative hazard for a particular variable is always constant\nUses the logrank statistic to compare two survival curves\nRelies on the condition that the explanatory variables (covariates) in the model are normally distributed.\n\n\n\n\nThe effect of weight could violate PH if:99\n\npeople of different weights are in control vs treatment group\npeople tend to weigh less over time\nthe hazard function for weight is not monotonic\nthe hazard function changes as a function of weight which is also changing over time\n\n\n\n\nThe effect of treatment could violate PH if:100\n\nthe treatment has no effect\nthe treatment produces short term benefits only\nthe treatment effect interacts with a different variable, like gender\nthere is more than one treatment group\n\n\n\n\nAIC, BIC, model validation, and stepwise regression are methods for101\n\nparameter estimation\nvariable selection\n\n\n\n\nIf \\(\\alpha = 0.05\\), I would expect 5% of all hypotheses to be rejected.102\n\nTRUE\nFALSE\n\n\n\n\nPower is:103\n\nP(type I error)\nP(type II error)\n1 – P(type I error)\n1 – P(type II error)\n\n\ntype I = \\(H_0\\) true, but we reject\ntype II = \\(H_0\\) false, but we fail to reject\npower = P(rejecting when \\(H_0\\) false)\n\n\nThe p-value is104\n\nP(\\(H_0\\) is true | data)\nP(\\(H_a\\) is true | data)\nP(data | \\(H_0\\) is true)\nP(data | \\(H_a\\) is true)\n1 – P(data | \\(H_0\\) is true)\n\n\n\nRA Fisher (1929) &gt;“… An observation is judged significant, if it would rarely have been produced, in the absence of a real cause of the kind we are seeking. It is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained. He should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation.”\n\n\nFor hypothesis testing, the problem of multiple comparisons (also known as the multiple testing problem) results from the increase in ________ that occurs when statistical tests are used repeatedly.105\n\nType I errors\nType II errors\nNull hypothesis\nStatistical hypothesis testing\n\n\n\n\nIf \\(H_0\\) is true, the p-values should be distributed:106\n\nUniformly (equal prob) on 0 to 1\nUniformly on -1 to 1\nUnimodal on 0 to 1\nSkewed left on 0 to 1\nSkewed right on 0 to 1\n\n\n\n\nGiven many many tests (presumably some are null and some are “true”), a good estimate of the number of null tests is:107\n\n(# p-values &gt; 0.5) / 2\n(# p-values &gt; 0.5) * 2\n(# p-values &lt; 0.5) / 2\n(# p-values &lt; 0.5) * 2\n\n\n\n\nWhat do I do if the adjusted p-value is bigger than 1?108\n\nLeave it unadjusted\nAssign the value of the previous (“smaller”) p-value\nRound it to 1\nDivide by 2\n\n\n\n\nWith Holm’s method, what do I do if the (m+1)^th adjusted p-value is smaller than the m^th adjusted p-value?109\n\nLeave it unadjusted\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\nRound it to 1\nDivide by 2\n\n\n\n\nThe false discovery rate represents110\n\nthe proportion of true discoveries out of the total tests\nthe proportion of true discoveries out of the total discoveries\nthe ratio of the number of true discoveries divided by the number of null discoveries\nthe number of null discoveries out of the total tests\nthe number of null discoveries out of the total discoveries\n\n\n\n\nFDR and FWER differ in that111\n\nFDR is a rate and FWER is a probability\nFDR controls the rate of false positives\nFWER controls the probability of getting a false positive\nsome of the above\nall of the above\n\n\n\n\nWhich multiple comparisons adjustment gives the highest power?112\n\nBonferonni\nHolm\nBenjamini-Hochberg\nStorey (q-values)"
  },
  {
    "objectID": "clicker_slides.html#footnotes",
    "href": "clicker_slides.html#footnotes",
    "title": "Methods in Biostatistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npreferably d or e. maybe c on some of them.\nthese are the topics we will be covering. Would be nice if you have heard of them.\nwherever you are, make sure you are communicating with me when you have questions!\nwherever you are, make sure you are communicating with me when you have questions!\n\non Gradescope\n\n\n\npushing the file(s)\n\n\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n\n\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\nwe don’t know the true standard deviation parameter\n\n\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\n\n\n\n\\(n_2\\)\n\n\n\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\nThe residuals are normally distributed (which induces a., d., and e.). There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).\n\n\nFALSE. We can always minimize the sums of squares, regardless of whether or not the model is any good.\n\nso that the inference is valid (and also for fun). Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the “inference” paradigm.\n\n\n\ndue to estimation and average\n\n\n\nhappiness and longer life are correlated\n\n\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n\n\ndecreases the variability of \\(b_1\\).\n\n\n\nincreases the variability of \\(b_1\\).\n\n\n\nincreases the variability of \\(b_1\\).\n\n\n\nso that the technical conditions are met.\n\n\n\nrandom allocation\n\n\n\n9 or more\n\n\n\n5.3 because (15/31)*11 = 5.3\n\n\n\nStrong evidence that Botox is more effective than the placebo.\n\n\n\nClose to 50% (the point estimate is 0.6)\n\n\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n\n\nboth (cross-classification: can measure all the probabilities)\n\n\n\ncase-control (they selected based on people who had died or not)\n\n\n\ncross-classification (they selected all uses of catheters)\n\n\n\nthe proportion of people in the population in each explanatory category (tbh, we can’t measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)\n\n\n\nthe ratio of two proportions\n\n\n\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\n\n\n\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\n\n\nThe worksheet solutions and clicker questions are on the main course website. The HW solutions are on Canvas under Files.\n\n0.5\n\n\n\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\n\n\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\n\n\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\n\n\na non-linear function of X (which depends on X )\n\n\n\nlog-linear\n\n\n\n0.25\n\n\n\nBernoulli Y given X\n\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\nFind the parameters which make the data most likely under the model.\n\n\n\nSome of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)\n\n\n\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\\(\\frac{e^{b_0}}{1 + e^{b_0}}\\)\n\n\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4\\)\n\n\n\n4 parameter estimates: \\(b_0, b_1, b_2, b_3\\)\n\n\n\n7 parameter estimates: \\(b_0, b_1, b_2, b_3, b_4, b_5, b_6\\)\n\n\n\n3 (7 - 4 = 3)\n\n\n\n34\n\n\n\n33 (34 - 1 = 33)\n\n\n\n2 (4 - 2 = 2)\n\n\n\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n\n\nTRUE\n\n\n\navoid talking about main effects on their own\n\n\n\nThe interaction between variables 1 and 2. (probably… although there are many schools of thought on how to build models)\n\n\n\n3 (1 * (4-1) = 3)\n\n\n\nThe model predicts test data well\n\n\n\non the last model only.\n\n\n\noverfitting\n\n\n\nunderfitting\n\n\n\nFALSE. CV reduces the effect of overfitting, but at the end of the day, you are still building a model on the dataset at hand, and it is possible that you will overfit that dataet.\n\n\n\nN1 = 10, N2 = 180, N3 = 20\n\n\n\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n\n\nblue because it is farther from the line y=x\n\n\n\nFalse Pos Rate which we want low\n\n\n\nA\n\n\n\nB\n\n\n\nC and D\n\n\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n\n\nTRUE\n\n\n\nTRUE\n\n\n\nFALSE\n\n\n\nFALSE\n\n\n\nFALSE\n\n\n\nTRUE (the coefficient values are forced to be zero)\n\n\n\nFALSE (the null model can exist within the full model because there is flexibility in the values of the coefficients)\n\n\n\nThere are many ways to find a good model. Also, c. there is no end to the fun.\n\n\n\npositive\n\n\n\npositive\n\n\n\npositive\n\n\n\nnegative\n\n\n\nunderestimate the parameter\n\n\n\nsometimes under and sometimes overestimate the parameter. Because censoring and survival time are independent, there isn’t any reason why the censored observations would be different from the other observations. However, removing censored observations isn’t ideal because you lose information.\n\n\n\nthere is no censoring at time \\(t_i\\)\n\n\n\nthere are no deaths at time \\(t_i\\)\n\n\n\n~0.65\n\n\n\ncan’t tell because they cross (and also because d. the p-value is big)\n\n\n\nBecause the row and column totals are fixed.\n\n\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n\n\nthe instantaneous rate of the event\n\n\n\nthe last observation was censored. The reason the hazard is zero is because the width of the time interval is unknown, that is we don’t know when the last event time is.\n\n\n\nAre assumed to have the same survival chances as uncensored observations\n\n\n\nhazard ratio\n\n\n\nrelative risk\n\n\n\nnegative\n\n\n\nzero\n\n\n\nthe ln ratio of the hazard curves is not linear with respect to BP\n\n\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time. and c. Assumes that the relative hazard for a particular variable is always constant\n\n\n\nthe hazard function changes as a function of weight which is also changing over time\n\n\n\nthe treatment produces short term benefits only\n\n\n\nvariable selection\n\n\n\nFALSE, we’d expect 5% of all null hypotheses to be rejected\n\n\n\n1 – P(type II error)\n\n\n\nP(data | \\(H_0\\) is true)\n\n\n\nType I errors\n\n\n\nUniformly (equal prob) on 0 to 1\n\n\n\n(# p-values &gt; 0.5) * 2\n\n\n\nRound it to 1\n\n\n\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\n\n\n\nthe number of null discoveries out of the total discoveries\n\n\n\nall of the above\n\n\n\nStorey (q-values)\n\n\n\nPocock\n\n\n\nall of the above. We restrict our analysis to a situation of controling the type I error. However, that isn’t why we want to stop early. We want to stop early for the other reasons listed."
  },
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Clicker Q",
    "section": "",
    "text": "to go with Practicing Statistics by Kuiper & Sklar. Math 150 - Methods in Biostatistics.\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatistics, you should know at least a little bit (hopefully a lotta bit) about the following topics.\n\nHypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-squared test.1\n\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics\n\nInteraction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.2\n\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\nR / R Studio / Quarto3\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “R”?\n\n\n\n\nGit / GitHub4\n\nall good\nstarted, progress is slow and steady\nstarted, very stuck\nhaven’t started yet\nwhat do you mean by “Git”?\n\n\n\n\nWhere can I get feedback on my HW assignments / quizzes?5\n\nprof will return paper versions\non Gradescope\non Canvas\non GitHub\n\n\n\n\nWhich of the following includes talking to the remote version of GitHub?6\n\nchanging your name (updating the YAML)\ncommitting the file(s)\npushing the file(s)\nsome of the above\nall of the above\n\n\n\n\nThe Central Limit Theorem (CLT) says:7\n\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\nThe p-value is the probability:8\n\n\nthat the null hypothesis is true given the observed data.\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\nWhy do we use a t distribution (instead of a z / normal distribution) in the t-test?9\n\n\nthe technical conditions don’t hold\nthe means are quite variable\nwe like the letter t\nwe have two samples\nwe don’t know the true standard deviation parameter\n\n\n\nWhat happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?10\n\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\nthe software won’t give a p-value as output\nthe rejection region needs to be calculated in the opposite direction\nthe world blows up\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_{i=1}^n x_i?\\)11\n\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_{i=1}^n x_iy_i?\\)12\n\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\nThe regression technical conditions include:13\n\nThe Y variable is normally distributed\nThe X variable is normally distributed\nThe residuals are normally distributed\nThe slope coefficient is normally distributed\nThe intercept coefficient is normally distributed\n\n\n\n\nWe need the technical conditions to hold in order to calculate \\(b_0\\) and \\(b_1.\\)14\n\nTRUE\nFALSE\nIt depends\n\n\n\n\nWhy do we check technical conditions?15\n\nso that the inference is valid\nso that the estimates are valid\nso that the p-value is more likely to be small\nso that the confidence level is right\nfor fun\n\n\n\n\nWhen writing the regression equation, why is there a hat \\((\\ \\hat{} \\ )\\) on the response variable?16\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\nWith a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?17\n\nhappiness causes longer lives\nlonger lives cause happiness\nhappiness and longer life are correlated\nhappiness and longer life are perfectly predictive\nhappiness and longer life are unrelated\n\n\n\n\nIf there is no relationship in the population (true correlation = 0), then r = 0.18\n\nTRUE\nFALSE\n\n\n\n\nIf there is no relationship in the population (true slope \\(\\beta_1 = 0\\)), then \\(b_1=0\\).19\n\nTRUE\nFALSE\n\n\n\n\nSmaller variability around the regression line \\((\\sigma):\\)20\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nSmaller variability in the explanatory variable (SD(X) = \\(s_X):\\)21\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nA smaller sample size \\((n):\\)22\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nWe transform our variables…23\n\n… to find the highest \\(r^2\\) value.\n… when the X variable is not normally distributed.\n… to make the model easier to interpret.\n… so that the technical conditions are met.\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?24\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\n“Observed data or more extreme” is:25\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?26\n\n0\n9\n5.3\n11\n15\n\n\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?27\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?28\n\nSubstantially greater than 50%\nSubstantially less than 50%\nClose to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions, but I didn’t happen to this week.\nI can’t make the scheduled office hours / mentor sessions\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, please contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA. What is measurable?29\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?30\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA. What is measurable?31\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\nFrom the NYT, March 21, 2023, https://www.nytimes.com/2023/03/21/sports/basketball/tall-basketball-march-madness.html\n\nAmerican men who are between 6 feet and 6-2 — significantly taller than the 5-9 average — have about a five in a million chance of making the N.B.A., according to “The Sports Gene,” a 2013 book by David Epstein about the science of athletic performance. But if you hit the genetic lottery and happen to be 7 feet tall, your chances of landing in the N.B.A. are roughly one in six. (There are 38 players on active rosters who are 7 feet or taller, according to N.B.A. Advanced Stats; the average height of an N.B.A. player is 6 feet 6.5 inches.)\n\nhttps://davidepstein.com/david-epstein-the-sports-gene/\n\n\nCalcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.32\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nIt is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.33\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nWhen we select individuals based on the explanatory variable, we cannot accurately measure34\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nRelative Risk is35\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nThe odds ratio is “invariant to which variable is explanatory and which is response” means:36\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nIn finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?37\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\nBecause taking the natural log of the RR makes the distribution approximately normal.\nBecause the natural log compresses values that are bigger than 1 and spreads values that are smaller than 1.\nBecause we can get exact p-values using Fisher’s Exact Test.\n\n\n\n\nIn order to find a CI for the true OR, our steps are:38\n\n\nfind \\(\\widehat{\\ln(\\mbox{OR})}\\)\nadd \\(\\pm \\ z^* \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\\)\ntake exp of the endpoints\n\n\nbecause the sampling distribution of \\(\\widehat{\\mbox{OR}}\\) is normal\nbecause OR is typically greater than 1\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\nI know where to find: the solutions to the worksheets, the clicker questions (with solutions), and the HW solutions39\n\nTRUE\nFALSE\n\n\n\n\nAt the value \\(x = -\\beta_0 / \\beta_1\\), the probability of success is:40\n\n0\n0.5\n1\ndepends on \\(\\beta_0\\)\ndepends on \\(\\beta_1\\)\n\n\n\n\nThe logistic model gives probability of failure:41\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of success:42\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of failure:43\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nWith a logistic regression model, the relative risk of success (for a one unit increase in X) is:44\n\n\\(- \\beta_0/\\beta_1\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\na non-linear function of X (which depends on X )\n\n\n\n\nIf we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:45\n\nlinear\nlogistic\ncomplementary log-log\nlog-linear\n\n\n\n\nYou take a sample of size 4 from a binary population and get: FSFF. (failure, success, failure, failure) What is your guess for p = P(success)?46\n\n0.05\n0.15\n0.25\n0.5\n0.75\n\n\n\n\nIn a logistic regression model, the variability is given by47\n\nNormal Y given X\nBinomial Y given X\nBernoulli Y given X\nPoisson Y given X\n\n\n\n\nWhen trying to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\), we maximize the likelihood. \\[\\prod_{i=1}^n \\bigg(\\frac{e^{\\beta_0+ \\beta_1 x_i}}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{y_i}\\bigg(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{1 - y_i}\\] Take the derivative with respect to which variable(s):48\n\nX\nY\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\nMaximum likelihood estimation seeks to:49\n\nFind the data which are most likely under the model.\nFind the parameters which are most likely under the model.\nFind the parameters which make the data most likely under the model.\nFind the data which make the parameters most likely under the model.\n\n\n\n\nWe use maximum likelihood estimation because:50\n\nIt gives an principled approach for estimating the parameters.\nThe estimates are asymptotically normally distributed.\nThe estimates are always easy to compute.\nAll of the above.\nSome of the above.\n\n\n\n\nWe know that for a given data set (with MLEs of \\(b_0\\),\\(b_1\\)):51\n\n\\(L(b_0,b_1)&lt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1)&gt; L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\leq L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\nIn a logistic regresion if \\(H_0\\) is true, what is the probability of success?52\n\n\n\\(p_0\\)\n\\(\\frac{e^{b_0}}{1 + e^{b_0}}\\)\n\\(\\frac{e^{b_1}}{1 + e^{b_1}}\\)\n\\(e^{b_0}\\)\n\\(\\frac{e^{b_0 + b_1 x_i}}{1 + e^{b_0 + b_1 x_i}}\\)\n\n\n\nWhich is the correct logistic regression model to predict disease status based on snoring (never, occasionally, often, always): \\(X_1 = 1\\) for occasionally; \\(X_2 = 1\\) for often; \\(X_3 = 1\\) for always.53\n\n\nlogit\\((p) = \\beta_0\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4\\)\n\nlogit\\((p) = \\beta_0 + \\beta_1 (X_1 + X_2 + X_3)\\)\n\n\n\nHow many parameters did we estimate in the HERS worksheet with the additive model?54\n\n1\n3\n4\n2757\n2761\n\n\n\n\nHow many parameters did we estimate in the HERS worksheet with the interaction model?55\n\n3\n4\n6\n7\n12\n\n\n\n\nWhat are the df for the LRT addressing whether interaction is needed in the HERS worksheet?56\n\n2\n3\n2760\n2754\n2757\n\n\n\n\n(Bird nest example) How many parameters do we estimate when considering Length as a categorical variable? (the only variable)57\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Length (as a categorical variable) belongs in the model?58\n\n0\n1\n2\n33\n34\n\n\n\n\n(Bird nest example) How many df for the LRT addressing whether Incubate and Color belong in the model (given Length is determined to be in the model)?59\n\n0\n1\n2\n3\n4\n\n\n\n\nAn interaction term in a multiple logistic regression model may be used when:60\n\nthe model fit is poor.\nthere is a quadratic relationship between the response and explanatory variables.\nneither one of two explanatory variables contribute significantly to the regression model.\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n\n\n\nThe interpretations of the main effects (on their own) make sense only when the interaction component is not significant.61\n\nTRUE\nFALSE\n\n\n\n\nIf the interaction is significant but the main effects aren’t:62\n\nreport on the significance of the main effects\nremove the main effects from the model\navoid talking about main effects on their own\ntest whether the main effects are significant without interaction in the model\n\n\n\n\nWith two variables of interest, what should you test first?63\n\nVariable 1.\nVariable 2.\nThe interaction between variables 1 and 2.\nNone of the above.\n\n\n\n\nConsider variable 1 is continuous and variable 2 has 4 levels. How many degrees of freedom are associated with the drop in deviance test (LRT) of their overall interaction?64\n\n1\n2\n3\n4\n5\n\n\n\n\nWhen selecting variables, it is important that65\n\nThe model predicts training data well\nThe model predicts test data well\nThe coefficients on the variables are all significant\nThe relationships between the variables make sense\n\n\n\n\nTo get a sense of the true accuracy of the model, the test data should be assessed (for accuracy)66\n\non the first model only.\non the last model only.\non every model in the process.\n\n\n\n\nIf I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on testing set, what should I look out for?67\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nIf I am picking and choosing between features of my dataset and I achieve 30% accuracy on my training set, and ~30% on testing set, what should I look out for?68\n\nUnderfitting\nNothing, the model is perfect\nOverfitting\n\n\n\n\nCross validating will guarantee that the model does not overfit.69\n\nTRUE\nFALSE\n\n\n\n\nSuppose we want to compute 10-Fold Cross-Validation error on 200 training examples. We need to compute a model error rate N1 times, and the Cross-Validation error is the average of the errors. To compute each error, we need to train a model with data of size N2, and test the model on the data of size N3. What are the numbers for N1, N2, N3?70\n\nN1 = 1, N2 = 180, N3 = 20\nN1 = 10, N2 = 180, N3 = 20\nN1 = 10, N2 = 200, N3 = 20\nN1 = 10, N2 = 200, N3 = 200\nN1 = 20, N2 = 180, N3 = 20\n\n\n\n\nYou are reviewing papers for Fancy Conference, and you see submissions with the following claims. Which ones would you consider accepting?71\n\nMy method achieves a training error lower than all previous methods!\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min test error.)\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\nMy method achieves a CV error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n\n\n\nWhich model is better (according to ROC)?72\n\npink because it goes closer to (1,1)\npink because it is closer to y=x\nblue because it is farther from y=x\nblue because it is steeper\nneither\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn ROC curve, the x-axis measures73\n\nTrue Pos Rate which we want high\nFalse Pos Rate which we want low\nTrue Neg Rate which we want high\nFalse Neg Rate which we want low\n\n\n\n\nQuiz on 11 topics (you know nothing). Your friends know topics:\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\nWho should you choose to help you answer the questions?74\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nWho do you want to choose next?75\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA\nB\nC\nD\ncan’t tell\n\n\n\n\nIf you can pick two people, who do you pick?76\nA: {1, 2, 3, 4, 5, 6, 7}\nB: {8, 9, 10}\nC: {1, 2, 3, 4, 8, 10}\nD: {5, 6, 7, 9, 11}\n\nA, B\nA, C\nA, D\nC, B\nC, D\n\n\n\n\nWhich variable should I put in first for the forward model process?77\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I put in second for the forward model process?78\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I remove first for the backward model process?79\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nWhich variable should I remove second for the backward model process?80\n\nLocation\nNo.eggs\nColor\nIncubate\nNestling\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.81\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.82\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by backward selection are a subset of the variables in the (k+1)-variable model identified by forward selection.83\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by forward selection are a subset of the variables in the (k+1)-variable model identified by backward selection.84\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nThe variables in the k-variable model identified by best-subsets selection are a subset of the variables in the (k+1)-variable model identified by best-subsets selection.85\n\nTRUE (always TRUE)\nFALSE (not always TRUE)\n\n\n\n\nIn a drop-in-deviance test (LRT), the reduced model corresponds to the null hypothesis being true.86\n\nTRUE\nFALSE\n\n\n\n\nIn a drop-in-deviance test (LRT), the full model corresponds to the alternative hypothesis being true.87\n\nTRUE\nFALSE\n\n\n\n\nWith model building:88\n\nThere are many ways to find a good model.\nThere is always one right answer.\nThere is no end to the fun.\nCan we take a pure math class yet?\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins, the coefficient on number of coins is:89\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of low coins, the coefficient on number of low coins is:90\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of coins is:91\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nWhen probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of low coins is:92\n\npositive\nnegative\nzero\nno intuition exists for being able to answer this question\n\n\n\n\nIf we consider the censored times to be event times, the empirical survival curve will (on average)93\n\nunderestimate the parameter\noverestimate the parameter\nsometimes under and sometimes overestimate the parameter\n\n\n\n\nIf we remove all the censored observations, the empirical survival curve will (on average)94\n\nunderestimate the parameter\noverestimate the parameter\nsometimes under and sometimes overestimate the parameter\n\n\n\n\n\\(n_i - d_i = n_{i+1}\\) when:95\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\n\\(\\frac{(n_i - d_i)}{n_i} = 1\\) when:96\n\nthere are no deaths at time \\(t_i\\)\nthere is no censoring at time \\(t_i\\)\nthere are no deaths at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i+1}\\)\nthere is no censoring at time \\(t_{i-1}\\)\n\n\n\n\nProp survive &gt; 50 days, treated (turquoise line)97\n\n~0.65\n~0.35\n~0.45\nwe only know it’s bigger than red\nwe only know it’s smaller than red\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaplan Meier curves (Log-Rank p-value),98\n\nblue is clearly better\nred is clearly better\ncan’t tell because they cross\ncan’t tell because the p-value is big\ncan’t tell because the p-value is small\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the log-rank test, why is it okay to consider only one cell of the 2x2 table at time \\(t_j\\)?99\n\nBecause the row totals are fixed.\nBecause the column totals are fixed.\nBecause the row and column totals are fixed.\nBecause the total number of observations is fixed.\n\n\n\n\nWhat does it mean for the log rank test to be more powerful than the Wilcoxon test?100\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is true.\nlog rank is less likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n\n\n\nThe hazard at time \\(t\\) represents:101\n\nthe probability of the event\nthe instantaneous rate of the event\nthe relative risk of the event\nthe odds ratio of the event\n\n\n\n\nThe last entry in the table for the h(t) column is NA because:102\n\nthe last observation was a death\nthe last observation was censored\nthe time interval is too big\nthe time interval is too small\n\n\n\n\n\n\n\nTable 9.6 [@KuiperSklar]\n\n\n\n\n\n\nCensored observations are\\(\\ldots\\)?103\n\nMore important than non-censored ones in survival analysis\nAre assumed to be normally distributed over time\nAre assumed to have the same survival chances as uncensored observations\nAre essential to allow calculation of the Kaplan Meier plot\nAre allocated to the baseline survival curve\n\n\n\n\nSurvival Analysis: for a one unit change of an explanatory variable, the corresponding coefficient \\(e^\\beta\\) represents:104\n\nbaseline survival\nsurvival ratio\nbaseline hazard\nhazard ratio\n\n\n\n\nIn survival analysis, the closest interpretation of the value \\(e^\\beta\\) is:105\n\nodds\nprobability\ntime to event\nrelative risk\nodds ratio\n\n\n\n\nLet the event be death. If larger values of the explanatory variable are associated with higher likelihood of survival, the coefficient \\((\\beta)\\) should be 106\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nLet the event be death. If larger values of the variable are NOT associated with higher (or lower) likelihood of survival, the coefficient \\((\\beta)\\) should be107\n\nbigger than 1\nsmaller than 1\npositive\nnegative\nzero\n\n\n\n\nBP violates the “linear HR” condition if:108\n\nthe ln ratio of the hazard curves is not linear with respect to BP\nthe ln ratio of the survival curves is not linear with respect to BP\nthe effect of BP is to increase the hazard\nthe effect of BP is to decrease the hazard\nthere is no effect due to BP\n\n\n\n\nA Cox regression analysis:109\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time.\nCan only be used when there are censored data\nAssumes that the relative hazard for a particular variable is always constant\nUses the logrank statistic to compare two survival curves\nRelies on the condition that the explanatory variables (covariates) in the model are normally distributed.\n\n\n\n\nThe effect of weight could violate PH if:110\n\npeople of different weights are in control vs treatment group\npeople tend to weigh less over time\nthe hazard function for weight is not monotonic\nthe hazard function changes as a function of weight which is also changing over time\n\n\n\n\nThe effect of treatment could violate PH if:111\n\nthe treatment has no effect\nthe treatment produces short term benefits only\nthe treatment effect interacts with a different variable, like gender\nthere is more than one treatment group\n\n\n\n\nAIC, BIC, model validation, and stepwise regression are methods for112\n\nparameter estimation\nvariable selection\n\n\n\n\nIf \\(\\alpha = 0.05\\), I would expect 5% of all hypotheses to be rejected.113\n\nTRUE\nFALSE\n\n\n\n\nPower is:114\n\nP(type I error)\nP(type II error)\n1 – P(type I error)\n1 – P(type II error)\n\n\ntype I = \\(H_0\\) true, but we reject\ntype II = \\(H_0\\) false, but we fail to reject\npower = P(rejecting when \\(H_0\\) false)\n\n\nThe p-value is115\n\nP(\\(H_0\\) is true | data)\nP(\\(H_a\\) is true | data)\nP(data | \\(H_0\\) is true)\nP(data | \\(H_a\\) is true)\n1 – P(data | \\(H_0\\) is true)\n\n\n\nRA Fisher (1929) &gt;“… An observation is judged significant, if it would rarely have been produced, in the absence of a real cause of the kind we are seeking. It is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained. He should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation.”\n\n\nFor hypothesis testing, the problem of multiple comparisons (also known as the multiple testing problem) results from the increase in ________ that occurs when statistical tests are used repeatedly.116\n\nType I errors\nType II errors\nNull hypothesis\nStatistical hypothesis testing\n\n\n\n\nIf \\(H_0\\) is true, the p-values should be distributed:117\n\nUniformly (equal prob) on 0 to 1\nUniformly on -1 to 1\nUnimodal on 0 to 1\nSkewed left on 0 to 1\nSkewed right on 0 to 1\n\n\n\n\nGiven many many tests (presumably some are null and some are “true”), a good estimate of the number of null tests is:118\n\n(# p-values &gt; 0.5) / 2\n(# p-values &gt; 0.5) * 2\n(# p-values &lt; 0.5) / 2\n(# p-values &lt; 0.5) * 2\n\n\n\n\nWhat do I do if the adjusted p-value is bigger than 1?119\n\nLeave it unadjusted\nAssign the value of the previous (“smaller”) p-value\nRound it to 1\nDivide by 2\n\n\n\n\nWith Holm’s method, what do I do if the (m+1)^th adjusted p-value is smaller than the m^th adjusted p-value?120\n\nLeave it unadjusted\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\nRound it to 1\nDivide by 2\n\n\n\n\nThe false discovery rate represents121\n\nthe proportion of true discoveries out of the total tests\nthe proportion of true discoveries out of the total discoveries\nthe ratio of the number of true discoveries divided by the number of null discoveries\nthe number of null discoveries out of the total tests\nthe number of null discoveries out of the total discoveries\n\n\n\n\nFDR and FWER differ in that122\n\nFDR is a rate and FWER is a probability\nFDR controls the rate of false positives\nFWER controls the probability of getting a false positive\nsome of the above\nall of the above\n\n\n\n\nWhich multiple comparisons adjustment gives the highest power?123\n\nBonferonni\nHolm\nBenjamini-Hochberg\nStorey (q-values)\n\n\n\n\nWhich stopping criteria is most aggressive with respect to stopping early (i.e., is most likely to stop early)?124\n\nBonferonni\nPocock\nPeto\nO’Brien-Fleming\n\n\n\n\nWhy do we want to stop early?125\n\nget out positive results sooner\nget out negative results sooner\nuse fewer observations (people)\nall of the above\ncontrol the type I error"
  },
  {
    "objectID": "clicker.html#footnotes",
    "href": "clicker.html#footnotes",
    "title": "Clicker Q",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npreferably d or e. maybe c on some of them.↩︎\nthese are the topics we will be covering. Would be nice if you have heard of them.↩︎\nwherever you are, make sure you are communicating with me when you have questions!↩︎\nwherever you are, make sure you are communicating with me when you have questions!↩︎\n\non Gradescope\n\n↩︎\n\npushing the file(s)\n\n↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n↩︎\n\nwe don’t know the true standard deviation parameter\n\n↩︎\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\n\n↩︎\n\n\\(n_2\\)\n\n↩︎\n\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n↩︎\n\nThe residuals are normally distributed (which induces a., d., and e.). There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).\n\n↩︎\nFALSE. We can always minimize the sums of squares, regardless of whether or not the model is any good.↩︎\n\nso that the inference is valid (and also for fun). Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the “inference” paradigm.\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nhappiness and longer life are correlated\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\nso that the technical conditions are met.\n\n↩︎\n\nrandom allocation\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\ncase-control (they selected based on people who had died or not)\n\n↩︎\n\ncross-classification (they selected all uses of catheters)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category (tbh, we can’t measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\n\n↩︎\n\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\n\n↩︎\nThe worksheet solutions and clicker questions are on the main course website. The HW solutions are on Canvas under Files.↩︎\n\n0.5\n\n↩︎\n\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\n↩︎\n\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\n↩︎\n\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\n↩︎\n\na non-linear function of X (which depends on X )\n\n↩︎\n\nlog-linear\n\n↩︎\n\n0.25\n\n↩︎\n\nBernoulli Y given X\n\n↩︎\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n↩︎\n\nFind the parameters which make the data most likely under the model.\n\n↩︎\n\nSome of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)\n\n↩︎\n\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n↩︎\n\n\\(\\frac{e^{b_0}}{1 + e^{b_0}}\\)\n\n↩︎\n\nlogit\\((p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4\\)\n\n↩︎\n\n4 parameter estimates: \\(b_0, b_1, b_2, b_3\\)\n\n↩︎\n\n7 parameter estimates: \\(b_0, b_1, b_2, b_3, b_4, b_5, b_6\\)\n\n↩︎\n\n3 (7 - 4 = 3)\n\n↩︎\n\n34\n\n↩︎\n\n33 (34 - 1 = 33)\n\n↩︎\n\n2 (4 - 2 = 2)\n\n↩︎\n\nthe relationship between X1 and P(success) changes for differing values of X2.\n\n↩︎\n\nTRUE\n\n↩︎\n\navoid talking about main effects on their own\n\n↩︎\n\nThe interaction between variables 1 and 2. (probably… although there are many schools of thought on how to build models)\n\n↩︎\n\n3 (1 * (4-1) = 3)\n\n↩︎\n\nThe model predicts test data well\n\n↩︎\n\non the last model only.\n\n↩︎\n\noverfitting\n\n↩︎\n\nunderfitting\n\n↩︎\n\nFALSE. CV reduces the effect of overfitting, but at the end of the day, you are still building a model on the dataset at hand, and it is possible that you will overfit that dataet.\n\n↩︎\n\nN1 = 10, N2 = 180, N3 = 20\n\n↩︎\n\nMy method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)\n\n↩︎\n\nblue because it is farther from the line y=x\n\n↩︎\n\nFalse Pos Rate which we want low\n\n↩︎\n\nA\n\n↩︎\n\nB\n\n↩︎\n\nC and D\n\n↩︎\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n↩︎\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n↩︎\n\nor e. Hard to say, could have removed Incubate, Nestling, or Totcare. (I removed Nestling.)\n\n↩︎\n\nColor has the smalles test statistic and correspondingly largest p-value.\n\n↩︎\n\nTRUE\n\n↩︎\n\nTRUE\n\n↩︎\n\nFALSE\n\n↩︎\n\nFALSE\n\n↩︎\n\nFALSE\n\n↩︎\n\nTRUE (the coefficient values are forced to be zero)\n\n↩︎\n\nFALSE (the null model can exist within the full model because there is flexibility in the values of the coefficients)\n\n↩︎\n\nThere are many ways to find a good model. Also, c. there is no end to the fun.\n\n↩︎\n\npositive\n\n↩︎\n\npositive\n\n↩︎\n\npositive\n\n↩︎\n\nnegative\n\n↩︎\n\nunderestimate the parameter\n\n↩︎\n\nsometimes under and sometimes overestimate the parameter. Because censoring and survival time are independent, there isn’t any reason why the censored observations would be different from the other observations. However, removing censored observations isn’t ideal because you lose information.\n\n↩︎\n\nthere is no censoring at time \\(t_i\\)\n\n↩︎\n\nthere are no deaths at time \\(t_i\\)\n\n↩︎\n\n~0.65\n\n↩︎\n\ncan’t tell because they cross (and also because d. the p-value is big)\n\n↩︎\n\nBecause the row and column totals are fixed.\n\n↩︎\n\nlog rank is more likely to reject \\(H_0\\) when \\(H_0\\) is false.\n\n↩︎\n\nthe instantaneous rate of the event\n\n↩︎\n\nthe last observation was censored. The reason the hazard is zero is because the width of the time interval is unknown, that is we don’t know when the last event time is.\n\n↩︎\n\nAre assumed to have the same survival chances as uncensored observations\n\n↩︎\n\nhazard ratio\n\n↩︎\n\nrelative risk\n\n↩︎\n\nnegative\n\n↩︎\n\nzero\n\n↩︎\n\nthe ln ratio of the hazard curves is not linear with respect to BP\n\n↩︎\n\nIs used to analyze survival data when individuals in the study are followed for varying lengths of time. and c. Assumes that the relative hazard for a particular variable is always constant\n\n↩︎\n\nthe hazard function changes as a function of weight which is also changing over time\n\n↩︎\n\nthe treatment produces short term benefits only\n\n↩︎\n\nvariable selection\n\n↩︎\n\nFALSE, we’d expect 5% of all null hypotheses to be rejected\n\n↩︎\n\n1 – P(type II error)\n\n↩︎\n\nP(data | \\(H_0\\) is true)\n\n↩︎\n\nType I errors\n\n↩︎\n\nUniformly (equal prob) on 0 to 1\n\n↩︎\n\n(# p-values &gt; 0.5) * 2\n\n↩︎\n\nRound it to 1\n\n↩︎\n\nAssign the value of the m^th adjusted p-value to the (m+1)^th adjusted p-value\n\n↩︎\n\nthe number of null discoveries out of the total discoveries\n\n↩︎\n\nall of the above\n\n↩︎\n\nStorey (q-values)\n\n↩︎\n\nPocock\n\n↩︎\n\nall of the above. We restrict our analysis to a situation of controling the type I error. However, that isn’t why we want to stop early. We want to stop early for the other reasons listed.\n\n↩︎"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Class notes can be found at http://st47s.com/Math150/Notes/.\n\n\n\n\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "handout/HW8_m150_s23.html",
    "href": "handout/HW8_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 8",
    "section": "",
    "text": "understanding of what a KM curve measures\nusing R to plot KM curves and CIs\nconsidering KM curves separately for a categorical explanatory variable\ntesting the null hypothesis of equality of S(t) across an explanatory variable\n\nNote that if you don’t know the R code either check my notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax."
  },
  {
    "objectID": "handout/HW8_m150_s23.html#important",
    "href": "handout/HW8_m150_s23.html#important",
    "title": "Math 150 - Methods in Biostatistics - Homework 8",
    "section": "Important",
    "text": "Important\nThe data are in the files tab on Canvas (in a folder called “data”).\nMake sure you have the survival (for the survival estimates) and the survminer (for the plots) packages installed.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 9, A26\nProvide a brief explanation of why the estimated variance of \\(\\hat{S}_{KM}(0)\\), and hence the standard error of \\(\\hat{S}_{KM}(0)\\), is equal to 0.\n\n\nQ3. Chp 9, E4\nImmediately after a heart transplant, patients are randomly assigned to two treatment therapies to improve recovery from the transplant, therapy 1 and therapy 2. The patients are then followed for up to 5 years after their surgery. Define the time-to-event random variable T as the time (in months) until recovery (the event) after a heart transplant. For each of the following study descriptions that involve T, sketch the graph of the survival curve (or curves) with as much detail as necessary. Please note that parts (a) through (d) are completely independent of each other.\n\nTherapy 1 is not very effective shortly after surgery, but everybody recovers before the study period is over.\nTherapy 2 is very effective shortly after surgery, but becomes less effective after 3 years. Not every patient fully recovers by the end of the study period.\nTwo curves on the same plot: Therapy 1 is consistently more effective than Therapy 2 over time.\nTwo curves on the same plot: Therapy 1 is more effective than Therapy 2 for the first 2.5 years, and then Therapy 2 is more effective than Therapy 1 for the remaining duration of the study.\n\n\n\nQ4. Chp 9, E6\nThe Kaplan-Meier curve in Figure 9.17 (see the text, on Sakai) displays hypothetical estimated survival probabilities of death due to brain cancer, where time (from diagnosis) until death is measured in months.\n\nIs the largest event time censored or complete? How do you know?\nUse the curve to estimate the mean time until death due to brain cancer. [Read page 295 in the text.]\n\n\n\nQ5. Chp 9, E11 Male Fruit Fly Longevity\n(Lots to read in the text about the dataset.)\n\nConstruct the Kaplan-Meier curve with a confidence interval for the Fruitfly data and describe the survival pattern for the fruitflies over time. Use Longevity as the time-to-event variable.\n\nLook at the online notes example (http://st47s.com/Math150/Notes/survival-analysis.html#Rsurv) for how to implement the R code, but the basics of what you need are below. Look up ?ggsurvplot (in the survival package) to see the different confidence interval types.\nNotice that the “response variable” is now two variables!!! And we put them together with the Surv() function The Surv() function takes two arguments. The first argument is the time variable. The second argument is the censoring information. You shouldn’t have to transform any of the variables in the dataset which is provided.\n# look at the fruitfly data after you read it in!\nfruitfly &lt;- read_csv(\"https://pomona.box.com/shared/static/qnsl0sp0twdutz6azidxb5yt37boee7v\",\n                     na=\"*\")\n\nfly_surv &lt;- survfit(Surv(___, ____ ) ~ 1, data=___ )\n\nggsurvplot(___, conf.type = \"___\") + \n  ggtitle(\"___\")\n\nConstruct the Kaplan-Meier curve for the lifetimes of the fruitflies by number of partners, using Partners as the grouping variable. Briefly comment on the observed relationship between survival and number of female partners.\n\nThe R code will be very similar to part (a), but model the survival response variable as a function of the explanatory variable Partners.\nfly_surv_part &lt;- survfit(Surv(___, ____ ) ~ Partners, data=___ )\n\nPerform the log-rank and Wilcoxon tests. Report the test statistics and p-values for both tests. State the conclusions for both tests. If the tests yield different conclusions, briefly explain why. [We will cover these tests on Tuesday in class.]\n\nThe R function which runs the two tests is called survdiff(). It acts on the Surv() response variable broken down by the explanatory variable, Partners.\nThe argument “rho = ___” controls which test you’d like to run. “rho = 0” gives the log-rank test. “rho = 1” gives the Wilcoxon test. Try typing ?survdiff to see the help file.\nsurvdiff(Surv(___,___) ~ ___, data= ___, rho=___)\nsurvdiff(Surv(___,___) ~ ___, data=___, rho=___)\n\npraise()\n\n[1] \"You are finest!\""
  },
  {
    "objectID": "handout/HW9_m150_s23.html",
    "href": "handout/HW9_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 9",
    "section": "",
    "text": "working with hazard functions as measures of survival (S(t) and h(t) are functions of each other!)\nworking with cumulative hazard functions"
  },
  {
    "objectID": "handout/HW9_m150_s23.html#important",
    "href": "handout/HW9_m150_s23.html#important",
    "title": "Math 150 - Methods in Biostatistics - Homework 9",
    "section": "Important",
    "text": "Important\nThe data are in the files tab on Canvas (in a folder called “data”).\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 9, E12 VA Lung Cancer Study\n(Lots to read in the text about the dataset.)\n\nCreate a graph with both Kaplan-Meier curves to compare the survival time (use the variable time) for subjects with the standard and the test chemotherapy treatment. What do you observe about the survival probabilities for the groups of subjects?\nConduct the log-rank test and the Wilcoxon test to compare the survival curves of both treatment groups. Interpret the results.\nIt may be beneficial to incorporate health as a variable in the analysis. Patients with low Karnofsky scores are less healthy than patients with high Karnofsky scores. Create four groups with the Veteran data: trt=1 and Karnofsky score low, trt=1 and Karnofsky score high, trt=2 and Karnofsky score low, and trt=2 and Karnofsky score high. Recall that it is often best to keep sample sizes as equivalent as possible when you determine what is a low or high Karnofsky score. Create a Kaplan-Meier curve for each of the four groups. Conduct the log-rank test and the Wilcoxon test to compare the survival curves of the four groups. (While we have only discussed using these tests to compare two groups, they can easily be extended to more than two groups.) Did incorporating health into your analysis impact your conclusions? [The R syntax works like other modeling we have done, just add the explanatory variables after the tilde: ~ trt + karno2.]\n\n\nVAlung &lt;- read_csv(\"https://pomona.box.com/shared/static/r6hoo1gawopkt0526xvwwze5fl3245de\",\n                     na=\"*\") %&gt;%\n  mutate(karno2 = ifelse(karno &lt;= 60, \"low\", \"high\"))\n\n\n\nQ3. Chp 9, A45\nI’ve included the R code to create a hazard curve (the R code came with your text book and is on Canvas). Note, however, as discussed, the hazard rate is extremely sensitive to each time interval. In lieu of looking at the hazard curve, it is often more informative to look at the cumulative hazard curve (see section 9.9). The values of the estimated hazard function can be seen in the cumulative hazard curve as the jumps at each time event.\nYou may use the code below (the function, below, is called plot.haz() ) or you can use the code (see R code in the class notes, set fun=\"cumhaz\") to plot the cumulative hazard function using ggsurvplot().\nUse the software instructions provided to plot the estimated hazard rates for the college graduation data (see page 311).\n\nplot.haz &lt;- function(KM.obj,plot=\"TRUE\") {\n  ti &lt;- summary(KM.obj)$time\n  di &lt;- summary(KM.obj)$n.event\n  ni &lt;- summary(KM.obj)$n.risk\n\n  #Est Hazard Function\n  est.haz &lt;- 1:(length(ti))\n  for (i in 1:(length(ti)-1))\n    est.haz[i] &lt;- di[i]/(ni[i]*(ti[i+1]-ti[i]))\n  est.haz[length(ti)] &lt;- est.haz[length(ti)-1]\n\n  if (plot==\"TRUE\") {\n    plot(ti,est.haz,type=\"s\",xlab=\"Time\", ylab=\"Hazard Rate\",\n         main=expression(paste(hat(h),(t)[KM])))\n    }\n  #return(list(est.haz=est.haz,time=ti))\n}\n\n\ngrad &lt;- read_csv(\"https://pomona.box.com/shared/static/yigpp4e8dvkyw9pf3f0c7o9nr0rt3k6m\", na=\"*\")\n\n\n\nQ4. Chp 9, A46\nAlthough the estimated hazard curve may not exhibit a distinguishable pattern, discuss some important features of the curve (see pg 311).\n\n\nQ5. Chp 9, A47\nIndicate periods of time during their college career when students are at their lowest and highest risk of graduating college. Does your answer match your common understanding of when students typically graduate from college? (see pg 311)\n\n\nQ6. Chp 9, E9\nSketch hazard functions that would correspond to the following time-to-event random variables (You may want to do a little background research.)\n\nLifetime of an individual measured from birth (don’t assume anything about the health or demographics of this person).\nTime until death after surgery to remove a cancerous tumor.\n\nBe sure to label the time axis, and mark time points appropriately. Briefly explain your reasons for any changes in the shape of the hazard function over time.\n\n\nQ7. Chp 9, E10\nThe graphs displayed in Figure 9.19 (see pg 325) are population cumulative hazard functions for three distributions of the time-to-event random variable, T. For each one, sketch a possible corresponding hazard function h(t). Be sure to label the same time points on your sketches as are provided on the graphs of H(t).\n\npraise()\n\n[1] \"You are peachy!\""
  },
  {
    "objectID": "handout/HW4_m150_s23.html",
    "href": "handout/HW4_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 4",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nfluent use of the logistic model for prediction and for coefficient interpretation\npractice using ggplot() so that visualizations can inform the larger analysis\n\nNote that if you don’t know the R code either check my notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax.\nAlso, note that you’ll need to get the data from Sakai and use it for this analysis. Look back to your own HW1 file to see the line of code you used to import the games1.csv dataset. Ask me if it isn’t obvious to you after you look at your own HW1.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 7, A1\nBased on the description of the Challenger disaster O-ring concerns, identify which variable in the Shuttle data set in Table 7.1 should be the explanatory variable and which should be the response variable.\n\n\nQ3. Chp 7, A2\nImagine you were an engineer working for Thiokol Corporation prior to January 1986. Create a few graphs of the data in Table 7.1. Is it obvious that temperature is related to the success of the O-rings? Submit any charts or graphs you have created that show a potential relationship between temperature and O-ring damage.\nnote: the data is coded with missing values represented by *. You may need to account for that. See how I did it below using na=\"*\". Again, ask me if you are having trouble!\nnote on graphs: if you tell me the type of graph you want, and you don’t know how to make it, ask me and I’ll send you code! Remember, your response is binary and your explanatory variable is continuous.\nnote on data: in order to get the assignment to work, you’ll need the data. Import it into the folder where the HW .Rmd file lives. Try not to use your downloads for everything!!\n\nshuttle &lt;- read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Shuttle.csv\",\n                     na=\"*\")\n\n# new names that make the data easier to work with:\n# mine loads with an empty 5th column\n# so I had to give the 5th column a name, also.\nnames(shuttle) &lt;- c(\"flight\", \"date\", \"temp\", \"launch\", \"X5\")  \n\n# remove the row that has a missing value for launch\n# also create a character variable for success\nshuttle &lt;- shuttle %&gt;% \n  filter(!is.na(launch)) %&gt;%\n  mutate(launchsucc = as.factor(ifelse(launch == 1, \"success\", \"failure\")))\n\n\n\nQ4. Chp 7, A3\nUse the data in Table 7.1 to create a scatterplot with a least squares regression line for the space shuttle data. Calculate the predicted response values (\\(\\hat{y} = b_0 + b_1 x\\)) when the temperature is 60F and when the temperature is 85F.\n\n\nQ5. Chp 7, A4\nSolve Equation (7.5) for \\(\\pi_i\\) to show that Equation (7.6) is true. Note that your text uses \\(\\pi_i\\) to represent the true model (akin to \\(p_i\\) that has been used in class). The difference is only in notation, not in meaning.\n\n\nQ6. Chp 7, A5\nUse Equation (7.6) to create twelve graphs: In each graph plot the explanatory variable (x) versus the expected probability of success (\\(p_i\\)) using the following values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(\\beta_0\\)\n-10\n-10\n-10\n-5\n-5\n-5\n10\n10\n10\n5\n5\n5\n\n\n\\(\\beta_1\\)\n0.5\n1\n1.5\n0.5\n1\n1.5\n-0.5\n-1\n-1.5\n-0.5\n-1\n-1.5\n\n\n\n\nDo not submit the graphs, but explain the impact of changing \\(\\beta_0\\) and \\(\\beta_1\\).\nFor all of the graphs, at what value of \\(\\pi\\) does there appear to be the steepest slope?\n\nI wrote the R code for you (hopefully you can follow along with what it is doing). All you need to do for this problem is change the parameter values and look at the graph. Do not include all the graphs in your assignment, just answer the questions based on your observations.\n\n#set the parameters\nbeta0 &lt;- -10\nbeta1 &lt;- 0.5\nvaluesofX &lt;- seq(0, 40, by=0.01)  # create a vector of X values\n\nprobfunc &lt;- function(b0, b1, ex){\n  exp(b0 + b1*ex) / (1 + exp(b0 + b1*ex))\n}\n\nvaluesofY &lt;- probfunc(beta0, beta1, valuesofX)\n\ndatatoplot &lt;- data.frame(explan = valuesofX, prob = valuesofY)\n\nggplot(datatoplot) + \n  geom_line(aes(x = explan, y = prob))\n\n\n\n\n\n\n\n\n\n\nQ7. Chp 7, A6\n[For the shuttle data:] Use statistical software to calculate the maximum likelihood estimates of \\(\\beta_0\\) and \\(\\beta_1\\). Compare the maximum likelihood estimates to the least squares estimates in A3. Use glm(response ~ explanatory, family = \"binomial\", data = yourdataset) %&gt;% tidy().\n\n\nQ8. Chp 7, A7\nUse Equation (7.9) to predict the probability that a launch has no O-ring damage when the temperature is 31F, 50F, and 75F.\n\n\nQ9. Chp 7, A8\nCalculate the odds of a launch with no O-ring damage when the temperature is 60F and when the temperature is 70F.\n\n\nQ10. Chp 7, A9\nFor the shuttle model above, when \\(x_i\\) increases by 10, state in terms of \\(e^{b_1}\\) how much you would expect the odds to change. (Here you are calculating the odds ratio for an increase in 10 degrees.)\n\n\nQ11. Chp 7, A10\nThe difference between the odds of success at 60F and 59F is about 0.3285 - 0.2605 = 0.068. Would you expect the difference between the odds at 52F and 51F to also be about 0.068? Explain why or why not.\n\n\nQ12. Chp 7, A11\nCreate a plot of two prediction models (one logistic, one linear). Plot temperature versus the estimated probability using maximum likelihood estimates from A6, and plot temperature versus the estimated probability using the least squares estimates from A3.\n\nR code:\nStep1. Look up at probfunc() above. Write a very similar function that is linear instead. Give it a different name.\nStep2. Using the two sets of coefficients (one from the linear and one from the logistic), predict the “y” value for both models for a vector of possible explanatory variables (e.g., valuesofX &lt;- seq(50,85,by=0.01)). You should have two different vectors of predictions (and the vector of X, the explanatory variable).\nStep3. Create a data.frame() with three columns. Let’s say you call it mypredictons. The ggplot code will look like this. Have fun with coloring the plot or changing the line types or something!\nggplot(shuttle) +\n   geom_point(aes(x = temp, y = launch)) + \n   geom_line(data = mypredictions, aes(x = valuesofX, y = yourlinearpreds)) +\n   geom_line(data = mypredictions, aes(x = valuesofX, y = yourlogisticpreds))\n\npraise()\n\n[1] \"You are unreal!\"\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW2_m150_s23.html",
    "href": "handout/HW2_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 2",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nRun a least square regression model, try different transformations on the explanatory and response variables to find a model for which the technical conditions hold.\nAnalyze two different datasets using a simulation method (you will need the infer package) as well as Fisher’s Exact Test\nFor plotting and infer code, see the class notes describing the Botox study:\nclick here to link for boxplots and click here to link for infer for simulating\n\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Hippel-Lindau disease\nEisenhofer et al. (1999) investigated the use of plasma normetanephrine and metanephrine for detecting pheochromocytoma in patients with von Hippel-Lindau disease and multiple endocrine neoplasia type 2. The data set (vonHippelLindau.csv, posted online) contains data from this study on 26 patients with von Hippel-Lindau disease and nine patients with multiple endocrineneoplasia. The variables in the data set are (problem from Dupont, chp 2.22, PubMed article at [http://www.ncbi.nlm.nih.gov/pubmed/10369850]):\nNote: the goal is to model p_ne (the response variable) from tumorvol (the explanatory variable).\n\n\n\nvariable\nunits\n\n\n\n\ndisease\n0: patient has von Hippel-Lindau disease\n\n\n\n1: patient has multiple endocrine neoplasia type 2\n\n\np_ne\nplasma norepinephrine (pg/ml)\n\n\ntumorvol\ntumor volume (ml)\n\n\n\nNote: the data this week is imported from the internet, so everyone can use the same link! The directories below do not go to my own computer, they go to a URL pointing to a dataset in the cloud.\n\ntumor &lt;- readr::read_csv(\"http://pages.pomona.edu/~jsh04747/courses/math150/vonHippelLindau.csv\")\nhead(tumor, 3)\n\n# A tibble: 3 × 4\n  disease    id  p_ne tumorvol\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1       0     2  1845      336\n2       0     3  1734      216\n3       0     4   739      128\n\n\n\nRegress plasma norepinephrine against tumor volume. Draw a scatter plot of norepinephrine against tumor volume together with the estimated linear regression curve. What is the slope estimate for this regression? What proportion of the total variation in norepinephrine levels is explained by the regression?\n\n\nR hints:\nIf the linear model is piped into tidy(), the output will be important information on a per parameter basis. For example, coefficients, standard errors, etc.\nIf the linear model is piped into glance(), the output will be important information on a per model basis. For example, \\(R^2\\), overall model p-value, model degrees of freedom, etc.\nIf the linear model is piped into augment(), the output will be be important information on a per observation basis. For example, residuals (.resid), fitted values / predicted values (.fitted), etc.\nTo make a plot in R you want to add a series of layers. The code below is meant as an example, although the variables are totally wrong. Work through the lines of code below and see if you can follow. If you don’t follow the lines, ask me!\n\ntumor %&gt;%                                 # which dataset?\n  ggplot(aes(x = id, y = tumorvol)) +     # set up the plot\n  geom_point() +                          # add the points\n  geom_smooth(method = \"lm\", se = FALSE)  # add the line a linear model without error bounds\n\n\n\n\n\n\n\n\n\nExperiment with different transformations of norepinephrine and tumor volume. Find transformations that provide a good fit to a linear model. Report your new linear model. What is your new \\(R^2\\)? Does the \\(R^2\\) matter in choosing your transformation? Explain.\n\n\n\nR hints:\nFirst transform one or both of your variables (see pg 49 in your text), then re-plot the data. Below is an example, but it turns out that I made a bad choice of transformation because the plot is terrible. Why (what makes the plot look bad)?\n\ntumor %&gt;%\n  ggplot(aes(x = 1/tumorvol, y = p_ne)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nQ3. Regression Conditions\nWhich of the following conditions are required to test hypotheses using simple linear regression? If the condition isn’t valid, explain why not.\n\nThe random variable \\(Y\\) (not conditional on \\(X\\)) is normally distributed.\nThe variance of \\(Y\\) depends on \\(X\\).\nThe random variable \\(Y\\) is normally distributed at each value of \\(X\\).\nThe mean of \\(Y\\) (given \\(X\\)) is a linear function of \\(X\\).\nThe random variable \\(X\\) is randomly distributed on some scale.\n\n\n\nQ4. Chp 6, E1: Cancer and Smoking: Fisher’s Exact Test and Simulations Studies\nAnswer the following questions for the data displayed below. Hint: see the class notes for help with the R code. And ask lots of questions!\n\n\n\n\nlung cancer\nhealthy\n\n\n\n\n\nsmoker\n41\n28\n69\n\n\nnon-smoker\n19\n32\n51\n\n\n\n60\n60\n120\n\n\n\n\nsmokecancer &lt;- data.frame(act = c(rep(\"non-smoker\", 51), rep(\"smoker\", 69)),\n                     outcome = c(rep(\"lung_cancer\", 19), rep(\"healthy\", 32), \n                                 rep(\"lung_cancer\", 41), rep(\"healthy\", 28)))\nsmokecancer %&gt;% table()\n\n            outcome\nact          healthy lung_cancer\n  non-smoker      32          19\n  smoker          28          41\n\n\n\nWas either the explanatory variable (row) or the response (column) variable fixed before the study was conducted?\nIs this an example of an experiment or an observational study?\nIs this a cross-classification, cohort, or case-control study? Explain.\nCreated a segmented bar chart for the data.\nCreate a simulation study to test the one-sided hypothesis that smokers are more likely to have lung cancer. Provide a p-value and state your conclusions.\nUse Fisher’s exact test to test the one-sided hypothesis that smokers are more likely to have lung cancer. Provide a p-value and state your conclusions.\n\n\npraise()\n\n[1] \"You are laudable!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW6_m150_s23.html",
    "href": "handout/HW6_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 6",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nconsiderations of working with variables\nfluent use of the multiple logistic model for prediction using the tidymodels framework\n\nNote that if you don’t know the R code either check the class notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax.\nAlso, note that you’ll need to get the data from Canvas and use it for this analysis. Look back to your own HW1 file to see the line of code you used to import the games1.csv dataset. Ask me if it isn’t obvious to you after you look at your own HW1.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. And the Winner Is…\nThe data this week come from kaggle as a compilation form Academy Award winners since the award began. https://www.kaggle.com/datasets/unanimad/the-oscar-award\nThe data wrangling below are an effort to consolidate the labels so that the variables measure the same over basic quality over the last 95 years. https://www.verdict.co.uk/oscars-90-whats-changed-years-oscars-records/\nThe dataset is in the files tab on Canvas (in a folder called “data”).\n\n# the only thing you should change in this R chunk is \n# the path of the dataset\nthe_oscar &lt;- readr::read_csv(\"~/Dropbox/teaching/MA150/the_oscar_award.csv\") %&gt;%\n  rename(year = year_ceremony) %&gt;%\n  filter(year != 2022) %&gt;%\n  select(-year_film, -ceremony, -name) %&gt;%\n  drop_na() %&gt;%\n  mutate(winner = case_when(\n    winner == TRUE ~ 1,\n    winner == FALSE ~ 0\n  )) %&gt;%\n  distinct(category, film, .keep_all = TRUE) %&gt;%\n  mutate(category = case_when(\n    category == \"OUTSTANDING PICTURE\" ~ \"BEST PICTURE\",\n    category == \"OUTSTANDING MOTION PICTURE\" ~ \"BEST PICTURE\",\n    category == \"OUTSTANDING PRODUCTION\" ~ \"BEST PICTURE\",\n    category == \"BEST MOTION PICTURE\" ~ \"BEST PICTURE\",\n    category == \"ACTOR\" ~ \"ACTOR IN A LEADING ROLE\",\n    category == \"ACTRESS\" ~ \"ACTRESS IN A LEADING ROLE\",\n    category == \"INTERNATIONAL FEATURE FILM\" ~ \"FOREIGN LANGUAGE FILM\",\n    str_detect(category, \"DIRECTING\") ~ \"DIRECTING\",\n    str_detect(category, \"WRITING\") ~ \"WRITING\",\n    str_detect(category, \"VISUAL EFFECTS\") ~ \"VISUAL EFFECTS\",\n    str_detect(category, \"SOUND\") ~ \"SOUND\",\n    str_detect(category, \"SHORT SUBJECT\") ~ \"SHORT SUBJECT\",\n    str_detect(category, \"SHORT FILM\") ~ \"SHORT FILM\",\n    str_detect(category, \"MUSIC\") ~ \"MUSIC\",\n    str_detect(category, \"MAKEUP\") ~ \"MAKEUP\",\n    str_detect(category, \"DOCUMENTARY\") ~ \"DOCUMENTARY\",\n    str_detect(category, \"COSTUME DESIGN\") ~ \"COSTUME DESIGN\",\n    str_detect(category, \"CINEMATOGRAPHY\") ~ \"CINEMATOGRAPHY\",\n    str_detect(category, \"ART DIRECTION\") ~ \"ART DIRECTION\",\n    str_detect(category, \"PRODUCTION\") ~ \"PRODUCTION\",\n    TRUE ~ category)) %&gt;%\n  filter(!(category %in% c(\"OUTSTANDING PICUTRE\",\n                         \"UNIQUE AND ARTISTIC PICTURE\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Effects)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Visual Effects)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Effects Editing)\",\n                         \"SPECIAL ACHIEVEMENT AWARD (Sound Editing)\",\n                         \"ENGINEERING EFFECTS\",\n                         \"DANCE DIRECTION\",\n                         \"ASSISTANT DIRECTOR\"\n                         ))) %&gt;%\n  group_by(film, category) %&gt;%\n  arrange(desc(winner)) %&gt;%\n  filter(row_number() == 1) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(id_cols = c(\"year\", \"film\"),\n              names_from = category, values_from = winner,\n              values_fill = 0) %&gt;%\n  janitor::clean_names() %&gt;%\n  mutate(best_picture = as.factor(ifelse(best_picture == 1, \"win\", \"not win\")))\n\n\nCreate a logistic regression model using all the explanatory variables except film (which is the title of the film).\n\nUse the framework from tidymodels which consists of the following steps:\n\nsplit the data into test training. you likely want to stratify on best_picture (because the dataset is very imbalanced).\nbuild a recipe. to communicate that you don’t consider film to be an identifier, add the following step to your recipe: update_role(film, new_role = \"ID\")\nset the model to be logistic regression\nfit the model on the training data\ntidy the model to see the coefficients / p-values\n\n\nThe glm.fit likely said that the fitted probabilities were numerically 0 or 1. Check the following three probability of best picture win: Cinema Paradiso, Parasite, The Hurt Locker (you’ll likely need to Google / look at wikipedia).\nUse the test data to predict whether or not each of the test movies will win an Academy Award for Best Picture. Summarize using accuracy, sensitivity, and specificity (here “successes” is “win”). Feel free to also make a plot to describe the table of predictions.\n\nNot due, but maybe fun? (d) Is your model able to predict the winner for best picture from 2022? (The dataset only goes up until 2021, so you’ll need to find the relevant information from the Google.)\n\n\nQ3. And the Winner Is…\nContinue to use the data on the Academy Awards. Using 10-fold Cross Validation, compare the following two models:\n\nA model based only on the four actor awards, makeup, and music\nA model based only on directing and writing\n\n\nBuild the models on the training data only using cross validation. (Note: in the formula add the variables and also add + film before updating the film role to be only an ID.)\nCross validate to assess which model is better. Choose a model based on overall accuracy. Commit to that model using only the training data. Which model did you choose?\nWhat do you think the accuracy of the model (from part (b)) will be when you apply your model in the real world? That is, use the variables from part (b), train the model using the training data, test the model using the test data.\n\n\n\nQ4. Voting\nAnother dataset which could have been used for this HW set was a dataset that comes from 538 on voting behavior. See the information about the dataset here: https://github.com/fivethirtyeight/data/tree/master/non-voters and the article about it here: https://projects.fivethirtyeight.com/non-voters-poll-2020-election/.\nIf you want, you can also import the data into R using the following code.\n\nvoters &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")\n\nConsider the following variables:\n\nQ21: do you plan to vote in Nov 2020?\nQ30: which political party do you consider yourself aligned with?\nINCOME_CAT: household income category\n\nFor each variable, explain (in words, no code here) how the variable would need to be transformed to be able to be used in the logistic regression model.\n\npraise()\n\n[1] \"You are wicked!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html",
    "href": "handout/2023-02-28-ROC-onestep.html",
    "title": "ROC + Adding Variables",
    "section": "",
    "text": "Metric: Receiver Operating Characteristic (ROC) Curves\nMetrics: AIC & BIC\nModel building one variables at a time"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#agenda",
    "href": "handout/2023-02-28-ROC-onestep.html#agenda",
    "title": "ROC + Adding Variables",
    "section": "",
    "text": "Metric: Receiver Operating Characteristic (ROC) Curves\nMetrics: AIC & BIC\nModel building one variables at a time"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#make-predictions-for-test-data",
    "href": "handout/2023-02-28-ROC-onestep.html#make-predictions-for-test-data",
    "title": "ROC + Adding Variables",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_test %&gt;% select(squirrels))\n\nfeeder_test_pred\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# … with 58,912 more rows"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#accuracy",
    "href": "handout/2023-02-28-ROC-onestep.html#accuracy",
    "title": "ROC + Adding Variables",
    "section": "Accuracy",
    "text": "Accuracy\n\nrbind(accuracy(feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      roc_auc(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_squirrels, event_level = \"second\"))\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n4 roc_auc     binary        0.721"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#roc-curves",
    "href": "handout/2023-02-28-ROC-onestep.html#roc-curves",
    "title": "ROC + Adding Variables",
    "section": "ROC Curves",
    "text": "ROC Curves\nReceiver Operating Characteristic Curves\n\nTPR = sensitivity = # of true predicted true / # true\nFPR = 1 - specificity = # false predicted true / # false\n\nHow can the plot use many TPRs and FPRs ???"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#roc-curve",
    "href": "handout/2023-02-28-ROC-onestep.html#roc-curve",
    "title": "ROC + Adding Variables",
    "section": "ROC Curve",
    "text": "ROC Curve"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#visualizing-accuracy",
    "href": "handout/2023-02-28-ROC-onestep.html#visualizing-accuracy",
    "title": "ROC + Adding Variables",
    "section": "Visualizing accuracy",
    "text": "Visualizing accuracy\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n4 roc_auc     binary        0.721"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#different-cut-off-0.8",
    "href": "handout/2023-02-28-ROC-onestep.html#different-cut-off-0.8",
    "title": "ROC + Adding Variables",
    "section": "Different cut-off = 0.8?",
    "text": "Different cut-off = 0.8?\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.8,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_test %&gt;% select(squirrels))\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.675\n2 sensitivity binary         0.683\n3 specificity binary         0.641\n4 roc_auc     binary         0.721"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#different-cut-off-0.15",
    "href": "handout/2023-02-28-ROC-onestep.html#different-cut-off-0.15",
    "title": "ROC + Adding Variables",
    "section": "Different cut-off = 0.15?",
    "text": "Different cut-off = 0.15?\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.15,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_test %&gt;% select(squirrels))\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary       0.807  \n2 sensitivity binary       0.999  \n3 specificity binary       0.00141\n4 roc_auc     binary       0.721"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#roc-curve-1",
    "href": "handout/2023-02-28-ROC-onestep.html#roc-curve-1",
    "title": "ROC + Adding Variables",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nroc_auc(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_squirrels, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.721\n\n\n\n\n\n\n\nTo get a single value (for model comparison), we use the area under the ROC curve."
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#aoc",
    "href": "handout/2023-02-28-ROC-onestep.html#aoc",
    "title": "ROC + Adding Variables",
    "section": "AOC",
    "text": "AOC\n\n\n\n\n\nAlexandria Ocasio-Cortez as Area Over the Curve. I didn’t make the image, and I don’t know who did!"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#more-metrics",
    "href": "handout/2023-02-28-ROC-onestep.html#more-metrics",
    "title": "ROC + Adding Variables",
    "section": "More metrics",
    "text": "More metrics\n\nAIC: Akaike’s Information Criteria = \\(-2\\ln L + 2p\\)\nBIC: Bayesian Information Criteria = \\(-2 \\ln L + p \\ln(n)\\)\n\n\nchoose a model with the smallest AIC or BIC"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#tools",
    "href": "handout/2023-02-28-ROC-onestep.html#tools",
    "title": "ROC + Adding Variables",
    "section": "Tools 😖",
    "text": "Tools 😖\n\ntidymodels does not make it easy to add or drop 1 variable at a time.\nadd1() and drop1() functions do not make it easy to work with dozens of predictors and missing data.\n\nTherefore, we’ll go back to the bird data from HW 5.\n\n\nLocation\n  bank  conif  decid ground  shrub   snag   wall \n     3     14     25     19     17      4      4"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#forward-1",
    "href": "handout/2023-02-28-ROC-onestep.html#forward-1",
    "title": "ROC + Adding Variables",
    "section": "Forward +1",
    "text": "Forward +1\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ 1\n         Df Deviance     AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       108.533 110.533                      \nLength    1  105.296 109.296  3.2373 0.0719792 .  \nLocation  6   77.065  91.065 31.4684 2.063e-05 ***\nNo.eggs   1   90.951  94.951 17.5816 2.752e-05 ***\nColor     1  108.087 112.087  0.4463 0.5041175    \nIncubate  1  108.267 112.267  0.2658 0.6061875    \nNestling  1   93.825  97.825 14.7078 0.0001255 ***\nTotcare   1   98.964 102.964  9.5688 0.0019791 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#forward-2",
    "href": "handout/2023-02-28-ROC-onestep.html#forward-2",
    "title": "ROC + Adding Variables",
    "section": "Forward +2",
    "text": "Forward +2\n\nglm(`Closed?` ~ Location, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ Location\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        77.065 91.065                      \nLength    1   71.704 87.704  5.3605    0.0206 *  \nNo.eggs   1   61.211 77.211 15.8530 6.846e-05 ***\nColor     1   74.758 90.758  2.3070    0.1288    \nIncubate  1   74.829 90.829  2.2355    0.1349    \nNestling  1   74.722 90.722  2.3425    0.1259    \nTotcare   1   76.635 92.635  0.4300    0.5120    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#forward-3",
    "href": "handout/2023-02-28-ROC-onestep.html#forward-3",
    "title": "ROC + Adding Variables",
    "section": "Forward +3",
    "text": "Forward +3\n\nglm(`Closed?` ~ No.eggs + Location, data = nests, family=\"binomial\") %&gt;%\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ No.eggs + Location\n         Df Deviance    AIC     LRT Pr(&gt;Chi)  \n&lt;none&gt;        61.211 77.211                   \nLength    1   58.229 76.229 2.98230  0.08418 .\nColor     1   59.925 77.925 1.28650  0.25669  \nIncubate  1   59.891 77.891 1.32019  0.25056  \nNestling  1   59.247 77.247 1.96461  0.16102  \nTotcare   1   60.751 78.751 0.46084  0.49723  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#backward--1",
    "href": "handout/2023-02-28-ROC-onestep.html#backward--1",
    "title": "ROC + Adding Variables",
    "section": "Backward -1",
    "text": "Backward -1\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling + Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)   \n&lt;none&gt;        46.252 70.252                    \nLength    1   52.812 74.812  6.5600 0.010430 * \nLocation  6   66.017 78.017 19.7648 0.003049 **\nNo.eggs   1   56.049 78.049  9.7973 0.001748 **\nColor     1   46.997 68.997  0.7457 0.387857   \nIncubate  0   46.252 70.252  0.0000            \nNestling  0   46.252 70.252  0.0000            \nTotcare   0   46.252 70.252  0.0000            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#backward--2",
    "href": "handout/2023-02-28-ROC-onestep.html#backward--2",
    "title": "ROC + Adding Variables",
    "section": "Backward -2",
    "text": "Backward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate  + Totcare, data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)   \n&lt;none&gt;        46.252 70.252                    \nLength    1   52.812 74.812  6.5600 0.010430 * \nLocation  6   66.017 78.017 19.7648 0.003049 **\nNo.eggs   1   56.049 78.049  9.7973 0.001748 **\nColor     1   46.997 68.997  0.7457 0.387857   \nIncubate  1   49.031 71.031  2.7796 0.095472 . \nTotcare   1   56.989 78.989 10.7368 0.001050 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#backward--2-1",
    "href": "handout/2023-02-28-ROC-onestep.html#backward--2-1",
    "title": "ROC + Adding Variables",
    "section": "Backward -2",
    "text": "Backward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Incubate  + Totcare, \n    data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Incubate + Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;        46.997 68.997                     \nLength    1   53.878 73.878  6.8809 0.008712 ** \nLocation  6   66.664 76.664 19.6663 0.003175 ** \nNo.eggs   1   57.418 77.418 10.4201 0.001247 ** \nIncubate  1   49.839 69.839  2.8416 0.091854 .  \nTotcare   1   58.227 78.227 11.2297 0.000805 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#backward--3",
    "href": "handout/2023-02-28-ROC-onestep.html#backward--3",
    "title": "ROC + Adding Variables",
    "section": "Backward -3",
    "text": "Backward -3\n\nglm(`Closed?` ~ Length + Location + No.eggs + Totcare, \n    data = nests, family=\"binomial\") %&gt;%\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Totcare\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        49.839 69.839                      \nLength    1   60.751 78.751 10.9116 0.0009556 ***\nLocation  6   69.236 77.236 19.3974 0.0035425 ** \nNo.eggs   1   61.940 79.940 12.1013 0.0005039 ***\nTotcare   1   58.229 76.229  8.3902 0.0037725 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#automatic-forward-w-aic",
    "href": "handout/2023-02-28-ROC-onestep.html#automatic-forward-w-aic",
    "title": "ROC + Adding Variables",
    "section": "Automatic: Forward w AIC",
    "text": "Automatic: Forward w AIC\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") %&gt;%\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, \n         direction = \"forward\", k = 2)\n\nStart:  AIC=110.53\n`Closed?` ~ 1\n\n           Df Deviance     AIC\n+ Location  6   77.065  91.065\n+ No.eggs   1   90.951  94.951\n+ Nestling  1   93.825  97.825\n+ Totcare   1   98.964 102.964\n+ Length    1  105.296 109.296\n&lt;none&gt;         108.533 110.533\n+ Color     1  108.087 112.087\n+ Incubate  1  108.267 112.267\n\nStep:  AIC=91.06\n`Closed?` ~ Location\n\n           Df Deviance    AIC\n+ No.eggs   1   61.211 77.211\n+ Length    1   71.704 87.704\n+ Nestling  1   74.722 90.722\n+ Color     1   74.758 90.758\n+ Incubate  1   74.829 90.829\n&lt;none&gt;          77.065 91.065\n+ Totcare   1   76.635 92.635\n\nStep:  AIC=77.21\n`Closed?` ~ Location + No.eggs\n\n           Df Deviance    AIC\n+ Length    1   58.229 76.229\n&lt;none&gt;          61.211 77.211\n+ Nestling  1   59.247 77.247\n+ Incubate  1   59.891 77.891\n+ Color     1   59.925 77.925\n+ Totcare   1   60.751 78.751\n\nStep:  AIC=76.23\n`Closed?` ~ Location + No.eggs + Length\n\n           Df Deviance    AIC\n+ Nestling  1   47.292 67.292\n+ Totcare   1   49.839 69.839\n&lt;none&gt;          58.229 76.229\n+ Color     1   56.989 76.989\n+ Incubate  1   58.227 78.227\n\nStep:  AIC=67.29\n`Closed?` ~ Location + No.eggs + Length + Nestling\n\n           Df Deviance    AIC\n&lt;none&gt;          47.292 67.292\n+ Color     1   46.580 68.580\n+ Incubate  1   46.997 68.997\n+ Totcare   1   46.997 68.997\n\n\n\nCall:  glm(formula = `Closed?` ~ Location + No.eggs + Length + Nestling, \n    family = \"binomial\", data = nests)\n\nCoefficients:\n   (Intercept)   Locationconif   Locationdecid  Locationground   Locationshrub  \n       11.1085        -19.2865        -16.8603        -20.5222        -18.6448  \n  Locationsnag    Locationwall         No.eggs          Length        Nestling  \n        0.6949        -18.3127          0.7950         -0.2194          0.3983  \n\nDegrees of Freedom: 85 Total (i.e. Null);  76 Residual\nNull Deviance:      108.5 \nResidual Deviance: 47.29    AIC: 67.29"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#final-forward-aic",
    "href": "handout/2023-02-28-ROC-onestep.html#final-forward-aic",
    "title": "ROC + Adding Variables",
    "section": "Final Forward AIC",
    "text": "Final Forward AIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Nestling,\n    data = nests, family=\"binomial\") %&gt;% tidy()\n\n# A tibble: 10 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      11.1   3328.      0.00334  0.997  \n 2 Length           -0.219    0.0754 -2.91     0.00364\n 3 Locationconif   -19.3   3328.     -0.00580  0.995  \n 4 Locationdecid   -16.9   3328.     -0.00507  0.996  \n 5 Locationground  -20.5   3328.     -0.00617  0.995  \n 6 Locationshrub   -18.6   3328.     -0.00560  0.996  \n 7 Locationsnag      0.695 4313.      0.000161 1.00   \n 8 Locationwall    -18.3   3328.     -0.00550  0.996  \n 9 No.eggs           0.795    0.262   3.04     0.00238\n10 Nestling          0.398    0.144   2.76     0.00577"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#automatic-backward-w-bic",
    "href": "handout/2023-02-28-ROC-onestep.html#automatic-backward-w-bic",
    "title": "ROC + Adding Variables",
    "section": "Automatic: Backward w BIC",
    "text": "Automatic: Backward w BIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare,\n    data = nests, family=\"binomial\") %&gt;%\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling + Totcare, \n         direction = \"backward\", k = log(86))\n\nStart:  AIC=99.7\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling + Totcare\n\n\nStep:  AIC=99.7\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling\n\n           Df Deviance     AIC\n- Location  6   66.017  92.743\n- Incubate  1   46.580  95.577\n- Color     1   46.997  95.995\n&lt;none&gt;          46.252  99.704\n- Length    1   52.812 101.810\n- No.eggs   1   56.049 105.047\n- Nestling  1   56.989 105.986\n\nStep:  AIC=92.74\n`Closed?` ~ Length + No.eggs + Color + Incubate + Nestling\n\n           Df Deviance     AIC\n- Incubate  1   66.175  88.447\n- Color     1   66.664  88.935\n&lt;none&gt;          66.017  92.743\n- No.eggs   1   74.635  96.907\n- Length    1   75.000  97.272\n- Nestling  1   85.891 108.163\n\nStep:  AIC=88.45\n`Closed?` ~ Length + No.eggs + Color + Nestling\n\n           Df Deviance     AIC\n- Color     1   66.762  84.579\n&lt;none&gt;          66.175  88.447\n- No.eggs   1   75.577  93.395\n- Length    1   79.115  96.932\n- Nestling  1   89.064 106.881\n\nStep:  AIC=84.58\n`Closed?` ~ Length + No.eggs + Nestling\n\n           Df Deviance     AIC\n&lt;none&gt;          66.762  84.579\n- No.eggs   1   76.704  90.067\n- Length    1   79.162  92.525\n- Nestling  1   90.053 103.416\n\n\n\nCall:  glm(formula = `Closed?` ~ Length + No.eggs + Nestling, family = \"binomial\", \n    data = nests)\n\nCoefficients:\n(Intercept)       Length      No.eggs     Nestling  \n    -6.7711      -0.1871       0.6476       0.4062  \n\nDegrees of Freedom: 85 Total (i.e. Null);  82 Residual\nNull Deviance:      108.5 \nResidual Deviance: 66.76    AIC: 74.76"
  },
  {
    "objectID": "handout/2023-02-28-ROC-onestep.html#final-backward-bic",
    "href": "handout/2023-02-28-ROC-onestep.html#final-backward-bic",
    "title": "ROC + Adding Variables",
    "section": "Final Backward BIC",
    "text": "Final Backward BIC\n\nglm(`Closed?` ~ Length + No.eggs + Nestling,\n    data = nests, family=\"binomial\") %&gt;% tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -6.77     1.73       -3.90 0.0000946\n2 Length        -0.187    0.0598     -3.13 0.00177  \n3 No.eggs        0.648    0.245       2.65 0.00815  \n4 Nestling       0.406    0.107       3.78 0.000156"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html",
    "href": "handout/2023-02-21-tidymodels.html",
    "title": "Tidymodels",
    "section": "",
    "text": "Workflow to help us think about model building\nBreaking up data for independent assessment\nAssessment metrics\n\n\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#agenda",
    "href": "handout/2023-02-21-tidymodels.html#agenda",
    "title": "Tidymodels",
    "section": "",
    "text": "Workflow to help us think about model building\nBreaking up data for independent assessment\nAssessment metrics\n\n\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#feeders",
    "href": "handout/2023-02-21-tidymodels.html#feeders",
    "title": "Tidymodels",
    "section": "Feeders",
    "text": "Feeders\nHow are other characteristics related to the presence of squirrels?\n\nsite_data %&gt;%\n  filter(!is.na(squirrels)) %&gt;%\n  group_by(squirrels) %&gt;%\n  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  squirrels    nearby_feeders\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 no squirrels          0.344\n2 squirrels             0.456"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#other-variables",
    "href": "handout/2023-02-21-tidymodels.html#other-variables",
    "title": "Tidymodels",
    "section": "Other variables",
    "text": "Other variables\nWhat about some of the variables describing the habitat?\n\nsite_data %&gt;%\n  filter(!is.na(squirrels)) %&gt;%\n  group_by(squirrels) %&gt;%\n  summarise(across(contains(\"hab\"), mean, na.rm = TRUE)) %&gt;%\n  pivot_longer(contains(\"hab\")) %&gt;%\n  mutate(name = str_remove(name, \"hab_\")) %&gt;%\n  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +\n  geom_col(alpha = 0.8, position = \"dodge\") +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"% of locations\", y = NULL, fill = NULL)\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(contains(\"hab\"), mean, na.rm = TRUE)`.\nℹ In group 1: `squirrels = no squirrels`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#train-test",
    "href": "handout/2023-02-21-tidymodels.html#train-test",
    "title": "Tidymodels",
    "section": "Train / test",
    "text": "Train / test\nCreate an initial split:\n\nset.seed(470)\nfeeder_split &lt;- site_data %&gt;%\n  initial_split(strata = squirrels) # prop = 3/4 in each group, by default\n\nSave training data\n\nfeeder_train &lt;- training(feeder_split)\ndim(feeder_train)\n\n[1] 176763     59\n\n\nSave testing data\n\nfeeder_test  &lt;- testing(feeder_split)\ndim(feeder_test)\n\n[1] 58922    59"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#training-data",
    "href": "handout/2023-02-21-tidymodels.html#training-data",
    "title": "Tidymodels",
    "section": "Training data",
    "text": "Training data\n\nfeeder_train\n\n# A tibble: 176,763 × 59\n   squirrels    yard_type_pavement yard_type_garden yard_type_landsca\n   &lt;fct&gt;                     &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 no squirrels                  0                0                 1\n 2 no squirrels                  0                0                 1\n 3 no squirrels                  0                0                 1\n 4 no squirrels                  0                0                 1\n 5 no squirrels                  0                0                 1\n 6 no squirrels                  0                0                 1\n 7 no squirrels                  0                0                 0\n 8 no squirrels                  0                0                 1\n 9 no squirrels                  0                0                 1\n10 no squirrels                  0                0                 1\n# ℹ 176,753 more rows\n# ℹ 55 more variables: yard_type_woods &lt;dbl&gt;, yard_type_desert &lt;dbl&gt;,\n#   hab_dcid_woods &lt;dbl&gt;, hab_evgr_woods &lt;dbl&gt;, hab_mixed_woods &lt;dbl&gt;,\n#   hab_orchard &lt;dbl&gt;, hab_park &lt;dbl&gt;, hab_water_fresh &lt;dbl&gt;,\n#   hab_water_salt &lt;dbl&gt;, hab_residential &lt;dbl&gt;, hab_industrial &lt;dbl&gt;,\n#   hab_agricultural &lt;dbl&gt;, hab_desert_scrub &lt;dbl&gt;, hab_young_woods &lt;dbl&gt;,\n#   hab_swamp &lt;dbl&gt;, hab_marsh &lt;dbl&gt;, evgr_trees_atleast &lt;dbl&gt;, …"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#feature-engineering",
    "href": "handout/2023-02-21-tidymodels.html#feature-engineering",
    "title": "Tidymodels",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#modeling-workflow",
    "href": "handout/2023-02-21-tidymodels.html#modeling-workflow",
    "title": "Tidymodels",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#initiate-a-recipe",
    "href": "handout/2023-02-21-tidymodels.html#initiate-a-recipe",
    "title": "Tidymodels",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  )\n\nfeeder_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#working-with-recipes",
    "href": "handout/2023-02-21-tidymodels.html#working-with-recipes",
    "title": "Tidymodels",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n. . .\n\n\n\n\n\n\nNote\n\n\n\nUsing prep() and bake() are shown here for demonstrative purposes. They do not need to be a part of your pipeline. I do find them assuring, however, so that I can see the effects of the recipe steps as the recipe is built."
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#impute-missing-values-remove-zero-variance-predictors",
    "href": "handout/2023-02-21-tidymodels.html#impute-missing-values-remove-zero-variance-predictors",
    "title": "Tidymodels",
    "section": "Impute missing values & Remove zero variance predictors",
    "text": "Impute missing values & Remove zero variance predictors\nImpute missing values (replace with mean).\nRemove all predictors that contain only a single value\n\nfeeder_rec &lt;- feeder_rec %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#prep-and-bake",
    "href": "handout/2023-02-21-tidymodels.html#prep-and-bake",
    "title": "Tidymodels",
    "section": "Prep and bake",
    "text": "Prep and bake\n\nfeeder_rec_trained &lt;- prep(feeder_rec)\n\nbake(feeder_rec_trained, feeder_train) %&gt;%\n  glimpse()\n\nRows: 176,763\nColumns: 59\n$ yard_type_pavement           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_garden             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_landsca            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ yard_type_woods              &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,…\n$ yard_type_desert             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hab_dcid_woods               &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_evgr_woods               &lt;dbl&gt; 0.223341, 0.223341, 0.000000, 0.223341, 0…\n$ hab_mixed_woods              &lt;dbl&gt; 1.000000, 1.000000, 1.000000, 0.655411, 0…\n$ hab_orchard                  &lt;dbl&gt; 0.09324933, 0.09324933, 0.00000000, 0.093…\n$ hab_park                     &lt;dbl&gt; 0.4502185, 0.4502185, 0.0000000, 0.450218…\n$ hab_water_fresh              &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_water_salt               &lt;dbl&gt; 0.05016023, 0.05016023, 0.00000000, 0.050…\n$ hab_residential              &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_industrial               &lt;dbl&gt; 0.2215792, 0.2215792, 0.0000000, 0.221579…\n$ hab_agricultural             &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 0.409835…\n$ hab_desert_scrub             &lt;dbl&gt; 0.09257371, 0.09257371, 0.00000000, 0.092…\n$ hab_young_woods              &lt;dbl&gt; 0.349782, 0.349782, 0.000000, 0.349782, 0…\n$ hab_swamp                    &lt;dbl&gt; 0.2917369, 0.2917369, 0.0000000, 0.291736…\n$ hab_marsh                    &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 0.175361…\n$ evgr_trees_atleast           &lt;dbl&gt; 11, 11, 11, 0, 11, 1, 4, 4, 4, 1, 1, 1, 1…\n$ evgr_shrbs_atleast           &lt;dbl&gt; 4, 4, 1, 4, 0, 1, 4, 4, 4, 1, 0, 4, 4, 11…\n$ dcid_trees_atleast           &lt;dbl&gt; 11, 11, 1, 4, 1, 4, 11, 11, 4, 1, 1, 1, 1…\n$ dcid_shrbs_atleast           &lt;dbl&gt; 4, 4, 4, 1, 1, 4, 11, 11, 1, 1, 0, 4, 4, …\n$ fru_trees_atleast            &lt;dbl&gt; 4.00000, 4.00000, 1.00000, 1.00000, 0.000…\n$ cacti_atleast                &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.000000…\n$ brsh_piles_atleast           &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 4, 1, 0, 0, 1, 1, 1,…\n$ water_srcs_atleast           &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 0.000000…\n$ bird_baths_atleast           &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 1, 4, 0, 0, 0, 0, 1,…\n$ nearby_feeders               &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,…\n$ cats                         &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,…\n$ dogs                         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,…\n$ humans                       &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ housing_density              &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, 2, 1, 2, 2,…\n$ fed_in_jan                   &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ fed_in_feb                   &lt;dbl&gt; 1.000000, 1.000000, 1.000000, 1.000000, 1…\n$ fed_in_mar                   &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ fed_in_apr                   &lt;dbl&gt; 1.0000000, 1.0000000, 0.0000000, 1.000000…\n$ fed_in_may                   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_jun                   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_jul                   &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 1.000000, 1…\n$ fed_in_aug                   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_sep                   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_oct                   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_nov                   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ fed_in_dec                   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ numfeeders_suet              &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 2, 3, 4, 1,…\n$ numfeeders_ground            &lt;dbl&gt; 0.000000, 0.000000, 1.270686, 1.000000, 0…\n$ numfeeders_hanging           &lt;dbl&gt; 1.000000, 1.000000, 2.707598, 2.000000, 2…\n$ numfeeders_platfrm           &lt;dbl&gt; 1.000000, 1.000000, 1.033629, 1.033629, 0…\n$ numfeeders_humming           &lt;dbl&gt; 0.0000000, 0.0000000, 0.4989854, 0.498985…\n$ numfeeders_water             &lt;dbl&gt; 1.0000000, 1.0000000, 0.7887147, 0.788714…\n$ numfeeders_thistle           &lt;dbl&gt; 0.000000, 0.000000, 1.007624, 1.000000, 1…\n$ numfeeders_fruit             &lt;dbl&gt; 0.0000000, 0.0000000, 0.1442454, 0.144245…\n$ numfeeders_hopper            &lt;dbl&gt; 1.385205, 1.385205, 1.000000, 1.385205, 0…\n$ numfeeders_tube              &lt;dbl&gt; 2.162308, 2.162308, 1.000000, 2.162308, 2…\n$ numfeeders_other             &lt;dbl&gt; 0.6037683, 0.6037683, 0.6037683, 0.603768…\n$ population_atleast           &lt;dbl&gt; 1, 1, 1, 25001, 5001, 25001, 1, 1, 1, 1, …\n$ count_area_size_sq_m_atleast &lt;dbl&gt; 1.01, 1.01, 1.01, 1.01, 1.01, 1.01, 375.0…\n$ squirrels                    &lt;fct&gt; no squirrels, no squirrels, no squirrels,…"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#specify-model",
    "href": "handout/2023-02-21-tidymodels.html#specify-model",
    "title": "Tidymodels",
    "section": "Specify model",
    "text": "Specify model\n\nfeeder_log &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#build-workflow",
    "href": "handout/2023-02-21-tidymodels.html#build-workflow",
    "title": "Tidymodels",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nfeeder_wflow &lt;- workflow() %&gt;%\n  add_recipe(feeder_rec) %&gt;%\n  add_model(feeder_log) \n\n\nSee next slide for workflow…"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#view-workflow",
    "href": "handout/2023-02-21-tidymodels.html#view-workflow",
    "title": "Tidymodels",
    "section": "View workflow",
    "text": "View workflow\n\nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#fit-model-to-training-data",
    "href": "handout/2023-02-21-tidymodels.html#fit-model-to-training-data",
    "title": "Tidymodels",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nfeeder_fit &lt;- feeder_wflow %&gt;%\n  fit(data = feeder_train)\n\nfeeder_fit %&gt;% tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n. . .\nSo many predictors!"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#model-fit-summary",
    "href": "handout/2023-02-21-tidymodels.html#model-fit-summary",
    "title": "Tidymodels",
    "section": "Model fit summary",
    "text": "Model fit summary\n\nfeeder_fit %&gt;% tidy() \n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#all-together",
    "href": "handout/2023-02-21-tidymodels.html#all-together",
    "title": "Tidymodels",
    "section": "All together",
    "text": "All together\n\nrecipemodelworkflowfitpredict\n\n\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n\n\n\nfeeder_log &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow &lt;- workflow() %&gt;%\n  add_recipe(feeder_rec) %&gt;%\n  add_model(feeder_log) \nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_fit &lt;- feeder_wflow %&gt;%\n  fit(data = feeder_train)\n\nfeeder_fit %&gt;% tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n\n\nfeeder_train_pred &lt;- predict(feeder_fit, feeder_train, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_train %&gt;% select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#make-predictions-for-training-data",
    "href": "handout/2023-02-21-tidymodels.html#make-predictions-for-training-data",
    "title": "Tidymodels",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\nfeeder_train_pred &lt;- predict(feeder_fit, feeder_train, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_train %&gt;% select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#accuracy",
    "href": "handout/2023-02-21-tidymodels.html#accuracy",
    "title": "Tidymodels",
    "section": "Accuracy",
    "text": "Accuracy\n\nrbind(accuracy(feeder_train_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#visualizing-accuracy",
    "href": "handout/2023-02-21-tidymodels.html#visualizing-accuracy",
    "title": "Tidymodels",
    "section": "Visualizing accuracy",
    "text": "Visualizing accuracy\n\nfeeder_train_pred %&gt;%\n  select(squirrels, .pred_class) %&gt;%\n  yardstick::conf_mat(squirrels, .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n. . ."
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#but-really",
    "href": "handout/2023-02-21-tidymodels.html#but-really",
    "title": "Tidymodels",
    "section": "But, really…",
    "text": "But, really…\nwho cares about predictions on training data?"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#make-predictions-for-testing-data",
    "href": "handout/2023-02-21-tidymodels.html#make-predictions-for-testing-data",
    "title": "Tidymodels",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_test %&gt;% select(squirrels))\n\nfeeder_test_pred\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# ℹ 58,912 more rows"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#evaluate-testing-data",
    "href": "handout/2023-02-21-tidymodels.html#evaluate-testing-data",
    "title": "Tidymodels",
    "section": "Evaluate testing data",
    "text": "Evaluate testing data\n\nrbind(accuracy(feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#evaluate-testing-data-1",
    "href": "handout/2023-02-21-tidymodels.html#evaluate-testing-data-1",
    "title": "Tidymodels",
    "section": "Evaluate testing data",
    "text": "Evaluate testing data\n\nfeeder_test_pred %&gt;%\n  select(squirrels, .pred_class) %&gt;%\n  yardstick::conf_mat(squirrels, .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale."
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#training-vs.-testing",
    "href": "handout/2023-02-21-tidymodels.html#training-vs.-testing",
    "title": "Tidymodels",
    "section": "Training vs. testing",
    "text": "Training vs. testing\n\nrbind(accuracy(feeder_train_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101\n\nrbind(accuracy(feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#whats-up-with-training-data",
    "href": "handout/2023-02-21-tidymodels.html#whats-up-with-training-data",
    "title": "Tidymodels",
    "section": "What’s up with training data?",
    "text": "What’s up with training data?\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#all-together-1",
    "href": "handout/2023-02-21-tidymodels.html#all-together-1",
    "title": "Tidymodels",
    "section": "All together",
    "text": "All together\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n\n\n\nfeeder_log &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow &lt;- workflow() %&gt;%\n  add_recipe(feeder_rec) %&gt;%\n  add_model(feeder_log) \n\nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_fit &lt;- feeder_wflow %&gt;%\n  fit(data = feeder_train)\n\nfeeder_fit %&gt;% tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") %&gt;%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %&gt;%\n  bind_cols(feeder_test %&gt;% select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows\n\n\n\n\n\nrbind(accuracy(feeder_test_pred, truth = squirrels, estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, estimate = .pred_class),\n      specificity(feeder_test_pred, truth = squirrels, estimate = .pred_class))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.0988\n3 specificity binary        0.985"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#competing-models",
    "href": "handout/2023-02-21-tidymodels.html#competing-models",
    "title": "Tidymodels",
    "section": "Competing models",
    "text": "Competing models\n\nwe use the test data to assess how the model does. But we haven’t yet thought about how to use the data to build a particular model.\ncompare two different models to predict whether or not there are squirrels\n\nModel 1: removes the information about the habitat and about the trees and shrubs\nModel 2: removes the information about feeding the birds"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#compare-recipes",
    "href": "handout/2023-02-21-tidymodels.html#compare-recipes",
    "title": "Tidymodels",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nrecipe 1recipe 2\n\n\n\nfeeder_rec1 &lt;- recipe(squirrels ~ ., data = feeder_train) %&gt;%\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) %&gt;%\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\n\n\n\nfeeder_rec2 &lt;- recipe(squirrels ~ ., data = feeder_train) %&gt;%\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) %&gt;%\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#how-can-we-decide",
    "href": "handout/2023-02-21-tidymodels.html#how-can-we-decide",
    "title": "Tidymodels",
    "section": "How can we decide?",
    "text": "How can we decide?\n\nmeasure which model does better on the test data\nmeasure which model does better on the training data\nmeasure which model does better on the cross validated data"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#how-does-cross-validation-work",
    "href": "handout/2023-02-21-tidymodels.html#how-does-cross-validation-work",
    "title": "Tidymodels",
    "section": "How does cross validation work?",
    "text": "How does cross validation work?\n\n\n\n\n\n4-fold CV is depicted. Notice that the holdout group is never used as part of the coefficient estimation process."
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#cross-validation-1",
    "href": "handout/2023-02-21-tidymodels.html#cross-validation-1",
    "title": "Tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation:\n\nRandomly partition the training data into v group\nUse v-1 groups to build the model (calculate MLEs); use 1 group for prediction / assessment\nRepeat v times, updating which group is used for assessment each time"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#cross-validation-step-1",
    "href": "handout/2023-02-21-tidymodels.html#cross-validation-step-1",
    "title": "Tidymodels",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nConsider the example below where the training data are randomly split into 3 partitions:\n\n\n\n\n\nSplitting the data into a partition of v=3 groups. Source: (Kuhn and Silge 2022)"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#cross-validation-steps-2-and-3",
    "href": "handout/2023-02-21-tidymodels.html#cross-validation-steps-2-and-3",
    "title": "Tidymodels",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time\n\n\n\n\n\n\nWith the data split into three groups, we can see how 2/3 of the observations are used to fit the model and 1/3 of the observations are used to estimate the performance of the model. Source: (Kuhn and Silge 2022)"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#cross-validation-using-tidymodels",
    "href": "handout/2023-02-21-tidymodels.html#cross-validation-using-tidymodels",
    "title": "Tidymodels",
    "section": "Cross validation using tidymodels",
    "text": "Cross validation using tidymodels\n\nset.seed(4747)\nfolds &lt;- vfold_cv(feeder_train, v = 3, strata = squirrels)\nfolds\n\n#  3-fold cross-validation using stratification \n# A tibble: 3 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [117841/58922]&gt; Fold1\n2 &lt;split [117842/58921]&gt; Fold2\n3 &lt;split [117843/58920]&gt; Fold3"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#fit-the-model-separately-to-each-fold",
    "href": "handout/2023-02-21-tidymodels.html#fit-the-model-separately-to-each-fold",
    "title": "Tidymodels",
    "section": "Fit the model separately to each fold",
    "text": "Fit the model separately to each fold\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec1 &lt;- recipe(squirrels ~ ., data = feeder_train) %&gt;%\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) %&gt;%\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Variables removed: contains(\"hab\")\n\n\n• Variables removed: contains(\"atleast\")\n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n\n\n\nfeeder_log &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow1 &lt;- workflow() %&gt;%\n  add_recipe(feeder_rec1) %&gt;%\n  add_model(feeder_log) \n\nfeeder_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nmetrics_interest &lt;- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs1 &lt;- feeder_wflow1 %&gt;%\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n\n\n\n\nfeeder_fit_rs1 %&gt;% augment() %&gt;%\n  select(squirrels, .pred_class) %&gt;%\n  yardstick::conf_mat(squirrels, .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\ncollect_metrics(feeder_fit_rs1)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#repeat-the-cv-analysis-for-the-second-model",
    "href": "handout/2023-02-21-tidymodels.html#repeat-the-cv-analysis-for-the-second-model",
    "title": "Tidymodels",
    "section": "Repeat the CV analysis for the second model",
    "text": "Repeat the CV analysis for the second model\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec2 &lt;- recipe(squirrels ~ ., data = feeder_train) %&gt;%\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) %&gt;%\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) %&gt;%\n  step_impute_mean(all_numeric_predictors()) %&gt;%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 58\n\n\n\n\n\n── Operations \n\n\n• Variables removed: contains(\"fed\")\n\n\n• Variables removed: contains(\"feed\")\n\n\n• Mean imputation for: all_numeric_predictors()\n\n\n• Sparse, unbalanced variable filter on: all_numeric_predictors()\n\n\n\n\n\nfeeder_log &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow2 &lt;- workflow() %&gt;%\n  add_recipe(feeder_rec2) %&gt;%\n  add_model(feeder_log) \n\nfeeder_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nmetrics_interest &lt;- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs2 &lt;- feeder_wflow2 %&gt;%\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n\n\n\n\nfeeder_fit_rs2 %&gt;% augment() %&gt;%\n  select(squirrels, .pred_class) %&gt;%\n  yardstick::conf_mat(squirrels, .pred_class) %&gt;%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\nScale for fill is already present.\nAdding another scale for fill, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n\n\n\ncollect_metrics(feeder_fit_rs2)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#compare-two-models",
    "href": "handout/2023-02-21-tidymodels.html#compare-two-models",
    "title": "Tidymodels",
    "section": "Compare two models",
    "text": "Compare two models\n\nModel 1\n\ncollect_metrics(feeder_fit_rs1)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1\n\n\n\n\nModel 2\n\ncollect_metrics(feeder_fit_rs2)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#cross-validation-then-test-data",
    "href": "handout/2023-02-21-tidymodels.html#cross-validation-then-test-data",
    "title": "Tidymodels",
    "section": "Cross validation then test data",
    "text": "Cross validation then test data\n\n\n\n\n\nNested cross-validation: two cross-validation loops are run one inside the other. (Varoquaux et al. 2017)"
  },
  {
    "objectID": "handout/2023-02-21-tidymodels.html#bias-variance-trade-off",
    "href": "handout/2023-02-21-tidymodels.html#bias-variance-trade-off",
    "title": "Tidymodels",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\n\n\n\n\nTest and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! (Hastie, Tibshirani, and Friedman 2001)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Final Project",
    "section": "",
    "text": "The HELP study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care, with the goal of linking them to primary medical care.\nThe primary endpoint was whether or not the individual connected to primary care. That is, the “event” of interest is an appointment with a primary care physician.\n\n\nSamet JH, Larson MJ, Horton NJ, Doyle K, Winter M, and Saitz R. Linking alcohol and drug-dependent adults to primary medical care: A randomized controlled trial of a multi-disciplinary health intervention in a detoxification unit. Addiction, 2003; 98(4):509-516. (You should be able to access the article through the Claremont Colleges Library after you login.)\n\n\n\nThe data can be loaded in directly from Box at: https://pomona.box.com/shared/static/uicqv6s6o5icfc14r9acu1kaafxisp3l.csv\nPlease use the above dataset in your analysis. However, feel free to look at the many variables that are described in the related dataset HELPfull (available in the mosaic package). Some things to notice:\n\nThe treatment variable (intervention) is called group.\nThe primary endpoint is marked by time: dayslink and censoring indicator: linkstatus.\nWhile there is certainly interest in the intervention, there is room for additional demographic variables to be associated with linking to primary care.\nThe goal of the semester project is to create a Cox Proportional Hazard model which uses explanatory variables to model hazard of “event”.\n\n\n\n\n\nWednesday, April 23. Post (to the GitHub repo that I created) both the Quarto file and the corresponding pdf document:\n\nInclude the names of the individuals in the group.\nSome sort of EDA (exploratory data analysis). Do not print the data, but do something that indicates you’ve uploaded the data and know what some of the variables are. You might have some summary statistics or a graph. The idea here is to explore the variables, not to run survival curves. What can you learn about the many many variables? How do they relate to one another? What about all the missingness? This is not an extensive assignment. [Again, the question here is not about the response variable, it’s about all the predictor variables. Learn about the dataset!]\nOutline the “something new” part of the assignment. You should indicate who is doing what, what resources each of you will use to learn about your new topic, and a few sentences on what the topic is or how it relates to survival analysis / the analysis at hand. Additionally, for each “new” thing, provide 1-2 sentences describing what will be challenging about learning something new.\n\n(Optional) Monday, May 5. If you would like me to look at / provide feedback for a completed rough draft, you should turn in the completed draft before May 5th (or May 2nd). (If you are a graduating senior who would like feedback, please provide me a draft by Friday, May 2nd.)\nWednesday, May 7. Last day of class, each person / group will present their visualization, and there will be secret ballot voting for the best visualization of the data / model.\nFriday, May 16. Complete assignment is due (turn in both the qmd and knitted pdf to your GitHub repository). I will be grading from GitHub directly, so no need to turn in to Gradescope. (If you are a graduating senior, the final assignment is due by noon on Thursday, May 8.)\nIf you are on a team with a graduating senior, then your due dates are all the same as the graduating seniors.\n\n\n\n\n\nThe analysis should be done in a complete Quarto file and pushed to the GitHub repo I will create for your team. The final report should address the following tasks below but should not enumerate the tasks (it should be written like a final report).\n\nExploratory data analysis. Graphical and numerical summaries of the data (both explanatory and response variables). Please do not attach every single graph you try. Instead, choose only a few images that give the reader a sense of the variables and their relationships.\nCox PH model. Using model building techniques, choose a subset of variables (including transformations, interactions, etc.) to find the model which produces the best survival information. The team whose model is best1 on a hold-out sample will earn an additional 3 points on the project.\nIn the final write-up, make sure the final model is very clearly specified.\nThe Cox PH analysis should include: an interpretation of your final survival model including a discussion of the sign of the coefficients (note: feel free to use interactions) Which variable(s) are in? Which are out? What do you conclude about linking to primary care? Is there anything worth mentioning about how you got to your final model? What can you say about causation? What can you say about generalizing to a larger population?\nSomething new. Each individual in the class will learn a new idea related to survival analysis (see below). The new part will be assessed on your ability to explain the new idea. That is, you must indicate a complete understanding of the topic as well as an adequate use of the method applied to the dataset.\nA visualization to enter into the competition. The visualization could be of the EDA variety, or it could be based on the model / results. The different visualizations will be presented to the class on the last day of the semester (May 7). The team whose visualization is best (secret ballot by the class) will earn an additional 3 points on the project. The visualization should be included in your GitHub repo (either within the final report or as a separate png / jpg / etc.). The visualization should be done in R, but interactive is fine (via Shiny or plotly etc.).\nA reflection on the decision making (goes into the final report). How did it feel to decide things? What was easy to decide (e.g., method because you were told to use Cox PH)? What was hard to decide (e.g., which variables to use)? What about missingness, what did you do? Does it matter? Reflect on the process and how you think your decisions may or may not have had an impact on the results you put forward. Also reflect on whether or not your decisions may have had any ethical consequences.\nThe final report which ties it all together should have an introduction, methods, results, discussion. The grading rubric should help you put together the final written project.\n\n\n\n\nEach individual should have some analysis that goes beyond a Cox PH model. For your analysis, you should give details of what is going on, how it is relevant, what are the technical conditions, what are the conclusions, etc. Your analysis should indicate a sense that you understand and that you can communicate the results to a possible client. Some possible topics to investigate include:\n\nInvestigation of the proportional hazards assumption (what does the R function cox.zph() do?)\nExponential or Weibull PH regression (parametric survival model)\nDeriving / detailing AIC & BIC for model selection on Cox PH\nPower analysis (a simulation? It is okay for you to use a built in function, but you’ll need to know and explain what that function is doing.)\nDerivation of the sample size calculation for the log rank test (and application to the data)\nAn analysis of the Schoenfeld residuals (how are they calculated and why is that calculation relevant?)\nBreslow estimate of the Survival curve under Cox PH assumption.\nBootstrapping the survival model (what are the assumptions? what do you conclude?)\nAn analysis / understanding / simulation of the multiple comparisons issues for assessing many different models (or other exploratory hypotheses).\nAndersen-Gill extension of the Cox PH model for time-varying covariates (available in rms R package). [Note: A-G isn’t meant to test PH, per se, but rather it allows for variables that are time varying.]\nAnother topic related to survival analysis that you find interesting.\n\n\n\n\n\nUse Quarto to create a reproducible analysis. Anyone should be able to run your analysis using only the .qmd file and the dataset.\nTurn in both .qmd and .pdf files for your final analysis.\nIf you are working in pairs, the project is extended in two ways. 1. You must both be working on the files in GitHub. That is, each person must make commits to the repository. 2. You must do two new items (one each).\nNote that the event of interest is “visit to primary care physician.”\nBe as creative as possible trying to think about how you might like to graphically display the data. If you come up with a cool idea for a graph but don’t know how to implement it, please let me know, and I will write the code for you!!\nPlease do not re-code the variables or change the variable names outside of R. You may, however, transform (mutate()) the variables within your R code (that is, for example, if you wanted to divide months by 12 to have years, or square a variable, etc.).\n\n\n\n\nYour primary assessment will be based on the above items (modeling, understanding of new topic, additional analysis, interpretation).\nAdditionally there will be two competitions. Winning either will add 3 points (out of 85) to your score.\n\nGraphic: the class will vote on who has the best graphic.\nModel: using a holdout sample (I only gave you part of the data), I will assess your final model. The group whose model best describes the holdout sample (as measured by the c-index) will win the model prize."
  },
  {
    "objectID": "project.html#health-evaluation-and-linkage-to-primary-care",
    "href": "project.html#health-evaluation-and-linkage-to-primary-care",
    "title": "Final Project",
    "section": "",
    "text": "The HELP study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomized to receive a multidisciplinary assessment and a brief motivational intervention or usual care, with the goal of linking them to primary medical care.\nThe primary endpoint was whether or not the individual connected to primary care. That is, the “event” of interest is an appointment with a primary care physician.\n\n\nSamet JH, Larson MJ, Horton NJ, Doyle K, Winter M, and Saitz R. Linking alcohol and drug-dependent adults to primary medical care: A randomized controlled trial of a multi-disciplinary health intervention in a detoxification unit. Addiction, 2003; 98(4):509-516. (You should be able to access the article through the Claremont Colleges Library after you login.)\n\n\n\nThe data can be loaded in directly from Box at: https://pomona.box.com/shared/static/uicqv6s6o5icfc14r9acu1kaafxisp3l.csv\nPlease use the above dataset in your analysis. However, feel free to look at the many variables that are described in the related dataset HELPfull (available in the mosaic package). Some things to notice:\n\nThe treatment variable (intervention) is called group.\nThe primary endpoint is marked by time: dayslink and censoring indicator: linkstatus.\nWhile there is certainly interest in the intervention, there is room for additional demographic variables to be associated with linking to primary care.\nThe goal of the semester project is to create a Cox Proportional Hazard model which uses explanatory variables to model hazard of “event”.\n\n\n\n\n\nWednesday, April 23. Post (to the GitHub repo that I created) both the Quarto file and the corresponding pdf document:\n\nInclude the names of the individuals in the group.\nSome sort of EDA (exploratory data analysis). Do not print the data, but do something that indicates you’ve uploaded the data and know what some of the variables are. You might have some summary statistics or a graph. The idea here is to explore the variables, not to run survival curves. What can you learn about the many many variables? How do they relate to one another? What about all the missingness? This is not an extensive assignment. [Again, the question here is not about the response variable, it’s about all the predictor variables. Learn about the dataset!]\nOutline the “something new” part of the assignment. You should indicate who is doing what, what resources each of you will use to learn about your new topic, and a few sentences on what the topic is or how it relates to survival analysis / the analysis at hand. Additionally, for each “new” thing, provide 1-2 sentences describing what will be challenging about learning something new.\n\n(Optional) Monday, May 5. If you would like me to look at / provide feedback for a completed rough draft, you should turn in the completed draft before May 5th (or May 2nd). (If you are a graduating senior who would like feedback, please provide me a draft by Friday, May 2nd.)\nWednesday, May 7. Last day of class, each person / group will present their visualization, and there will be secret ballot voting for the best visualization of the data / model.\nFriday, May 16. Complete assignment is due (turn in both the qmd and knitted pdf to your GitHub repository). I will be grading from GitHub directly, so no need to turn in to Gradescope. (If you are a graduating senior, the final assignment is due by noon on Thursday, May 8.)\nIf you are on a team with a graduating senior, then your due dates are all the same as the graduating seniors."
  },
  {
    "objectID": "project.html#assignment",
    "href": "project.html#assignment",
    "title": "Final Project",
    "section": "",
    "text": "The analysis should be done in a complete Quarto file and pushed to the GitHub repo I will create for your team. The final report should address the following tasks below but should not enumerate the tasks (it should be written like a final report).\n\nExploratory data analysis. Graphical and numerical summaries of the data (both explanatory and response variables). Please do not attach every single graph you try. Instead, choose only a few images that give the reader a sense of the variables and their relationships.\nCox PH model. Using model building techniques, choose a subset of variables (including transformations, interactions, etc.) to find the model which produces the best survival information. The team whose model is best1 on a hold-out sample will earn an additional 3 points on the project.\nIn the final write-up, make sure the final model is very clearly specified.\nThe Cox PH analysis should include: an interpretation of your final survival model including a discussion of the sign of the coefficients (note: feel free to use interactions) Which variable(s) are in? Which are out? What do you conclude about linking to primary care? Is there anything worth mentioning about how you got to your final model? What can you say about causation? What can you say about generalizing to a larger population?\nSomething new. Each individual in the class will learn a new idea related to survival analysis (see below). The new part will be assessed on your ability to explain the new idea. That is, you must indicate a complete understanding of the topic as well as an adequate use of the method applied to the dataset.\nA visualization to enter into the competition. The visualization could be of the EDA variety, or it could be based on the model / results. The different visualizations will be presented to the class on the last day of the semester (May 7). The team whose visualization is best (secret ballot by the class) will earn an additional 3 points on the project. The visualization should be included in your GitHub repo (either within the final report or as a separate png / jpg / etc.). The visualization should be done in R, but interactive is fine (via Shiny or plotly etc.).\nA reflection on the decision making (goes into the final report). How did it feel to decide things? What was easy to decide (e.g., method because you were told to use Cox PH)? What was hard to decide (e.g., which variables to use)? What about missingness, what did you do? Does it matter? Reflect on the process and how you think your decisions may or may not have had an impact on the results you put forward. Also reflect on whether or not your decisions may have had any ethical consequences.\nThe final report which ties it all together should have an introduction, methods, results, discussion. The grading rubric should help you put together the final written project."
  },
  {
    "objectID": "project.html#something-new",
    "href": "project.html#something-new",
    "title": "Final Project",
    "section": "",
    "text": "Each individual should have some analysis that goes beyond a Cox PH model. For your analysis, you should give details of what is going on, how it is relevant, what are the technical conditions, what are the conclusions, etc. Your analysis should indicate a sense that you understand and that you can communicate the results to a possible client. Some possible topics to investigate include:\n\nInvestigation of the proportional hazards assumption (what does the R function cox.zph() do?)\nExponential or Weibull PH regression (parametric survival model)\nDeriving / detailing AIC & BIC for model selection on Cox PH\nPower analysis (a simulation? It is okay for you to use a built in function, but you’ll need to know and explain what that function is doing.)\nDerivation of the sample size calculation for the log rank test (and application to the data)\nAn analysis of the Schoenfeld residuals (how are they calculated and why is that calculation relevant?)\nBreslow estimate of the Survival curve under Cox PH assumption.\nBootstrapping the survival model (what are the assumptions? what do you conclude?)\nAn analysis / understanding / simulation of the multiple comparisons issues for assessing many different models (or other exploratory hypotheses).\nAndersen-Gill extension of the Cox PH model for time-varying covariates (available in rms R package). [Note: A-G isn’t meant to test PH, per se, but rather it allows for variables that are time varying.]\nAnother topic related to survival analysis that you find interesting."
  },
  {
    "objectID": "project.html#r-thoughts",
    "href": "project.html#r-thoughts",
    "title": "Final Project",
    "section": "",
    "text": "Use Quarto to create a reproducible analysis. Anyone should be able to run your analysis using only the .qmd file and the dataset.\nTurn in both .qmd and .pdf files for your final analysis.\nIf you are working in pairs, the project is extended in two ways. 1. You must both be working on the files in GitHub. That is, each person must make commits to the repository. 2. You must do two new items (one each).\nNote that the event of interest is “visit to primary care physician.”\nBe as creative as possible trying to think about how you might like to graphically display the data. If you come up with a cool idea for a graph but don’t know how to implement it, please let me know, and I will write the code for you!!\nPlease do not re-code the variables or change the variable names outside of R. You may, however, transform (mutate()) the variables within your R code (that is, for example, if you wanted to divide months by 12 to have years, or square a variable, etc.).\n\n\n\n\nYour primary assessment will be based on the above items (modeling, understanding of new topic, additional analysis, interpretation).\nAdditionally there will be two competitions. Winning either will add 3 points (out of 85) to your score.\n\nGraphic: the class will vote on who has the best graphic.\nModel: using a holdout sample (I only gave you part of the data), I will assess your final model. The group whose model best describes the holdout sample (as measured by the c-index) will win the model prize."
  },
  {
    "objectID": "project.html#footnotes",
    "href": "project.html#footnotes",
    "title": "Final Project",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nmeasured by the \\(c\\) index↩︎"
  },
  {
    "objectID": "syllabus.html#math-150-spring-2025",
    "href": "syllabus.html#math-150-spring-2025",
    "title": "syllabus",
    "section": "",
    "text": "Class: Mondays & Wednesday, 1:15-2:30pm\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMonday: 2:30-4:30pm\nTuesday: 9-11am\nThursday: 1:15-3pm\n\n\nTuesday 6-8pm\nEstella 2113\n\n\n\n\n\nArtwork by @allison_horst."
  },
  {
    "objectID": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "href": "LICENSE.html#creative-commons-attribution-sharealike-4.0-international-public-license",
    "title": "Attribution-ShareAlike 4.0 International",
    "section": "Creative Commons Attribution-ShareAlike 4.0 International Public License",
    "text": "Creative Commons Attribution-ShareAlike 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution-ShareAlike 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\n\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nBY-SA Compatible License means a license listed at creativecommons.org/compatiblelicenses, approved by Creative Commons as essentially the equivalent of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicense Elements means the license attributes listed in the name of a Creative Commons Public License. The License Elements of this Public License are Attribution and ShareAlike.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\n\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\nA. reproduce and Share the Licensed Material, in whole or in part; and\nB. produce, reproduce, and Share Adapted Material.\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)(4) never produces Adapted Material.\nDownstream recipients.\nA. Offer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nB. __Additional offer from the Licensor – Adapted Material. Every recipient of Adapted Material from You automatically receives an offer from the Licensor to exercise the Licensed Rights in the Adapted Material under the conditions of the Adapter’s License You apply.\nC. No downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\nA. retain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nB. indicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nC. indicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\n\nShareAlike.\n\nIn addition to the conditions in Section 3(a), if You Share Adapted Material You produce, the following conditions also apply.\n\nThe Adapter’s License You apply must be a Creative Commons license with the same License Elements, this version or later, or a BY-SA Compatible License.\nYou must include the text of, or the URI or hyperlink to, the Adapter’s License You apply. You may satisfy this condition in any reasonable manner based on the medium, means, and context in which You Share Adapted Material.\nYou may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, Adapted Material that restrict exercise of the rights granted under the Adapter’s License You apply.\n\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material, including for purposes of Section 3(b); and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\n\n\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUnless otherwise separately undertaken by the Licensor, to the extent possible, the Licensor offers the Licensed Material as-is and as-available, and makes no representations or warranties of any kind concerning the Licensed Material, whether express, implied, statutory, or other. This includes, without limitation, warranties of title, merchantability, fitness for a particular purpose, non-infringement, absence of latent or other defects, accuracy, or the presence or absence of errors, whether or not known or discoverable. Where disclaimers of warranties are not allowed in full or in part, this disclaimer may not apply to You.\nTo the extent possible, in no event will the Licensor be liable to You on any legal theory (including, without limitation, negligence) or otherwise for any direct, special, indirect, incidental, consequential, punitive, exemplary, or other losses, costs, expenses, or damages arising out of this Public License or use of the Licensed Material, even if the Licensor has been advised of the possibility of such losses, costs, expenses, or damages. Where a limitation of liability is not allowed in full or in part, this limitation may not apply to You.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\n\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\n\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.t stated herein are separate from and independent of the terms and conditions of this Public License.\n\n\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org"
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "Working on assignments with GitHub",
    "section": "",
    "text": "In Methods in Biosatistics (Math 150), we will use GitHub + Gradescope to access and submit assignments. Here is the basic structure of how it will work:\n\nGet the assignment materials from GitHub.\nClone the repository to any machine you are using.\nWork on the assignment and push back to GitHub.\nSubmit the assignment on Gradescope.\n\n\n\n\n\n\n\nimage credit: https://xkcd.com/1597/,\n\n\n\n\n\nThe following diagram lays out the process, and the rest of the document provides a more detailed set of instructions.\n\n\n\n\n\nFlowchart of assignment process."
  },
  {
    "objectID": "github.html#pull",
    "href": "github.html#pull",
    "title": "Working on assignments with GitHub",
    "section": "pull",
    "text": "pull\nIf you are working with a colleague or on different machines it is so incredibly important to get in the habit of immediately clicking on pull when you start your work. (If you are working alone on a single machine pull won’t hurt! You’ll just be told that your files are already up to date.)\n\n\n\n\n\nAlways pull before you start. pull-work-save-commit-push"
  },
  {
    "objectID": "github.html#render-your-work",
    "href": "github.html#render-your-work",
    "title": "Working on assignments with GitHub",
    "section": "Render your work",
    "text": "Render your work\nDon’t forget to put your name on the assignment. Also, make sure that you Render to pdf. Render early and often. The more often you Render, the fewer headaches you will have.\n\n\n\n\n\nAlways pull before you start. pull-work-render-commit-push"
  },
  {
    "objectID": "github.html#commit-your-work",
    "href": "github.html#commit-your-work",
    "title": "Working on assignments with GitHub",
    "section": "commit your work",
    "text": "commit your work\nYou don’t need to commit every file, but you do need to commit files that are integral to the analysis (always commit .qmd, .pdf, data files, images that created the pdf, etc.).\n\n\n\n\n\npull-work-render-commit-push"
  },
  {
    "objectID": "github.html#push-your-work-to-github",
    "href": "github.html#push-your-work-to-github",
    "title": "Working on assignments with GitHub",
    "section": "push your work to GitHub",
    "text": "push your work to GitHub\nIt is good practice to use meaningful commit messages to help your future self figure out your past work.\n\n\n\n\n\npull-work-render-commit-push"
  },
  {
    "objectID": "github.html#check-your-work-on-github",
    "href": "github.html#check-your-work-on-github",
    "title": "Working on assignments with GitHub",
    "section": "check your work on GitHub",
    "text": "check your work on GitHub\nTo make sure that the work went through, always check your GitHub repo online to confirm any changes you made.\n\n\n\n\n\nCheck that your changes are correct."
  },
  {
    "objectID": "github.html#connecting-your-github-account-to-gradescope",
    "href": "github.html#connecting-your-github-account-to-gradescope",
    "title": "Working on assignments with GitHub",
    "section": "Connecting your GitHub account to Gradescope",
    "text": "Connecting your GitHub account to Gradescope\nThe first time you go to submit an assignment on Gradescope, you will be asked to connect to GitHub. Here are the steps to follow to make that connection:\n\nAccess Gradescope from Canvas: From Canvas, click on Gradescope in the Course Navigation menu. You will be asked to authorize the Gradescope integration.\nNavigate to Gradescope.com: In a new tab (same browser), navigate to https://www.gradescope.com/. Gradescope should recognize your student user account from the Canvas integration.\nGo to Gradescope Account Settings: Click on Account (bottom left of the screen) and then Edit Account.\n\n\n\n\n\n\nChange your Gradescope account settings.\n\n\n\n\nThis will take you to your Account Settings in Gradescope. Here, you’ll have the option to verify your Pomona email address and set up a password.\n\nLink Your GitHub Account to Gradescope: Scroll to the bottom of the page to the Link External Account menu. Click on Link a GitHub account.\n\n\n\n\n\n\nLinking GitHub to Gradescope.\n\n\n\n\nYou’ll be prompted to authorize GitHub and connect it to Gradescope. In the drop-down menu under Repositories, be sure to select “Public and private” to enable full access.\n\n\n\n\n\nAuthorizing Gradescope to talk to GitHub.\n\n\n\n\nWhen prompted, log in to your GitHub account to complete the process (I don’t know if you need your PAT or your “Go(ubs!” password, try both!).\nAfter your accounts have been linked, you’ll see a message that says “Successfully authenticated with GitHub.”\n\n\n\n\n\nSuccessful integration of GitHub and Gradescope\n\n\n\n\n\nReturn to Canvas & Verify the Connection\n\nNow, you can return to Canvas and navigate back to Gradescope. If you are returning to your previous tab, you may need to refresh the page to make sure your account settings are updated.\nClick on your programming assignment in Gradescope. Verify that the GitHub connection is working, and that you can see a list of your GitHub files in the drop-down menu when you are submitting an assignment.\n\n\n\n\n\nSubmitting HW from GitHub to Gradescope.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you see a blank window when you click on Gradescope within Canvas, that means you need to enable third-party cookies in your browser. Since Gradescope is a third party integration in Canvas, cookies need to be enabled for it to work properly.\nITS has instructions for how to enable cookies on a range of different browsers: How do I enable third-party cookies in my browser?"
  },
  {
    "objectID": "github.html#submitting-assignments-after-having-connected-to-github",
    "href": "github.html#submitting-assignments-after-having-connected-to-github",
    "title": "Working on assignments with GitHub",
    "section": "Submitting assignments after having connected to GitHub",
    "text": "Submitting assignments after having connected to GitHub\nTo submit your assignment, complete the following steps:\n\nVia Canvas, access the course’s Gradescope site, select the appropriate assignment, and then choose GitHub as the submission method.\nSelect the appropriate GitHub repository. The branch will always be “main”.\n\nYou can submit multiple times before the deadline. Your last submission will determine your grade.\nOnce assignments are completely graded, you will be able to see your grade and assignment feedback on Gradescope. Grades will also be synced with Canvas."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#agenda",
    "href": "handout/2025-02-26-tidymodels.html#agenda",
    "title": "Tidymodels",
    "section": "Agenda",
    "text": "Agenda\n\n\nWorkflow to help us think about model building\nBreaking up data for independent assessment\nAssessment metrics"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#feeders",
    "href": "handout/2025-02-26-tidymodels.html#feeders",
    "title": "Tidymodels",
    "section": "Feeders",
    "text": "Feeders\nHow are other characteristics related to the presence of squirrels?\n\nsite_data |&gt;\n  group_by(squirrels) |&gt;\n  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))\n\n# A tibble: 2 × 2\n  squirrels    nearby_feeders\n  &lt;fct&gt;                 &lt;dbl&gt;\n1 no squirrels          0.344\n2 squirrels             0.456\n\n\nFor those observations that saw squirrels, 45% had feeders nearby; for those observations that did not see squirrels, 34% had feeders nearby."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#other-variables",
    "href": "handout/2025-02-26-tidymodels.html#other-variables",
    "title": "Tidymodels",
    "section": "Other variables",
    "text": "Other variables\nWhat about some of the variables describing the habitat?\n\nvizcode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsite_data |&gt;\n  group_by(squirrels) |&gt;\n  summarise(across(contains(\"hab\"), mean, na.rm = TRUE)) |&gt;\n  pivot_longer(contains(\"hab\")) |&gt;\n  mutate(name = str_remove(name, \"hab_\")) |&gt;\n  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +\n  geom_col(alpha = 0.8, position = \"dodge\") +\n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"% of locations\", y = NULL, fill = NULL)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#train-test",
    "href": "handout/2025-02-26-tidymodels.html#train-test",
    "title": "Tidymodels",
    "section": "Train / test",
    "text": "Train / test\nCreate an initial split:\n\nset.seed(470)\nfeeder_split &lt;- site_data |&gt;\n  initial_split(strata = squirrels) # default prop = 3/4 of each group\n\nSave training data\n\nfeeder_train &lt;- training(feeder_split)\ndim(feeder_train)\n\n[1] 176763     59\n\n\nSave testing data\n\nfeeder_test  &lt;- testing(feeder_split)\ndim(feeder_test)\n\n[1] 58922    59"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#training-data",
    "href": "handout/2025-02-26-tidymodels.html#training-data",
    "title": "Tidymodels",
    "section": "Training data",
    "text": "Training data\n\nfeeder_train\n\n# A tibble: 176,763 × 59\n   squirrels    yard_type_pavement yard_type_garden yard_type_landsca\n   &lt;fct&gt;                     &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 no squirrels                  0                0                 1\n 2 no squirrels                  0                0                 1\n 3 no squirrels                  0                0                 1\n 4 no squirrels                  0                0                 1\n 5 no squirrels                  0                0                 1\n 6 no squirrels                  0                0                 1\n 7 no squirrels                  0                0                 0\n 8 no squirrels                  0                0                 1\n 9 no squirrels                  0                0                 1\n10 no squirrels                  0                0                 1\n# ℹ 176,753 more rows\n# ℹ 55 more variables: yard_type_woods &lt;dbl&gt;, yard_type_desert &lt;dbl&gt;,\n#   hab_dcid_woods &lt;dbl&gt;, hab_evgr_woods &lt;dbl&gt;,\n#   hab_mixed_woods &lt;dbl&gt;, hab_orchard &lt;dbl&gt;, hab_park &lt;dbl&gt;,\n#   hab_water_fresh &lt;dbl&gt;, hab_water_salt &lt;dbl&gt;,\n#   hab_residential &lt;dbl&gt;, hab_industrial &lt;dbl&gt;,\n#   hab_agricultural &lt;dbl&gt;, hab_desert_scrub &lt;dbl&gt;, …"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#feature-engineering",
    "href": "handout/2025-02-26-tidymodels.html#feature-engineering",
    "title": "Tidymodels",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)\nFeature engineering is the act of creating new variables from the ones that already exist."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#modeling-workflow",
    "href": "handout/2025-02-26-tidymodels.html#modeling-workflow",
    "title": "Tidymodels",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#initiate-a-recipe",
    "href": "handout/2025-02-26-tidymodels.html#initiate-a-recipe",
    "title": "Tidymodels",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  )\n\nfeeder_rec"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#working-with-recipes",
    "href": "handout/2025-02-26-tidymodels.html#working-with-recipes",
    "title": "Tidymodels",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\n\nNote\n\n\nUsing prep() and bake() are shown here for demonstrative purposes. They do not need to be a part of your pipeline. I do find them assuring, however, so that I can see the effects of the recipe steps as the recipe is built."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#impute-missing-values-remove-zero-variance-predictors",
    "href": "handout/2025-02-26-tidymodels.html#impute-missing-values-remove-zero-variance-predictors",
    "title": "Tidymodels",
    "section": "Impute missing values & Remove zero variance predictors",
    "text": "Impute missing values & Remove zero variance predictors\nImpute missing values (replace with mean).\nRemove all predictors that contain only a single value\n\nfeeder_rec &lt;- feeder_rec |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#prep-and-bake",
    "href": "handout/2025-02-26-tidymodels.html#prep-and-bake",
    "title": "Tidymodels",
    "section": "Prep and bake",
    "text": "Prep and bake\n\nfeeder_rec_trained &lt;- prep(feeder_rec)\n\nbake(feeder_rec_trained, feeder_train) |&gt; \n  select(starts_with(\"hab\"), everything()) # re-ordering the columns\n\n# A tibble: 176,763 × 59\n   hab_dcid_woods hab_evgr_woods hab_mixed_woods hab_orchard hab_park\n            &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1          1              0.223           1          0.0932    0.450\n 2          1              0.223           1          0.0932    0.450\n 3          1              0               1          0         0    \n 4          1              0.223           0.655      0.0932    0.450\n 5          1              0               0          0         0    \n 6          1              0.223           1          0.0932    0.450\n 7          0              0               1          0         0    \n 8          0.495          0.223           0.655      0.0932    0.450\n 9          1              1               0.655      0.0932    1    \n10          0              0               1          0         0    \n# ℹ 176,753 more rows\n# ℹ 54 more variables: hab_water_fresh &lt;dbl&gt;, hab_water_salt &lt;dbl&gt;,\n#   hab_residential &lt;dbl&gt;, hab_industrial &lt;dbl&gt;,\n#   hab_agricultural &lt;dbl&gt;, hab_desert_scrub &lt;dbl&gt;,\n#   hab_young_woods &lt;dbl&gt;, hab_swamp &lt;dbl&gt;, hab_marsh &lt;dbl&gt;,\n#   yard_type_pavement &lt;dbl&gt;, yard_type_garden &lt;dbl&gt;,\n#   yard_type_landsca &lt;dbl&gt;, yard_type_woods &lt;dbl&gt;, …"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#specify-model",
    "href": "handout/2025-02-26-tidymodels.html#specify-model",
    "title": "Tidymodels",
    "section": "Specify model",
    "text": "Specify model\n\nfeeder_log &lt;- logistic_reg() |&gt;\n  set_engine(engine = \"glm\", family = \"binomial\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#build-workflow",
    "href": "handout/2025-02-26-tidymodels.html#build-workflow",
    "title": "Tidymodels",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nfeeder_wflow &lt;- workflow() |&gt;\n  add_recipe(feeder_rec) |&gt;\n  add_model(feeder_log) \n\n\nSee next slide for workflow…"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#view-workflow",
    "href": "handout/2025-02-26-tidymodels.html#view-workflow",
    "title": "Tidymodels",
    "section": "View workflow",
    "text": "View workflow\n\nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#fit-model-to-training-data",
    "href": "handout/2025-02-26-tidymodels.html#fit-model-to-training-data",
    "title": "Tidymodels",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nfeeder_fit &lt;- feeder_wflow |&gt;\n  fit(data = feeder_train)\n\nfeeder_fit |&gt; tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#model-fit-summary",
    "href": "handout/2025-02-26-tidymodels.html#model-fit-summary",
    "title": "Tidymodels",
    "section": "Model fit summary",
    "text": "Model fit summary\n\nfeeder_fit |&gt; tidy() \n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#all-together",
    "href": "handout/2025-02-26-tidymodels.html#all-together",
    "title": "Tidymodels",
    "section": "All together",
    "text": "All together\n\nrecipemodelworkflowfitpredict\n\n\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n\n\n\n\nfeeder_log &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow &lt;- workflow() |&gt;\n  add_recipe(feeder_rec) |&gt;\n  add_model(feeder_log) \nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_fit &lt;- feeder_wflow |&gt;\n  fit(data = feeder_train)\n\nfeeder_fit |&gt; tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n\n\nfeeder_train_pred &lt;- predict(object = feeder_fit, new_data = feeder_train, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_train |&gt; select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#make-predictions-for-training-data",
    "href": "handout/2025-02-26-tidymodels.html#make-predictions-for-training-data",
    "title": "Tidymodels",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\nfeeder_train_pred &lt;- predict(object = feeder_fit, new_data = feeder_train, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_train |&gt; select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#accuracy",
    "href": "handout/2025-02-26-tidymodels.html#accuracy",
    "title": "Tidymodels",
    "section": "Accuracy",
    "text": "Accuracy\n\nrbind(accuracy(data = feeder_train_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#visualizing-accuracy",
    "href": "handout/2025-02-26-tidymodels.html#visualizing-accuracy",
    "title": "Tidymodels",
    "section": "Visualizing accuracy",
    "text": "Visualizing accuracy\n\nfeeder_train_pred |&gt;\n  select(squirrels, .pred_class) |&gt;\n  yardstick::conf_mat(squirrels, .pred_class) |&gt;\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\")"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#but-really",
    "href": "handout/2025-02-26-tidymodels.html#but-really",
    "title": "Tidymodels",
    "section": "But, really…",
    "text": "But, really…\nwho cares about predictions on training data?"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#make-predictions-for-testing-data",
    "href": "handout/2025-02-26-tidymodels.html#make-predictions-for-testing-data",
    "title": "Tidymodels",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\nfeeder_test_pred &lt;- predict(object = feeder_fit, new_data = feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\nfeeder_test_pred\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# ℹ 58,912 more rows"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#evaluate-testing-data",
    "href": "handout/2025-02-26-tidymodels.html#evaluate-testing-data",
    "title": "Tidymodels",
    "section": "Evaluate testing data",
    "text": "Evaluate testing data\n\nrbind(accuracy(data = feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#evaluate-testing-data-1",
    "href": "handout/2025-02-26-tidymodels.html#evaluate-testing-data-1",
    "title": "Tidymodels",
    "section": "Evaluate testing data",
    "text": "Evaluate testing data\n\nfeeder_test_pred |&gt;\n  select(squirrels, .pred_class) |&gt;\n  yardstick::conf_mat(squirrels, .pred_class) |&gt;\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\")"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#training-vs.-testing",
    "href": "handout/2025-02-26-tidymodels.html#training-vs.-testing",
    "title": "Tidymodels",
    "section": "Training vs. testing",
    "text": "Training vs. testing\n\nrbind(accuracy(data = feeder_train_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101\n\nrbind(accuracy(data = feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#whats-up-with-training-data",
    "href": "handout/2025-02-26-tidymodels.html#whats-up-with-training-data",
    "title": "Tidymodels",
    "section": "What’s up with training data?",
    "text": "What’s up with training data?\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#all-together-1",
    "href": "handout/2025-02-26-tidymodels.html#all-together-1",
    "title": "Tidymodels",
    "section": "All together",
    "text": "All together\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec &lt;- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n\n\n\n\nfeeder_log &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow &lt;- workflow() |&gt;\n  add_recipe(feeder_rec) |&gt;\n  add_model(feeder_log) \n\nfeeder_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\n\n\n\nfeeder_fit &lt;- feeder_wflow |&gt;\n  fit(data = feeder_train)\n\nfeeder_fit |&gt; tidy()\n\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# ℹ 49 more rows\n\n\n\n\n\nfeeder_test_pred &lt;- predict(object = feeder_fit, new_data = feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\nfeeder_test_pred\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# ℹ 58,912 more rows\n\n\n\n\n\nrbind(accuracy(data = feeder_test_pred, truth = squirrels, estimate = .pred_class),\n      sensitivity(data = feeder_test_pred, truth = squirrels, estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred, truth = squirrels, estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#competing-models",
    "href": "handout/2025-02-26-tidymodels.html#competing-models",
    "title": "Tidymodels",
    "section": "Competing models",
    "text": "Competing models\n\nwe use the test data to assess how the model does. But we haven’t yet thought about how to use the data to build a particular model.\ncompare two different models to predict whether or not there are squirrels\n\nModel 1: removes the information about the habitat and about the trees and shrubs\nModel 2: removes the information about feeding the birds"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#compare-recipes",
    "href": "handout/2025-02-26-tidymodels.html#compare-recipes",
    "title": "Tidymodels",
    "section": "Compare recipes",
    "text": "Compare recipes\n\nrecipe 1recipe 2\n\n\n\nfeeder_rec1 &lt;- recipe(squirrels ~ ., data = feeder_train) |&gt;\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) |&gt;\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\n\n\n\nfeeder_rec2 &lt;- recipe(squirrels ~ ., data = feeder_train) |&gt;\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) |&gt;\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#how-can-we-decide",
    "href": "handout/2025-02-26-tidymodels.html#how-can-we-decide",
    "title": "Tidymodels",
    "section": "How can we decide?",
    "text": "How can we decide?\n\nmeasure which model does better on the test data\nmeasure which model does better on the training data\nmeasure which model does better on the cross validated data"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#how-does-cross-validation-work",
    "href": "handout/2025-02-26-tidymodels.html#how-does-cross-validation-work",
    "title": "Tidymodels",
    "section": "How does cross validation work?",
    "text": "How does cross validation work?\n\n4-fold CV is depicted. Notice that the holdout group is never used as part of the coefficient estimation process."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#cross-validation-1",
    "href": "handout/2025-02-26-tidymodels.html#cross-validation-1",
    "title": "Tidymodels",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation:\n\nRandomly partition the training data into v group\nUse v-1 groups to build the model (calculate MLEs); use 1 group for prediction / assessment\nRepeat v times, updating which group is used for assessment each time"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#cross-validation-step-1",
    "href": "handout/2025-02-26-tidymodels.html#cross-validation-step-1",
    "title": "Tidymodels",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nConsider the example below where the training data are randomly split into 3 partitions:\n\nSplitting the data into a partition of v=3 groups. Source: (Kuhn and Silge 2022)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#cross-validation-steps-2-and-3",
    "href": "handout/2025-02-26-tidymodels.html#cross-validation-steps-2-and-3",
    "title": "Tidymodels",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\nUse 1 partition for assessment, and the remaining v-1 partitions for analysis\nRepeat v times, updating which partition is used for assessment each time\n\n\nWith the data split into three groups, we can see how 2/3 of the observations are used to fit the model and 1/3 of the observations are used to estimate the performance of the model. Source: (Kuhn and Silge 2022)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#cross-validation-using-tidymodels",
    "href": "handout/2025-02-26-tidymodels.html#cross-validation-using-tidymodels",
    "title": "Tidymodels",
    "section": "Cross validation using tidymodels",
    "text": "Cross validation using tidymodels\n\nset.seed(4747)\nfolds &lt;- vfold_cv(data = feeder_train, v = 3, strata = squirrels)\nfolds\n\n#  3-fold cross-validation using stratification \n# A tibble: 3 × 2\n  splits                 id   \n  &lt;list&gt;                 &lt;chr&gt;\n1 &lt;split [117841/58922]&gt; Fold1\n2 &lt;split [117842/58921]&gt; Fold2\n3 &lt;split [117843/58920]&gt; Fold3"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#fit-the-model-separately-to-each-fold",
    "href": "handout/2025-02-26-tidymodels.html#fit-the-model-separately-to-each-fold",
    "title": "Tidymodels",
    "section": "Fit the model separately to each fold",
    "text": "Fit the model separately to each fold\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec1 &lt;- recipe(squirrels ~ ., data = feeder_train) |&gt;\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) |&gt;\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec1\n\n\n\n\nfeeder_log &lt;- logistic_reg() |&gt;\n  set_engine(engine = \"glm\", family = \"binomial\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow1 &lt;- workflow() |&gt;\n  add_recipe(feeder_rec1) |&gt;\n  add_model(feeder_log) \n\nfeeder_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm \n\n\n\n\n\nmetrics_interest &lt;- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs1 &lt;- feeder_wflow1 |&gt;\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n\n\n\n\nfeeder_fit_rs1 |&gt; augment() |&gt;\n  select(squirrels, .pred_class) |&gt;\n  yardstick::conf_mat(squirrels, .pred_class) |&gt;\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\n\n\n\n\n\n\n\n\n\n\ncollect_metrics(feeder_fit_rs1)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#repeat-the-cv-analysis-for-the-second-model",
    "href": "handout/2025-02-26-tidymodels.html#repeat-the-cv-analysis-for-the-second-model",
    "title": "Tidymodels",
    "section": "Repeat the CV analysis for the second model",
    "text": "Repeat the CV analysis for the second model\n\nrecipemodelworkflowfitpredictassess\n\n\n\nfeeder_rec2 &lt;- recipe(squirrels ~ ., data = feeder_train) |&gt;\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) |&gt;\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) |&gt;\n  step_impute_mean(all_numeric_predictors()) |&gt;\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec2\n\n\n\n\nfeeder_log &lt;- logistic_reg() |&gt;\n  set_engine(engine = \"glm\", family = \"binomial\")\n\nfeeder_log\n\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm \n\n\n\n\n\nfeeder_wflow2 &lt;- workflow() |&gt;\n  add_recipe(feeder_rec2) |&gt;\n  add_model(feeder_log) \n\nfeeder_wflow2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nEngine-Specific Arguments:\n  family = binomial\n\nComputational engine: glm \n\n\n\n\n\nmetrics_interest &lt;- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs2 &lt;- feeder_wflow2 |&gt;\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n\n\n\n\nfeeder_fit_rs2 |&gt; augment() |&gt;\n  select(squirrels, .pred_class) |&gt;\n  yardstick::conf_mat(squirrels, .pred_class) |&gt;\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n\n\n\n\n\n\n\n\n\n\n\ncollect_metrics(feeder_fit_rs2)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#compare-two-models",
    "href": "handout/2025-02-26-tidymodels.html#compare-two-models",
    "title": "Tidymodels",
    "section": "Compare two models",
    "text": "Compare two models\nModel 1\n\ncollect_metrics(feeder_fit_rs1)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1\n\n\nModel 2\n\ncollect_metrics(feeder_fit_rs2)\n\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#cross-validation-then-test-data",
    "href": "handout/2025-02-26-tidymodels.html#cross-validation-then-test-data",
    "title": "Tidymodels",
    "section": "Cross validation then test data",
    "text": "Cross validation then test data\n\nNested cross-validation: two cross-validation loops are run one inside the other. (Varoquaux et al. 2017)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#bias-variance-trade-off",
    "href": "handout/2025-02-26-tidymodels.html#bias-variance-trade-off",
    "title": "Tidymodels",
    "section": "Bias-variance trade-off",
    "text": "Bias-variance trade-off\n\nTest and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! (Hastie, Tibshirani, and Friedman 2001)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#references",
    "href": "handout/2025-02-26-tidymodels.html#references",
    "title": "Tidymodels",
    "section": "",
    "text": "Hastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning. Springer.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. https://www.tmwr.org/.\n\n\nVaroquaux, G., P. Reddy Raamana, D. Engemann, A. Hoyos-Idrobo, Y. Schwartz, and B. Thirion. 2017. “Assessing and Tuning Brain Decoders: Cross-Validation, Caveats, and Guidelines.” NeuroImage 145: 166–79."
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#comparing-models",
    "href": "handout/2025-02-26-tidymodels.html#comparing-models",
    "title": "Tidymodels",
    "section": "Comparing Models",
    "text": "Comparing Models\n\n\n\n\n\n\n\nData Modeling: The analysis in this culture starts with assuming a stochastic data model for the inside of the black box…The values of the parameters are estimated from the data and the model then used for information and/or prediction\nAlgorithmic Modeling: The analysis in this culture considers the inside of the box complex and unknown. The approach is to find a function f(x) — an algorithm that operates on x to predict the responses y.\n\nReference: Leo Breiman (2001)"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#data-goal",
    "href": "handout/2025-02-26-tidymodels.html#data-goal",
    "title": "Tidymodels",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: Project FeederWatch#TidyTuesday dataset on Project FeederWatch, a citizen science project for bird science, by way of TidyTuesday.\nCan the characteristics of the bird feeder site like the surrounding yard and habitat predict whether a bird feeder site will be used by squirrels?\n\n\ndatacode\n\n\n\n\n# A tibble: 235,685 × 59\n   squirrels    yard_type_pavement yard_type_garden yard_type_landsca\n   &lt;fct&gt;                     &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n 1 no squirrels                  0                0                 1\n 2 no squirrels                  0                0                 1\n 3 no squirrels                  0                0                 1\n 4 no squirrels                  0                0                 1\n 5 no squirrels                  0                0                 1\n 6 no squirrels                  0                0                 1\n 7 squirrels                     0                0                 0\n 8 squirrels                     0                0                 0\n 9 squirrels                     0                0                 0\n10 squirrels                     0                0                 0\n# ℹ 235,675 more rows\n# ℹ 55 more variables: yard_type_woods &lt;dbl&gt;, yard_type_desert &lt;dbl&gt;,\n#   hab_dcid_woods &lt;dbl&gt;, hab_evgr_woods &lt;dbl&gt;,\n#   hab_mixed_woods &lt;dbl&gt;, hab_orchard &lt;dbl&gt;, hab_park &lt;dbl&gt;,\n#   hab_water_fresh &lt;dbl&gt;, hab_water_salt &lt;dbl&gt;,\n#   hab_residential &lt;dbl&gt;, hab_industrial &lt;dbl&gt;,\n#   hab_agricultural &lt;dbl&gt;, hab_desert_scrub &lt;dbl&gt;, …\n\n\n\n\n\nsite_data &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv') |&gt;\n  mutate(squirrels = as.factor(ifelse(squirrels, \"squirrels\", \"no squirrels\"))) |&gt; \n  filter(!is.na(squirrels))\n\nsite_data &lt;- site_data |&gt;\n  select(where(~!all(is.na(.x)))) |&gt;\n  select(-loc_id, -proj_period_id, -fed_yr_round) |&gt;\n  select(squirrels, everything()) \n\nsite_data"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#how-many-squirrels",
    "href": "handout/2025-02-26-tidymodels.html#how-many-squirrels",
    "title": "Tidymodels",
    "section": "How many squirrels?",
    "text": "How many squirrels?\n\nsite_data |&gt;\n  group_by(squirrels) |&gt;\n  summarise(number_squirrels = n())\n\n# A tibble: 2 × 2\n  squirrels    number_squirrels\n  &lt;fct&gt;                   &lt;int&gt;\n1 no squirrels            45323\n2 squirrels              190362"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#different-predictions-for-training-data",
    "href": "handout/2025-02-26-tidymodels.html#different-predictions-for-training-data",
    "title": "Tidymodels",
    "section": "Different predictions for training data",
    "text": "Different predictions for training data\nNote that you now have to be predicted 0.75 to be a squirrel!\n\nfeeder_train_pred_0.75 &lt;- predict(feeder_fit, feeder_train, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.75,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_train |&gt; select(squirrels))\n\nfeeder_train_pred\n\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# ℹ 176,753 more rows\n\n\n\nrbind(accuracy(feeder_train_pred_0.75, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_train_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_train_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.744\n2 sensitivity binary         0.805\n3 specificity binary         0.490"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#feature-engineering---examples",
    "href": "handout/2025-02-26-tidymodels.html#feature-engineering---examples",
    "title": "Tidymodels",
    "section": "Feature engineering - examples",
    "text": "Feature engineering - examples\n\ndate: change to weekend / weekday\ncategorical: reduce # levels, one-hot encoding\nnumerical: transform using square, ln, normalize, scale\nbin numerical data to make it categorical\nmissing data: its own level? impute?\nadjust outliers"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html",
    "href": "handout/2025-03-03-ROC-onestep.html",
    "title": "ROC + Adding Variables",
    "section": "",
    "text": "Metric: Receiver Operating Characteristic (ROC) Curves\nMetrics: AIC & BIC\nModel building one variables at a time\n\n\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#agenda",
    "href": "handout/2025-03-03-ROC-onestep.html#agenda",
    "title": "ROC + Adding Variables",
    "section": "Agenda",
    "text": "Agenda\n\n\nMetric: Receiver Operating Characteristic (ROC) Curves\nMetrics: AIC & BIC\nModel building one variables at a time"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#make-predictions-for-test-data",
    "href": "handout/2025-03-03-ROC-onestep.html#make-predictions-for-test-data",
    "title": "ROC + Adding Variables",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.5,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\nfeeder_test_pred\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# ℹ 58,912 more rows"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#accuracy",
    "href": "handout/2025-03-03-ROC-onestep.html#accuracy",
    "title": "ROC + Adding Variables",
    "section": "Accuracy",
    "text": "Accuracy\n\nrbind(accuracy(feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      roc_auc(feeder_test_pred, truth = squirrels, \n                  .pred_squirrels, event_level = \"second\")\n      )\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n4 roc_auc     binary        0.721"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#roc-curves",
    "href": "handout/2025-03-03-ROC-onestep.html#roc-curves",
    "title": "ROC + Adding Variables",
    "section": "ROC Curves",
    "text": "ROC Curves\nReceiver Operating Characteristic Curves\n\nTPR = sensitivity = # of true predicted true / # true\nFPR = 1 - specificity = # false predicted true / # false\n\nHow can the plot use many TPRs and FPRs ???"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#roc-curve",
    "href": "handout/2025-03-03-ROC-onestep.html#roc-curve",
    "title": "ROC + Adding Variables",
    "section": "ROC Curve",
    "text": "ROC Curve"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#visualizing-accuracy",
    "href": "handout/2025-03-03-ROC-onestep.html#visualizing-accuracy",
    "title": "ROC + Adding Variables",
    "section": "Visualizing accuracy",
    "text": "Visualizing accuracy\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n4 roc_auc     binary        0.721"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#different-cut-off-0.8",
    "href": "handout/2025-03-03-ROC-onestep.html#different-cut-off-0.8",
    "title": "ROC + Adding Variables",
    "section": "Different cut-off = 0.8?",
    "text": "Different cut-off = 0.8?\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.8,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.675\n2 sensitivity binary         0.683\n3 specificity binary         0.641\n4 roc_auc     binary         0.721"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#different-cut-off-0.15",
    "href": "handout/2025-03-03-ROC-onestep.html#different-cut-off-0.15",
    "title": "ROC + Adding Variables",
    "section": "Different cut-off = 0.15?",
    "text": "Different cut-off = 0.15?\n\nfeeder_test_pred &lt;- predict(feeder_fit, feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.15,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\n\n\n# A tibble: 4 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary       0.807  \n2 sensitivity binary       0.999  \n3 specificity binary       0.00141\n4 roc_auc     binary       0.721"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#roc-curve-1",
    "href": "handout/2025-03-03-ROC-onestep.html#roc-curve-1",
    "title": "ROC + Adding Variables",
    "section": "ROC Curve",
    "text": "ROC Curve\n\nroc_auc(data = feeder_test_pred, truth = squirrels, \n                  .pred_squirrels, event_level = \"second\")\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.721\n\n\n\nTo get a single value (for model comparison), we use the area under the ROC curve."
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#aoc",
    "href": "handout/2025-03-03-ROC-onestep.html#aoc",
    "title": "ROC + Adding Variables",
    "section": "AOC",
    "text": "AOC\n\nAlexandria Ocasio-Cortez as Area Over the Curve. I didn’t make the image, and I don’t know who did!"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#more-metrics",
    "href": "handout/2025-03-03-ROC-onestep.html#more-metrics",
    "title": "ROC + Adding Variables",
    "section": "More metrics",
    "text": "More metrics\n\nAIC: Akaike’s Information Criteria = \\(-2\\ln L + 2p\\)\nBIC: Bayesian Information Criteria = \\(-2 \\ln L + p \\ln(n)\\)\n\n\nchoose a model with the smallest AIC or BIC"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#tools",
    "href": "handout/2025-03-03-ROC-onestep.html#tools",
    "title": "ROC + Adding Variables",
    "section": "Tools 😖",
    "text": "Tools 😖\n\ntidymodels does not make it easy to add or drop 1 variable at a time.\nadd1() and drop1() functions do not make it easy to work with dozens of predictors and missing data.\n\nTherefore, we’ll go back to the bird data from HW 5.\n\n\nLocation\n  bank  conif  decid ground  shrub   snag   wall \n     3     14     25     19     17      4      4"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#forward-1",
    "href": "handout/2025-03-03-ROC-onestep.html#forward-1",
    "title": "ROC + Adding Variables",
    "section": "Forward +1",
    "text": "Forward +1\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") |&gt;\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ 1\n         Df Deviance     AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;       108.533 110.533                      \nLength    1  105.296 109.296  3.2373 0.0719792 .  \nLocation  6   77.065  91.065 31.4684 2.063e-05 ***\nNo.eggs   1   90.951  94.951 17.5816 2.752e-05 ***\nColor     1  108.087 112.087  0.4463 0.5041175    \nIncubate  1  108.267 112.267  0.2658 0.6061875    \nNestling  1   93.825  97.825 14.7078 0.0001255 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#forward-2",
    "href": "handout/2025-03-03-ROC-onestep.html#forward-2",
    "title": "ROC + Adding Variables",
    "section": "Forward +2",
    "text": "Forward +2\n\nglm(`Closed?` ~ Location, data = nests, family=\"binomial\") |&gt;\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ Location\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        77.065 91.065                      \nLength    1   71.704 87.704  5.3605    0.0206 *  \nNo.eggs   1   61.211 77.211 15.8530 6.846e-05 ***\nColor     1   74.758 90.758  2.3070    0.1288    \nIncubate  1   74.829 90.829  2.2355    0.1349    \nNestling  1   74.722 90.722  2.3425    0.1259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#forward-3",
    "href": "handout/2025-03-03-ROC-onestep.html#forward-3",
    "title": "ROC + Adding Variables",
    "section": "Forward +3",
    "text": "Forward +3\n\nglm(`Closed?` ~ No.eggs + Location, data = nests, family=\"binomial\") |&gt;\n  add1(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, test = \"Chisq\")\n\nSingle term additions\n\nModel:\n`Closed?` ~ No.eggs + Location\n         Df Deviance    AIC    LRT Pr(&gt;Chi)  \n&lt;none&gt;        61.211 77.211                  \nLength    1   58.229 76.229 2.9823  0.08418 .\nColor     1   59.925 77.925 1.2865  0.25669  \nIncubate  1   59.891 77.891 1.3202  0.25056  \nNestling  1   59.247 77.247 1.9646  0.16102  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#backward--1",
    "href": "handout/2025-03-03-ROC-onestep.html#backward--1",
    "title": "ROC + Adding Variables",
    "section": "Backward -1",
    "text": "Backward -1\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, data = nests, family=\"binomial\") |&gt;\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling\n         Df Deviance    AIC     LRT Pr(&gt;Chi)   \n&lt;none&gt;        46.252 70.252                    \nLength    1   52.812 74.812  6.5600 0.010430 * \nLocation  6   66.017 78.017 19.7648 0.003049 **\nNo.eggs   1   56.049 78.049  9.7973 0.001748 **\nColor     1   46.997 68.997  0.7457 0.387857   \nIncubate  1   46.580 68.580  0.3279 0.566919   \nNestling  1   56.989 78.989 10.7368 0.001050 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#backward--2",
    "href": "handout/2025-03-03-ROC-onestep.html#backward--2",
    "title": "ROC + Adding Variables",
    "section": "Backward -2",
    "text": "Backward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Nestling, data = nests, family=\"binomial\") |&gt;\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Color + Nestling\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        46.580 68.580                      \nLength    1   58.124 78.124 11.5445 0.0006795 ***\nLocation  6   66.175 76.175 19.5956 0.0032675 ** \nNo.eggs   1   56.939 76.939 10.3597 0.0012879 ** \nColor     1   47.292 67.292  0.7127 0.3985498    \nNestling  1   56.989 76.989 10.4091 0.0012540 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#backward--2-1",
    "href": "handout/2025-03-03-ROC-onestep.html#backward--2-1",
    "title": "ROC + Adding Variables",
    "section": "Backward -2",
    "text": "Backward -2\n\nglm(`Closed?` ~ Length + Location + No.eggs + Incubate  + Totcare, \n    data = nests, family=\"binomial\") |&gt;\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Incubate + Totcare\n         Df Deviance    AIC     LRT Pr(&gt;Chi)    \n&lt;none&gt;        46.997 68.997                     \nLength    1   53.878 73.878  6.8809 0.008712 ** \nLocation  6   66.664 76.664 19.6663 0.003175 ** \nNo.eggs   1   57.418 77.418 10.4201 0.001247 ** \nIncubate  1   49.839 69.839  2.8416 0.091854 .  \nTotcare   1   58.227 78.227 11.2297 0.000805 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#backward--3",
    "href": "handout/2025-03-03-ROC-onestep.html#backward--3",
    "title": "ROC + Adding Variables",
    "section": "Backward -3",
    "text": "Backward -3\n\nglm(`Closed?` ~ Length + Location + No.eggs + Nestling, \n    data = nests, family=\"binomial\") |&gt;\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Nestling\n         Df Deviance    AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;        47.292 67.292                     \nLength    1   59.247 77.247 11.954 0.0005451 ***\nLocation  6   66.762 74.762 19.470 0.0034401 ** \nNo.eggs   1   58.337 76.337 11.045 0.0008894 ***\nNestling  1   58.229 76.229 10.937 0.0009427 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#automatic-forward-w-aic",
    "href": "handout/2025-03-03-ROC-onestep.html#automatic-forward-w-aic",
    "title": "ROC + Adding Variables",
    "section": "Automatic: Forward w AIC",
    "text": "Automatic: Forward w AIC\n\nglm(`Closed?` ~ 1, data = nests, family=\"binomial\") |&gt;\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, \n         direction = \"forward\", k = 2)\n\nStart:  AIC=110.53\n`Closed?` ~ 1\n\n           Df Deviance     AIC\n+ Location  6   77.065  91.065\n+ No.eggs   1   90.951  94.951\n+ Nestling  1   93.825  97.825\n+ Length    1  105.296 109.296\n&lt;none&gt;         108.533 110.533\n+ Color     1  108.087 112.087\n+ Incubate  1  108.267 112.267\n\nStep:  AIC=91.06\n`Closed?` ~ Location\n\n           Df Deviance    AIC\n+ No.eggs   1   61.211 77.211\n+ Length    1   71.704 87.704\n+ Nestling  1   74.722 90.722\n+ Color     1   74.758 90.758\n+ Incubate  1   74.829 90.829\n&lt;none&gt;          77.065 91.065\n\nStep:  AIC=77.21\n`Closed?` ~ Location + No.eggs\n\n           Df Deviance    AIC\n+ Length    1   58.229 76.229\n&lt;none&gt;          61.211 77.211\n+ Nestling  1   59.247 77.247\n+ Incubate  1   59.891 77.891\n+ Color     1   59.925 77.925\n\nStep:  AIC=76.23\n`Closed?` ~ Location + No.eggs + Length\n\n           Df Deviance    AIC\n+ Nestling  1   47.292 67.292\n&lt;none&gt;          58.229 76.229\n+ Color     1   56.989 76.989\n+ Incubate  1   58.227 78.227\n\nStep:  AIC=67.29\n`Closed?` ~ Location + No.eggs + Length + Nestling\n\n           Df Deviance    AIC\n&lt;none&gt;          47.292 67.292\n+ Color     1   46.580 68.580\n+ Incubate  1   46.997 68.997\n\n\n\nCall:  glm(formula = `Closed?` ~ Location + No.eggs + Length + Nestling, \n    family = \"binomial\", data = nests)\n\nCoefficients:\n   (Intercept)   Locationconif   Locationdecid  Locationground   Locationshrub  \n       11.1085        -19.2865        -16.8603        -20.5222        -18.6448  \n  Locationsnag    Locationwall         No.eggs          Length        Nestling  \n        0.6949        -18.3127          0.7950         -0.2194          0.3983  \n\nDegrees of Freedom: 85 Total (i.e. Null);  76 Residual\nNull Deviance:      108.5 \nResidual Deviance: 47.29    AIC: 67.29"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#final-forward-aic",
    "href": "handout/2025-03-03-ROC-onestep.html#final-forward-aic",
    "title": "ROC + Adding Variables",
    "section": "Final Forward AIC",
    "text": "Final Forward AIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Nestling,\n    data = nests, family=\"binomial\") |&gt; tidy()\n\n# A tibble: 10 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      11.1   3328.      0.00334  0.997  \n 2 Length           -0.219    0.0754 -2.91     0.00364\n 3 Locationconif   -19.3   3328.     -0.00580  0.995  \n 4 Locationdecid   -16.9   3328.     -0.00507  0.996  \n 5 Locationground  -20.5   3328.     -0.00617  0.995  \n 6 Locationshrub   -18.6   3328.     -0.00560  0.996  \n 7 Locationsnag      0.695 4313.      0.000161 1.00   \n 8 Locationwall    -18.3   3328.     -0.00550  0.996  \n 9 No.eggs           0.795    0.262   3.04     0.00238\n10 Nestling          0.398    0.144   2.76     0.00577"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#automatic-backward-w-bic",
    "href": "handout/2025-03-03-ROC-onestep.html#automatic-backward-w-bic",
    "title": "ROC + Adding Variables",
    "section": "Automatic: Backward w BIC",
    "text": "Automatic: Backward w BIC\n\nglm(`Closed?` ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling,\n    data = nests, family=\"binomial\") |&gt;\n  stats::step(scope = ~ Length + Location + No.eggs + Color +\n         Incubate + Nestling, \n         direction = \"backward\", k = log(86))\n\nStart:  AIC=99.7\n`Closed?` ~ Length + Location + No.eggs + Color + Incubate + \n    Nestling\n\n           Df Deviance     AIC\n- Location  6   66.017  92.743\n- Incubate  1   46.580  95.577\n- Color     1   46.997  95.995\n&lt;none&gt;          46.252  99.704\n- Length    1   52.812 101.810\n- No.eggs   1   56.049 105.047\n- Nestling  1   56.989 105.986\n\nStep:  AIC=92.74\n`Closed?` ~ Length + No.eggs + Color + Incubate + Nestling\n\n           Df Deviance     AIC\n- Incubate  1   66.175  88.447\n- Color     1   66.664  88.935\n&lt;none&gt;          66.017  92.743\n- No.eggs   1   74.635  96.907\n- Length    1   75.000  97.272\n- Nestling  1   85.891 108.163\n\nStep:  AIC=88.45\n`Closed?` ~ Length + No.eggs + Color + Nestling\n\n           Df Deviance     AIC\n- Color     1   66.762  84.579\n&lt;none&gt;          66.175  88.447\n- No.eggs   1   75.577  93.395\n- Length    1   79.115  96.932\n- Nestling  1   89.064 106.881\n\nStep:  AIC=84.58\n`Closed?` ~ Length + No.eggs + Nestling\n\n           Df Deviance     AIC\n&lt;none&gt;          66.762  84.579\n- No.eggs   1   76.704  90.067\n- Length    1   79.162  92.525\n- Nestling  1   90.053 103.416\n\n\n\nCall:  glm(formula = `Closed?` ~ Length + No.eggs + Nestling, family = \"binomial\", \n    data = nests)\n\nCoefficients:\n(Intercept)       Length      No.eggs     Nestling  \n    -6.7711      -0.1871       0.6476       0.4062  \n\nDegrees of Freedom: 85 Total (i.e. Null);  82 Residual\nNull Deviance:      108.5 \nResidual Deviance: 66.76    AIC: 74.76"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#final-backward-bic",
    "href": "handout/2025-03-03-ROC-onestep.html#final-backward-bic",
    "title": "ROC + Adding Variables",
    "section": "Final Backward BIC",
    "text": "Final Backward BIC\n\nglm(`Closed?` ~ Length + No.eggs + Nestling,\n    data = nests, family=\"binomial\") |&gt; tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -6.77     1.73       -3.90 0.0000946\n2 Length        -0.187    0.0598     -3.13 0.00177  \n3 No.eggs        0.648    0.245       2.65 0.00815  \n4 Nestling       0.406    0.107       3.78 0.000156"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#different-predictions-for-test-data",
    "href": "handout/2025-02-26-tidymodels.html#different-predictions-for-test-data",
    "title": "Tidymodels",
    "section": "Different predictions for test data",
    "text": "Different predictions for test data\nNote that you now have to be predicted 0.75 to be a squirrel!\n\nfeeder_test_pred_0.75 &lt;- predict(object = feeder_fit, new_data = feeder_test, type = \"prob\") |&gt;\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels &gt;=0.75,\n                                        \"squirrels\", \"no squirrels\"))) |&gt;\n  bind_cols(feeder_test |&gt; select(squirrels))\n\nfeeder_test_pred_0.75\n\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class  squirrels   \n                  &lt;dbl&gt;           &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt;       \n 1               0.201            0.799 squirrels    no squirrels\n 2               0.156            0.844 squirrels    no squirrels\n 3               0.352            0.648 no squirrels no squirrels\n 4               0.197            0.803 squirrels    squirrels   \n 5               0.121            0.879 squirrels    squirrels   \n 6               0.272            0.728 no squirrels squirrels   \n 7               0.0459           0.954 squirrels    squirrels   \n 8               0.0341           0.966 squirrels    squirrels   \n 9               0.0631           0.937 squirrels    squirrels   \n10               0.0312           0.969 squirrels    squirrels   \n# ℹ 58,912 more rows"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#accuracy-for-different-prediction-cutoff",
    "href": "handout/2025-02-26-tidymodels.html#accuracy-for-different-prediction-cutoff",
    "title": "Tidymodels",
    "section": "Accuracy for different prediction cutoff",
    "text": "Accuracy for different prediction cutoff\n\nrbind(accuracy(data = feeder_test_pred_0.75, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.744\n2 sensitivity binary         0.803\n3 specificity binary         0.497\n\nrbind(accuracy(data = feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2025-02-26-tidymodels.html#different-prediction-cutoff",
    "href": "handout/2025-02-26-tidymodels.html#different-prediction-cutoff",
    "title": "Tidymodels",
    "section": "Different prediction cutoff",
    "text": "Different prediction cutoff\n\nrbind(accuracy(data = feeder_test_pred_0.75, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred_0.75, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary         0.744\n2 sensitivity binary         0.803\n3 specificity binary         0.497\n\nrbind(accuracy(data = feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(data = feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#backward--4",
    "href": "handout/2025-03-03-ROC-onestep.html#backward--4",
    "title": "ROC + Adding Variables",
    "section": "Backward -4",
    "text": "Backward -4\n\nglm(`Closed?` ~ Length + Location + No.eggs + Totcare, \n    data = nests, family=\"binomial\") |&gt;\n  drop1(test = \"Chisq\")\n\nSingle term deletions\n\nModel:\n`Closed?` ~ Length + Location + No.eggs + Totcare\n         Df Deviance    AIC     LRT  Pr(&gt;Chi)    \n&lt;none&gt;        49.839 69.839                      \nLength    1   60.751 78.751 10.9116 0.0009556 ***\nLocation  6   69.236 77.236 19.3974 0.0035425 ** \nNo.eggs   1   61.940 79.940 12.1013 0.0005039 ***\nTotcare   1   58.229 76.229  8.3902 0.0037725 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "handout/2025-03-03-ROC-onestep.html#final-model-comparison",
    "href": "handout/2025-03-03-ROC-onestep.html#final-model-comparison",
    "title": "ROC + Adding Variables",
    "section": "Final model comparison",
    "text": "Final model comparison\nForward (AIC)\n\nglm(`Closed?` ~ Length + Location + No.eggs + Nestling,\n    data = nests, family=\"binomial\") |&gt; tidy()\n\n# A tibble: 10 × 5\n   term           estimate std.error statistic p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      11.1   3328.      0.00334  0.997  \n 2 Length           -0.219    0.0754 -2.91     0.00364\n 3 Locationconif   -19.3   3328.     -0.00580  0.995  \n 4 Locationdecid   -16.9   3328.     -0.00507  0.996  \n 5 Locationground  -20.5   3328.     -0.00617  0.995  \n 6 Locationshrub   -18.6   3328.     -0.00560  0.996  \n 7 Locationsnag      0.695 4313.      0.000161 1.00   \n 8 Locationwall    -18.3   3328.     -0.00550  0.996  \n 9 No.eggs           0.795    0.262   3.04     0.00238\n10 Nestling          0.398    0.144   2.76     0.00577\n\n\nBackward (BIC)\n\nglm(`Closed?` ~ Length + No.eggs + Nestling,\n    data = nests, family=\"binomial\") |&gt; tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -6.77     1.73       -3.90 0.0000946\n2 Length        -0.187    0.0598     -3.13 0.00177  \n3 No.eggs        0.648    0.245       2.65 0.00815  \n4 Nestling       0.406    0.107       3.78 0.000156"
  }
]