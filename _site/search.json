[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Schedule",
    "section": "",
    "text": "Here’s your roadmap for the semester! Each week, follow the general process outlined below:\n\nEnjoy the notes / text \nAttend class, review the warm-up and solutions if you have any questions after completing it during class.\nComplete the HW assignment ( pdf &  Rmd linked below),  submit the assignment via Gradescope accessed on the course Canvas site. HW is due weekly on Wednesday at 11:59pm.\nDiscuss the reflection questions  and ethics considerations  (see the  class notes) with your classmates, mentor, and professor.\nThe textbook is Practicing Statistics, by Kuiper and Sklar.\n\n\n\n\n\n\n\n\n \n  \n    date \n    agenda \n    readings \n    homework \n    article (Tues) \n    warm-ups \n  \n \n\n  \n    Week 1  1.17.23 \n    • t-tests + • SLR \n    Kuiper & Sklar: 2   introduction   t-test   t-test as SLR \n     \n     none   Emma Benn \n     WU 1   WU 2 \n  \n  \n    Week 2  1.24.23 \n    • SLR  • contingency analysis \n    Kuiper & Sklar: 2 & 6   SLR   Fisher's Exact Test   categorical analysis \n     HW1 pdf   HW1 Rmd   HW1 turn-in \n    A/B testing at Netflix   Rafael Irizarry \n     WU 3   WU 4 \n  \n  \n    Week 3  1.31.23 \n    • contingency analysis \n    Kuiper & Sklar: 6   types of studies   RR and CI   OR and CI \n     HW2 pdf   HW2 Rmd   HW2 turn-in \n    what is efficacy?  efficacy   Desi Small-Rodriguez \n     WU 5   WU 6 \n  \n  \n    Week 4  2.7.23 \n    • logistic regression \n    Kuiper & Sklar: 7   logistic regression   MLE \n     HW3 pdf   HW3 Rmd   HW3 turn-in \n    intersectional data   David Blackwell \n     WU 7   WU 8 \n  \n  \n    Week 5  2.14.23 \n    • logistic regression \n    Kuiper & Sklar: 7   inference, logistic regression   multiple logistic regression \n     HW4 pdf   HW4 Rmd   HW4 turn-in \n    Obesity in Children Plummets   JAMA article here (see Table 6)  Kim Sellers \n     WU 9   WU 10 \n  \n  \n    Week 6  2.21.23 \n    • logistic regression \n    Kuiper & Sklar: 7   model building   ROC \n     \n    missing data   Robert Santos \n     \n  \n  \n    Week 7  2.28.23 \n    • modeling \n     cross validation   `tidymodels` \n     \n    placebo effect (read abstract and Fig 3)   Jacqueline Hughes-Oliver \n     \n  \n  \n    Week 8  3.7.23 \n    review & catch-up \n    exam 1 in class +  3.9.23 \n     \n     see Canvas for sample exam 1 Q & sol \n     \n  \n  \n    3.14.23 \n    Spring Break \n     \n     \n     \n     \n  \n  \n    Week 9  3.21.23 \n    • survival analysis \n    Kuiper & Sklar: 9   time to event    KM curves   KM CI \n     \n     \n     \n  \n  \n    Week 10  3.28.23 \n    • survival analysis \n    Kuiper & Sklar: 9   log rank tests    hazard functions \n     \n     \n     \n  \n  \n    Week 11  4.4.23 \n    • survival analysis \n    Kuiper & Sklar: 9   Cox PH model    multiple Cox PH \n     \n    race-conscious medicine \n     \n  \n  \n    Week 12  4.11.23 \n    • survival analysis \n    Kuiper & Sklar: 9   assessing PH    most published research \n     \n    most published research  science problems  abuse of power \n     \n  \n  \n    Week 13  4.18.23 \n    • multiple comparisons \n     multiple comparisons    false discovery rate   interim analyses \n     \n    pausing trials \n     \n  \n  \n    Week 14  4.25.23 \n    review & catch-up \n    exam 2 in class +  4.27.23 \n     \n     see Canvas for sample exam 2 Q & sol \n     \n  \n  \n    Week 15  5.2.23 \n    • Poisson regression \n    Kuiper & Sklar: 9   Poisson regression  \n     \n     \n     \n  \n  \n    Tuesday  5.9.23  2-5pm \n    Final Project  \n     \n     \n     \n     \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker_study.html",
    "href": "clicker_study.html",
    "title": "Methods in Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Practicing Statistics by Kuiper & Sklar. Math 150 - Methods in Biostatistics.\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you should know at least a little bit (hopefully a lotta bit) about the following topics.\n\nHypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-square test.1\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics\n\nInteraction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.2\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\n\nThe Central Limit Theorem (CLT) says:3\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nThe p-value is the probability:4\n\nthat the null hypothesis is true given the observed data.\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\n\nWhy do we use a t distribution (instead of a z / normal distribution) in the t-test?5\n\nthe technical conditions don’t hold\nthe means are quite variable\nwe like the letter t\nwe have two samples\nwe don’t know the true standard deviation parameter\n\n\n\n\nWhat happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?6\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\nthe software won’t give a p-value as output\nthe rejection region needs to be calculated in the opposite direction\nthe world blows up\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_i\\)?7\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_iy_i\\)?8\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nThe regression technical conditions include:9\n\nThe Y variable is normally distributed\nThe X variable is normally distributed\nThe residuals are normally distributed\nThe slope coefficient is normally distributed\nThe intercept coefficient is normally distributed\n\n\n\n\nWe need the technical conditions to hold in order to calculate \\(b_0\\) and \\(b_1.\\)10\n\nTRUE\nFALSE\nIt depends\n\n\n\n\nWhy do we check technical conditions?11\n\nso that the inference is valid\nso that the estimates are valid\nso that the p-value is more likely to be small\nso that the confidence level is right\nfor fun\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?12\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\nWith a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?13\n\nhappiness causes longer lives\nlonger lives cause happiness\nhappiness and longer life are correlated\nhappiness and longer life are perfectly predictive\nhappiness and longer life are unrelated\n\n\n\n\nIf there is no relationship in the population (true correlation = 0), then r = 0.14\n\nTRUE\nFALSE\n\n\n\n\nIf there is no relationship in the population (true slope \\(\\beta_1 = 0\\)), then \\(b_1=0\\).15\n\nTRUE\nFALSE\n\n\n\n\nSmaller variability around the regression line (\\(\\sigma\\)):16\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nSmaller variability in the explanatory variable (SD(X) = \\(s_X\\)):17\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nA smaller sample size (\\(n\\)):18\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nWe transform our variables…19\n\n… to find the highest \\(r^2\\) value.\n… when the X variable is not normally distributed.\n… to make the model easier to interpret.\n… so that the technical conditions are met.\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?20\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\n“Observed data or more extreme” is:21\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?22\n\n0\n9\n5.3\n11\n15\n\n\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?23\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?24\n\nSubstantially greater than 50%\nSubstantially less than 50%\nClose to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions, but I didn’t happen to this week.\nI can’t make the scheduled office hours / mentor sessions\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA. What is measurable?25\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?26\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA. What is measurable?27\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nCalcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.28\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nIt is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.29\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nWhen we select individuals based on the explanatory variable, we cannot accurately measure30\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nRelative Risk is31\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nThe odds ratio is “invariant to which variable is explanatory and which is response” means:32\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nIn finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?33\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\nBecause taking the natural log of the RR makes the distribution approximately normal.\nBecause the natural log compresses values that are bigger than 1 and spreads values that are smaller than 1.\nBecause we can get exact p-values using Fisher’s Exact Test.\n\n\n\n\nIn order to find a CI for the true OR, our steps are:34\n\n\nfind \\(\\widehat{\\ln(\\mbox{OR})}\\)\nadd \\(\\pm \\ z^* \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\\)\ntake exp of the endpoints\n\n\nbecause the sampling distribution of \\(\\widehat{\\mbox{OR}}\\) is normal\nbecause OR is typically greater than 1\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\nI know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW solutions35\n\nTRUE\nFALSE\n\n\n\n\nAt the value \\(x = -\\beta_0 / \\beta_1\\), the probability of success is:36\n\n0\n0.5\n1\ndepends on \\(\\beta_0\\)\ndepends on \\(\\beta_1\\)\n\n\n\n\nThe logistic model gives probability of failure:37\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of success:38\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of failure:39\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nWith a logistic regression model, the relative risk of success (for a one unit increase in X) is:40\n\n\\(- \\beta_0/\\beta_1\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\na non-linear function of X (which depends on X )\n\n\n\n\nIf we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:41\n\nlinear\nlogistic\ncomplementary log-log\nlog-linear\n\n\n\n\nYou take a sample of size 4 from a binary population and get: FSFF. (failure, success, failure, failure) What is your guess for p = P(success)?42\n\n0.05\n0.15\n0.25\n0.5\n0.75\n\n\n\n\nIn a logistic regression model, the variability is given by43\n\nNormal Y given X\nBinomial Y given X\nBernoulli Y given X\nPoisson Y given X\n\n\n\n\nWhen trying to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\), we maximize the likelihood. \\[\\prod_{i=1}^n \\bigg(\\frac{e^{\\beta_0+ \\beta_1 x_i}}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{y_i}\\bigg(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{1 - y_i}\\] Take the derivative with respect to which variable(s):44\n\nX\nY\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\nMaximum likelihood estimation seeks to:45\n\nFind the data which are most likely under the model.\nFind the parameters which are most likely under the model.\nFind the parameters which make the data most likely under the model.\nFind the data which make the parameters most likely under the model.\n\n\n\n\nWe use maximum likelihood estimation because:46\n\nIt gives an principled approach for estimating the parameters.\nThe estimates are asymptotically normally distributed.\nThe estimates are always easy to compute.\nAll of the above.\nSome of the above.\n\n\n\n\nWe know that for a given data set (with MLEs of \\(b_0\\),\\(b_1\\)):47\n\n\\(L(b_0,b_1)< L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1)> L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\leq L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\n\n\n\nFootnotes\n\n\npreferably d or e. maybe c on some of them.↩︎\nthese are the topics we will be covering. Would be nice if you have heard of them.↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n↩︎\n\nwe don’t know the true standard deviation parameter\n\n↩︎\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\n\n↩︎\n\n\\(n_2\\)\n\n↩︎\n\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n↩︎\n\nThe residuals are normally distributed (which induces a., d., and e.). There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).\n\n↩︎\nFALSE. We can always minimize the sums of squares, regardless of whether or not the model is any good.↩︎\n\nso that the inference is valid (and also for fun). Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the “inference” paradigm.\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nhappiness and longer life are correlated\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nso that the technical conditions are met.\n\n↩︎\n\nrandom allocation\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\ncase-control (they selected based on people who had died or not)\n\n↩︎\n\ncross-classification (they selected all uses of catheters)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category (tbh, we can’t measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\n\n↩︎\n\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\n\n↩︎\nThe warm-up solutions and clicker questions are on the main course website. The HW solutions are on Canvas under Files.↩︎\n\n0.5\n\n↩︎\n\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\n↩︎\n\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\n↩︎\n\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\n↩︎\n\na non-linear function of X (which depends on X )\n\n↩︎\n\nlog-linear\n\n↩︎\n\n0.25\n\n↩︎\n\nBernoulli Y given X\n\n↩︎\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n↩︎\n\nFind the parameters which make the data most likely under the model.\n\n↩︎\n\nSome of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)\n\n↩︎\n\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "syllabus",
    "section": "",
    "text": "Class: Tuesdays & Thursdays, 1:15-2:30pm\nJo Hardin\n2351 Estella\njo.hardin@pomona.edu\n\n\nMondays 1:30-3pm\nTuesdays 2:30-3:30pm\nWednesday 9-11am\nThursday 3-4pm\nor by appointment\n\n\nMonday 6-8pm\nWednesday 8-10pm\nEstella 2131\n\n\n\n\n\nArtwork by @allison_horst.\n\n\n\n\n\n\n\n\n\nMethods in Biostatistics is a second course in biostatistics, designed to follow either an Introduction to Statistics or Introduction to Biostatistics course. No biology background is needed, but examples and methods will be focused on those found in the life sciences. In particular, the main statistical topics covered include a logistic regression, survival analysis, and methods to ameliorate multiple comparison issues.\n\n\n\n\n\n\nAnonymous Feedback\n\n\n\nAs someone who is, myself, constantly learning and growing in many ways, I welcome your feedback about the course, the classroom dynamics, or anything else you’d like me to know. There is a link to an anonymous feedback form on the landing page of our Canvas webpage. Please provide me with feedback at any time!\n\n\n\n\n\nBy the end of the semester, students will be able to do the following:\n\nevaluate quantitative information with regards to clinical and biological data. We’ll be sure to keep in mind:\n\nCareful presentation of data\nConsideration of variability\nMeaningful comparisons\n\ncritically evaluate the medical literature with respect to design, analysis, and interpretation of results.\nunderstand the role of inherent variability and keep it in perspective when inferring results to a population.\ncritically evaluate medical results given in the mainstream media.\nread published studies with skepticism. Some people (in all fields!) wrongly believe that all studies published in a peer review publication must be 100% accurate and/or well designed studies. In this course, you will learn the tools to recognize, interpret, and critique statistical results in medical literature.\n\n\n\n\nIn an ideal world, science would be objective. However, much of science is subjective and is historically built on a small subset of privileged voices. In this class, we will make an effort to recognize how science (and statistics!) has played a role in both understanding diversity as well as in promoting systems of power and privilege. I acknowledge that there may be both overt and covert biases in the material due to the lens with which it was written, even though the material is primarily of a scientific nature. Integrating a diverse set of experiences is important for a more comprehensive understanding of science. I would like to discuss issues of diversity in statistics as part of the course from time to time.\nPlease contact me if you have any suggestions to improve the quality of the course materials.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this:\n\nIf you have a name and/or set of pronouns that differ from those that appear in your official records, please let me know!\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. You can also relay information to me via your mentors. I want to be a resource for you. If you prefer to speak with someone outside of the course, the math liaisons, Dean of Students, or QSC staff are all excellent resources.\n\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to me about it. As a participant in course discussions, you should also strive to honor the diversity of your classmates.\n\n\n\n\n\nPracticing Statistics, by Kuiper & Sklar\n\n\n\n\n\n\nExam dates\n\n\n\nExam 1 – Thursday, March 9th\nExam 2 – Thursday, April 27th\nFinal Project due – Tuesday, May 9th, 5pm\n\n\n\n\n\n\nEnough R\nR tutorial\nGreat tutorials through the Coding Club\nA true beginner’s introduction to the tidyverse, the introverse.\nfor a good start to R in general\nA fantastic ggplot2 tutorial\nGreat tutorials through the Coding Club\nGoogle for R\nsome R ideas that I wrote up\nIncredibly helpful cheatsheets from RStudio.\n\ndata wrangling\nggplot2\nR Markdown\nRStudio IDE\n\n\n\n\n\nR will be used for all homework assignments. You can use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to get Pomona login information.)\nAlternatively, feel free to download R onto your own computer. R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, you are required to install RStudio and turn in all R assignments using RMarkdown. http://rstudio.org/. (You can use the LaTeX compiler at: https://yihui.name/tinytex/)\n\n\n\nThis course uses Canvas as the main learning management system. The Canvas login is http://canvas.pomona.edu/. If you haven’t used Canvas before, I recommend bookmarking Canvas Student Guides and Canvas Student Videos for easy reference to tips and tutorials. If you run into an issue with Canvas, help is available.\n\nFrom anywhere in Canvas, select the Help button, located in the blue Global Navigation menu on the left.\n\nClick on Pomona Service Desk - Canvas Support to report a problem by submitting a service request ticket. Be sure to include “Canvas Issue” in your subject line.\nFor additional assistance, you can click on Ask Your Instructor or simply send me an email.\n\n\nPlease be proactive and reach out for help as soon as possible to resolve the issue you are experiencing.\n\n\n\n\n\n\nThe prerequisites for this class are Introductory Statistics (Math 58 or equivalent) and completion of one semester of calculus. We rely heavily on these prerequisites, and students with no background in statistics or very light mathematics background will find themselves trying to catch up throughout the semester. You should be familiar with topics such as probability, confidence intervals, hypothesis testing, p-values, linear regression.\n\n\n\nHomework will be assigned from the text and due every Wednesday at 11:59pm. One homework grade will be automatically dropped, so there are no late assignments. Homework will be turned in via Gradescope on Canvas.\n\n\n\nThere will be one project at the end of the semester based primarily on the survival analysis material. You will be able to work in pairs or alone. More information to come on the project.\n\n\n\nThroughout the semester, you will be challenged, and you may find yourself stuck. Every single one of us has been there, I promise. Below, I’ve provided Pomona’s academic honesty policy. But before the policy, I’ve given some thoughts on cheating which I have taken from Nick Ball’s CHEM 147 Collective (thank you, Prof Ball!). Prof Ball gives us all something to think about when we are learning in a classroom as well as on our journey to become scientists and professionals:\n\n\n\n\n\n\nWhy Cheat?\n\n\n\nThere are many known reasons why we may feel the need to “cheat” on problem sets or exams:\n\nAn academic environment that values grades above learning.\nFinancial aid is critical for remaining in school that places undue pressure on maintaining a high GPA.\nNavigating school, work, and/or family obligations that have diverted focus from class.\nChallenges balancing coursework and mental health.\nBalancing academic, family, peer, or personal issues.\n\nBeing accused of cheating – whether it has occurred or not – can be devastating for students. The college requires me to respond to potential academic dishonesty with a process that is very long and damaging. As your instructor, I care about you and want to offer alternatives to prevent us from having to go through this process.\n\n\nIf you find yourself in a situation where “cheating” seems like the only option, please come talk to me. We will figure this out together.\nPomona College is an academic community, all of whose members are expected to abide by ethical standards both in their conduct and in their exercise of responsibilities toward other members of the community. The college expects students to understand and adhere to basic standards of honesty and academic integrity. These standards include, but are not limited to, the following:\n\nIn projects and assignments prepared independently, students never represent the ideas or the language of others as their own.\nStudents do not destroy or alter either the work of other students or the educational resources and materials of the College.\nStudents neither give nor receive assistance in examinations.\nStudents do not take unfair advantage of fellow students by representing work completed for one course as original work for another or by deliberately disregarding course rules and regulations.\nIn laboratory or research projects involving the collection of data, students accurately report data observed and do not alter these data for any reason.\n\n\n\n\nPlease email and / or set up a time to talk if you have any questions about or difficulty with the material, the computing, or the course. Talk to me as soon as possible if you find yourself struggling. The material will build on itself, so it will be much easier to catch up if the concepts get clarified earlier rather than later. This semester is going to be fun. Let’s do it.\n\n\n\n\n\n\nGrading\n\n\n\n\n25% Homework\n50% Midterms\n20% Final Project\n5% Class Participation"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "clicker.html",
    "href": "clicker.html",
    "title": "Methods in Biostatistics",
    "section": "",
    "text": "Clicker Q\nto go with Practicing Statistics by Kuiper & Sklar. Math 150 - Methods in Biostatistics.\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you should know at least a little bit (hopefully a lotta bit) about the following topics.\n\nHypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-square test.1\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\nIn terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics\n\nInteraction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.2\n\nNever heard of it\nHeard of it, but don’t know anything about it\nKnow a little about it (or did once)\nKnow something about it\nConfident about it\n\n\n\n\nThe Central Limit Theorem (CLT) says:3\n\nThe sample average (statistic) converges to the true average (parameter)\nThe sample average (statistic) converges to some point\nThe distribution of the sample average (statistic) converges to a normal distribution\nThe distribution of the sample average (statistic) converges to some distribution\nI have no idea what the CLT says\n\n\n\n\nThe p-value is the probability:4\n\nthat the null hypothesis is true given the observed data.\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n\n\n\nWhy do we use a t distribution (instead of a z / normal distribution) in the t-test?5\n\nthe technical conditions don’t hold\nthe means are quite variable\nwe like the letter t\nwe have two samples\nwe don’t know the true standard deviation parameter\n\n\n\n\nWhat happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?6\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\nthe software won’t give a p-value as output\nthe rejection region needs to be calculated in the opposite direction\nthe world blows up\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_i\\)?7\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nWe use linear regression to run a test of means (\\(x_i = 0\\) for controls, group 1; \\(x_i = 1\\) for cases, group 2) What is: \\(\\sum_i x_iy_i\\)?8\n\n\\(n\\)\n\\(n_1\\)\n\\(n_2\\)\n\\(n_1 \\cdot \\overline{y}_1\\)\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n\n\n\nThe regression technical conditions include:9\n\nThe Y variable is normally distributed\nThe X variable is normally distributed\nThe residuals are normally distributed\nThe slope coefficient is normally distributed\nThe intercept coefficient is normally distributed\n\n\n\n\nWe need the technical conditions to hold in order to calculate \\(b_0\\) and \\(b_1.\\)10\n\nTRUE\nFALSE\nIt depends\n\n\n\n\nWhy do we check technical conditions?11\n\nso that the inference is valid\nso that the estimates are valid\nso that the p-value is more likely to be small\nso that the confidence level is right\nfor fun\n\n\n\n\nWhen writing the regression equation, why is there a hat ( ^) on the response variable?12\n\nbecause the prediction is an estimate\nbecause the prediction is an average\nbecause the prediction may be due to extrapolation\na & b\nall of the above\n\n\n\n\nWith a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?13\n\nhappiness causes longer lives\nlonger lives cause happiness\nhappiness and longer life are correlated\nhappiness and longer life are perfectly predictive\nhappiness and longer life are unrelated\n\n\n\n\nIf there is no relationship in the population (true correlation = 0), then r = 0.14\n\nTRUE\nFALSE\n\n\n\n\nIf there is no relationship in the population (true slope \\(\\beta_1 = 0\\)), then \\(b_1=0\\).15\n\nTRUE\nFALSE\n\n\n\n\nSmaller variability around the regression line (\\(\\sigma\\)):16\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nSmaller variability in the explanatory variable (SD(X) = \\(s_X\\)):17\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nA smaller sample size (\\(n\\)):18\n\nincreases the variability of \\(b_1\\).\ndecreases the variability of \\(b_1\\).\ndoesn’t necessarily change the variability of \\(b_1\\).\n\n\n\n\nWe transform our variables…19\n\n… to find the highest \\(r^2\\) value.\n… when the X variable is not normally distributed.\n… to make the model easier to interpret.\n… so that the technical conditions are met.\n\n\n\n\nIn the Botox and Pain Relief example, the p-value is calculated. What does “probability” refer to?20\n\nrandom allocation\nrandom sample\n\n\n\np-value = probability of the observed data or more extreme given the null hypothesis is true.\n\n\n“Observed data or more extreme” is:21\n\nfewer than 9\n9 or fewer\n9 or more\nmore than 9\n\n\n\n\nWhat is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?22\n\n0\n9\n5.3\n11\n15\n\n\n\n\nWhat conclusion would you draw from the Back Pain and Botox study?23\n\nNot enough evidence to conclude that Botox is more effective than the placebo.\nStrong evidence that Botox is equally as effective as the placebo.\nStrong evidence that Botox is more effective than the placebo.\n\n\n\n\nIf we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?24\n\nSubstantially greater than 50%\nSubstantially less than 50%\nClose to 50%\n\n\n\n\nMaterial check-in\n\nSo far, so good\nConcepts are good, R is confusing\nR is good, concepts are confusing\nEverything is confusing\n\n\n\n\nPeople check-in\n\nSo far, so good\nI can go to office hours / mentor sessions, but I didn’t happen to this week.\nI can’t make the scheduled office hours / mentor sessions\nI’m looking for someone to study with\n\n\n\nSee Canvas front page for anonymous survey / feedback for the class. Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.\n\n\nSample 1,000,000 people who are over 6’ tall and 1,000,000 people who are under 6’ tall. Record if the person is in the NBA. What is measurable?25\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6’ tall. What is measurable?26\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nSample 10,000,000 people. Record their height and whether or not they are in the NBA. What is measurable?27\n\nP(NBA if 6’ tall)\nP(6’ tall if in the NBA)\nboth\nneither\n\n\n\n\nCalcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.28\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nIt is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.29\n\nCase-control\nCohort\nCross-classification\n\n\n\n\nWhen we select individuals based on the explanatory variable, we cannot accurately measure30\n\nthe proportion of people in the population in each explanatory category\nthe proportion of people in the population in each response group\nanything about the population\nconfounding variables\n\n\n\n\nRelative Risk is31\n\nthe difference of two proportions\nthe ratio of two proportions\nthe log of the ratio of two proportions\nthe log of the difference of two proportions\n\n\n\n\nThe odds ratio is “invariant to which variable is explanatory and which is response” means:32\n\nwe always put the bigger odds in the numerator\nwe must collect data so that we can estimate the response in the population\nwhich variable is called the explanatory changes the value of the OR\nwhich variable is called the explanatory does not change the value of the OR\n\n\n\n\nIn finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?33\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\nBecause taking the natural log of the RR makes the distribution approximately normal.\nBecause the natural log compresses values that are bigger than 1 and spreads values that are smaller than 1.\nBecause we can get exact p-values using Fisher’s Exact Test.\n\n\n\n\nIn order to find a CI for the true OR, our steps are:34\n\n\nfind \\(\\widehat{\\ln(\\mbox{OR})}\\)\nadd \\(\\pm \\ z^* \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\\)\ntake exp of the endpoints\n\n\nbecause the sampling distribution of \\(\\widehat{\\mbox{OR}}\\) is normal\nbecause OR is typically greater than 1\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\nbecause OR is invariant to the choice of explanatory or response variable\n\n\n\nI know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW solutions35\n\nTRUE\nFALSE\n\n\n\n\nAt the value \\(x = -\\beta_0 / \\beta_1\\), the probability of success is:36\n\n0\n0.5\n1\ndepends on \\(\\beta_0\\)\ndepends on \\(\\beta_1\\)\n\n\n\n\nThe logistic model gives probability of failure:37\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of success:38\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nThe logistic model gives odds of failure:39\n\n\\(\\frac{e^{\\beta_0+ \\beta_1 x}}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\n\n\n\nWith a logistic regression model, the relative risk of success (for a one unit increase in X) is:40\n\n\\(- \\beta_0/\\beta_1\\)\n\\(\\beta_0+ \\beta_1 x\\)\n\\(e^{\\beta_0+ \\beta_1 x}\\)\na non-linear function of X (which depends on X )\n\n\n\n\nIf we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:41\n\nlinear\nlogistic\ncomplementary log-log\nlog-linear\n\n\n\n\nYou take a sample of size 4 from a binary population and get: FSFF. (failure, success, failure, failure) What is your guess for p = P(success)?42\n\n0.05\n0.15\n0.25\n0.5\n0.75\n\n\n\n\nIn a logistic regression model, the variability is given by43\n\nNormal Y given X\nBinomial Y given X\nBernoulli Y given X\nPoisson Y given X\n\n\n\n\nWhen trying to find estimates for \\(\\beta_0\\) and \\(\\beta_1\\), we maximize the likelihood. \\[\\prod_{i=1}^n \\bigg(\\frac{e^{\\beta_0+ \\beta_1 x_i}}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{y_i}\\bigg(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x_i}}\\bigg)^{1 - y_i}\\] Take the derivative with respect to which variable(s):44\n\nX\nY\n\\(\\beta_0\\)\n\\(\\beta_1\\)\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n\n\n\nMaximum likelihood estimation seeks to:45\n\nFind the data which are most likely under the model.\nFind the parameters which are most likely under the model.\nFind the parameters which make the data most likely under the model.\nFind the data which make the parameters most likely under the model.\n\n\n\n\nWe use maximum likelihood estimation because:46\n\nIt gives an principled approach for estimating the parameters.\nThe estimates are asymptotically normally distributed.\nThe estimates are always easy to compute.\nAll of the above.\nSome of the above.\n\n\n\n\nWe know that for a given data set (with MLEs of \\(b_0\\),\\(b_1\\)):47\n\n\\(L(b_0,b_1)< L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1)> L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\leq L(b_0,\\beta_1=0)\\) always\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n\n\n\n\n\n\nFootnotes\n\n\npreferably d or e. maybe c on some of them.↩︎\nthese are the topics we will be covering. Would be nice if you have heard of them.↩︎\n\nThe distribution of the sample average (statistic) converges to a normal distribution\n\n↩︎\n\nof data as or more extreme than the observed data given that the null hypothesis is true.\n\n↩︎\n\nwe don’t know the true standard deviation parameter\n\n↩︎\n\nthe p-value isn’t actually the probability of our data or more extreme if H0 is true.\n\n↩︎\n\n\\(n_2\\)\n\n↩︎\n\n\\(n_2 \\cdot \\overline{y}_2\\)\n\n↩︎\n\nThe residuals are normally distributed (which induces a., d., and e.). There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).\n\n↩︎\nFALSE. We can always minimize the sums of squares, regardless of whether or not the model is any good.↩︎\n\nso that the inference is valid (and also for fun). Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the “inference” paradigm.\n\n↩︎\n\ndue to estimation and average\n\n↩︎\n\nhappiness and longer life are correlated\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\nFALSE, there is no reason that the statistic will equal the parameter.\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nincreases the variability of \\(b_1\\).\n\n↩︎\n\ndecreases the variability of \\(b_1\\).\n\n↩︎\n\nso that the technical conditions are met.\n\n↩︎\n\nrandom allocation\n\n↩︎\n\n9 or more\n\n↩︎\n\n5.3 because (15/31)*11 = 5.3\n\n↩︎\n\nStrong evidence that Botox is more effective than the placebo.\n\n↩︎\n\nClose to 50% (the point estimate is 0.6)\n\n↩︎\n\nP(NBA if 6’ tall) (cohort: cannot measure the probability of the explanatory variable given the response)\n\n↩︎\n\nP(6’ tall if in the NBA) (case-control: cannot measure the probability of the response variable given a level of the explanatory variable)\n\n↩︎\n\nboth (cross-classification: can measure all the probabilities)\n\n↩︎\n\ncase-control (they selected based on people who had died or not)\n\n↩︎\n\ncross-classification (they selected all uses of catheters)\n\n↩︎\n\nthe proportion of people in the population in each explanatory category (tbh, we can’t measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)\n\n↩︎\n\nthe ratio of two proportions\n\n↩︎\n\nwhich variable is called the explanatory does not change the value of the OR\n\n↩︎\n\nBecause if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.\n\n↩︎\n\nbecause the \\(\\ln\\) transformation makes the sampling distribution almost normal\n\n↩︎\nThe warm-up solutions and clicker questions are on the main course website. The HW solutions are on Canvas under Files.↩︎\n\n0.5\n\n↩︎\n\n\\(\\frac{1}{1+ e^{\\beta_0+ \\beta_1 x}}\\)\n\n↩︎\n\n\\(e^{\\beta_0+ \\beta_1 x}\\)\n\n↩︎\n\n\\(e^{-(\\beta_0+ \\beta_1 x)}\\)\n\n↩︎\n\na non-linear function of X (which depends on X )\n\n↩︎\n\nlog-linear\n\n↩︎\n\n0.25\n\n↩︎\n\nBernoulli Y given X\n\n↩︎\n\n\\(\\beta_0\\) and \\(\\beta_1\\)\n\n↩︎\n\nFind the parameters which make the data most likely under the model.\n\n↩︎\n\nSome of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)\n\n↩︎\n\n\\(L(b_0,b_1) \\geq L(b_0,\\beta_1=0)\\) always\n\n↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "Class notes can be found at http://st47s.com/Math150/Notes/.\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW1_m150_s23.html",
    "href": "handout/HW1_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "",
    "text": "Practice using R to run t-tests and linear models\nProvide details about what the models mean\n\n\n\nAt the bottom of this assignment, I’ve given an R Tutorial of sorts with some of the main ideas that we’ll be using this semester. You might want to read over them and try out some of the commands yourself.\nPlease ask questions as you go along! Asking questions about the R code (and about the course in general) is your key to success.\n\nLook carefully at the line of code in the R chunk immediately below. Notice that a dataset from the textbook is being loaded in so that it can be analyzed. The datasets for the textbook are all provided to you on Canvas. You’ll need to get the data from Canvas (the file for this HW is called C2 Games1.csv), and copy it either to your own computer or to the file system on the Pomona R Server (if you are using the server). Then you need to point to the location of that dataset.\nIf it is difficult to find the location, look at the upper right side of the R Studio window for the icon that says Import Dataset. By clicking through, import the dataset into R. But wait, you aren’t done! After you import the data, you’ll see the correct path in the Console. That path must be written into the R chunk which imports the data.\nYou will know that you have successfully imported the data if you can knit this file to a pdf without any errors. After you’ve imported the data (and before you move on to the rest of the assignment), try to knit to pdf. Does it work? Great! Does it not work? Read the paragraph above again (then ask questions on Slack).\n\ngames1 <- readr::read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 02/CSV Files/C2 Games1.csv\")\n\n\n\n\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\n\nUse R to calculate a two-sample test statistic (assuming equal variances) and find the p-value corresponding to this statistic. In addition, calculate a 95% confidence interval for the difference between the two means (\\(\\mu_1 - \\mu_2\\)). The end of chapter exercises will provide details on conducting this calculation by hand. If \\(H_0: \\mu_1 = \\mu_2\\) is true, the p-value states how likely that just random sampling variability would create a difference between two sample means (\\(\\overline{y}_1 - \\overline{y}_2\\)) at least as large as we observed. Based on the p-value, what can you conclude about these two types of games?\ngames1 %>%\n    t.test(Time ~ Type, data=., var.equal = TRUE)\n\ngames1 %>%\n    t.test(Time ~ Type, data=., var.equal = TRUE) %>%\n    tidy()\n\n\n\nTo fit a linear model, the Type variable will need to be binary. Fit a linear model in R using lm() and notice which level of Type gets set to 0 and which gets set to 1. How can you tell?\nDevelop a regression model using Time as the response and the indicator on Type as the explanatory variable.\nCreate a linear model (lm()) and then tidy() the model. The following example code might help.\ngames1 %>%\n  lm(Time ~ Type, data = .) %>%\n  summary()\n\ngames1 %>%\n  lm(Time ~ Type, data = .) %>%\n  tidy()\n\n\n\nUse R to calculate the t-statistic and p-value for the hypothesis test \\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\ne 0\\). In addition, construct at 95% confidence interval for \\(\\beta_1\\). Based on these statistics, can you conclude that the coefficient \\(\\beta_1\\) is significantly different from zero? How is the test of \\(\\beta_1\\) equivalent to the t.test() in the earlier question?\nThe argument conf.int = TRUE inside tidy() on the linear model will find confidence intervals for the coefficients.\n\n\n\nAssume you are conducting a t-test to determine if there is a difference between two means. You have the following summary statistics: \\(\\overline{x}_1 = 10, \\overline{x}_2 = 20\\) and \\(s_1=s_2=10\\). Without completing the hypothesis test, explain why \\(n_1=n_2=100\\) would result in a smaller p-value than \\(n_1=n_2=16\\).\n\n\n\nIf the hypothesis test \\(H_0: \\beta_1 = 0\\) vs \\(H_a: \\beta_1 \\ne 0\\) results in a small p-value, can we be confident that the regression model provides a good estimate of the response value for a given value of \\(x_i\\)? Provide an explanation for your answer.\n\n\n\nWhat model technical conditions (if any) need to be satisfied in order to calculate \\(b_0\\) and \\(b_1\\) in a simple linear regression model?\n\n\n\nExplain why the model notation \\(y_i = \\beta_0 + \\beta_1 x_i\\) is not appropriate, but \\(\\hat{y}_i = b_0 + b_1 x_i\\) is appropriate.\n\n\nHW assignments will be graded out of 5 points, which are based on a combination of accuracy and effort. Below are rough guidelines for grading."
  },
  {
    "objectID": "handout/HW1_m150_s23.html#getting-started",
    "href": "handout/HW1_m150_s23.html#getting-started",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "Getting started",
    "text": "Getting started\n\nLoad packages\nHere we will explore the data using functions that can be found in the tidyverse package. The data can be found in the package nycflights13.\nLet’s load the packages.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nThe data\nThe Bureau of Transportation Statistics (BTS) is a statistical agency that is a part of the Research and Innovative Technology Administration (RITA). As its name implies, BTS collects and makes available transportation data, such as the flights data we will be working with here.\nWe begin by loading the flights data frame. Run the following command by clicking on the green triangle:\n\ndata(flights)\n\nThe data set flights that shows up in your workspace is a data matrix, with each row representing an observation and each column representing a variable. R calls this data format a data frame, which is a term that will be used throughout the course. For this data set, each observation is a single flight.\nTo view the names of the variables, run the command\n\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\n\nThis returns the names of the variables in this data frame. The codebook (description of the variables) can be accessed by pulling up the help file (run this line with the green triangle and then look at the box on the bottom right of the RStudio screen):\n\n?flights\n\nOne of the variables refers to the carrier (i.e. airline) of the flight, which is coded according to the following system.\n\ncarrier: Two letter carrier abbreviation.\n\n9E: Endeavor Air Inc.\nAA: American Airlines Inc.\nAS: Alaska Airlines Inc.\nB6: JetBlue Airways\nDL: Delta Air Lines Inc.\nEV: ExpressJet Airlines Inc.\nF9: Frontier Airlines Inc.\nFL: AirTran Airways Corporation\nHA: Hawaiian Airlines Inc.\nMQ: Envoy Air\nOO: SkyWest Airlines Inc.\nUA: United Air Lines Inc.\nUS: US Airways Inc.\nVX: Virgin America\nWN: Southwest Airlines Co.\nYV: Mesa Airlines Inc.\n\n\nA very useful function for taking a quick peek at your data frame and viewing its dimensions and data types is glimpse().\n\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\n\nThe flights data frame is a massive trove of information (336,776 observations!!!). Notice also that the glimpse() function let’s you know how the variables are stored: integer, double (a fancy way to say decimal number), character string, date/time, etc. Let’s think about some questions we might want to answer with these data:\n\nHow delayed were flights that were headed to Los Angeles?\nHow do departure delays vary over months?\nWhich of the three major NYC airports has a better on time percentage for departing flights?\n\n\n\nTidy Structure of Data\nFor plotting, analyses, model building, etc., the data should be structured according to certain principles.\n\nTidy Data: rows (cases/observational units) and columns (variables).\nThe key is that every row is a case and every column is a variable.\nNo exceptions.\nCreating tidy data is often not trivial.\n\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), it is important to understand how to build data using the patterns of the language.\nSome things to consider:\n\nobject_name <- anything is a way of assigning anything to the new object_name.\nobject_name <- function_name(data_frame, arguments) is a way of using a function to create a new object.\nobject_name <- data_frame %>% function_name(arguments) uses chaining syntax as an extension of the ideas of functions.\nIn chaining, the value on the left side of %>% becomes the first argument to the function on the right side.\n\nobject_name <- data_frame %>%\n                    function_name(arguments) %>% \n                    another_function_name(other_arguments)\nis extended chaining. %>% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %>% is always a data table. * The pipe syntax should be read as then, %>%.\n\n\nUsing the pipe to chain\nThe pipe syntax (%>%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function.\nFor example:\nx %>% f(y) is the same as f(x, y)\ny %>% f(x, ., z) is the same as f(x,y,z)\nPipes are used commonly with functions in the dplyr package and they allow us to sequentially build data wrangling operations. We’ll start with short pipes and throughout the course build up to longer pipes that perform multiple operations."
  },
  {
    "objectID": "handout/HW1_m150_s23.html#analysis",
    "href": "handout/HW1_m150_s23.html#analysis",
    "title": "Math 150 - Methods in Biostatistics - Homework 1",
    "section": "Analysis",
    "text": "Analysis\n\nDeparture delays\nLet’s start by examining the distribution of departure delays of all flights using the summary function. The first item (on the left) is the data set. Subsequently, two functions are applied, (1) select() to get only the dep_delay variables, and (2) summary() to produce the numerical summary.\n\nflights %>% \n  dplyr::select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   : -43.00  \n 1st Qu.:  -5.00  \n Median :  -2.00  \n Mean   :  12.64  \n 3rd Qu.:  11.00  \n Max.   :1301.00  \n NA's   :8255     \n\n\n\nfilter (create a smaller dataset)\nIf we want to focus only on departure delays of flights headed to Los Angeles, we need to first filter() the data for flights with that destination (dest == \"LAX\"). The departure delay for the LAX flights can then be summarized.\n\nlax_flights <- flights %>%\n  dplyr::filter(dest == \"LAX\")\n\nlax_flights %>% \n  dplyr::select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\nLet’s decipher these two commands (It’s common to add a break to a new line after %>% to help readability).\n\nCommand 1: Take the flights data frame, filter() for flights headed to LAX, and save the result as a new data frame called lax_flights.\n\n== means “if it is equal to”. (notice that there are TWO equals signs)\nLAX is in quotation marks since it is a character string.\n\nCommand 2: Basically the same call for summarizing the departure delay.\n\nNotice that if we only want the summary of dep_delay for the LAX flights (and we don’t need to keep a copy of the dataset), we can perform the above tasks by combining them into fewer steps:\n\nflights %>%\n  filter(dest == \"LAX\") %>%\n  dplyr::select(dep_delay) %>% \n  summary()\n\n   dep_delay      \n Min.   :-16.000  \n 1st Qu.: -4.000  \n Median : -1.000  \n Mean   :  9.401  \n 3rd Qu.:  7.000  \n Max.   :800.000  \n NA's   :98       \n\n\n\nLogical operators:  Filtering for certain observations (e.g. flights from a particular airport) is often of interest in data frames where we might want to examine observations with certain characteristics separately from the rest of the data. To do so we use the filter() function and a series of logical operators. The most commonly used logical operators for data analysis are as follows:\n\n== means “equal to”\n!= means “not equal to”\n> or < means “greater than” or “less than”\n>= or <= means “greater than or equal to” or “less than or equal to”\n\n\n\n\nsummarize (calculate statistics)\nWe can also obtain numerical summaries for these flights:\n\nlax_flights %>%\n  summarize(mean_dd = mean(dep_delay, na.rm=TRUE), \n            median_dd = median(dep_delay, na.rm=TRUE), n_dd = n())\n\n# A tibble: 1 × 3\n  mean_dd median_dd  n_dd\n    <dbl>     <dbl> <int>\n1    9.40        -1 16174\n\n\nNote that in the summarize() function we created a list of three different numerical summaries that we were interested in. The names of these elements are user defined, like mean_dd, median_dd, n_dd, and you could customize these names as you like (just don’t use spaces in your names). Calculating these summary statistics also require that you know the function calls. Note that n() reports the sample size.\n\nSummary statistics:  Some useful function calls for summary statistics for a single numerical variable are as follows:\n\nmean\nmedian\nsd\nIQR\nmin\nmax\n\nNote that each of these functions take a single vector as an argument, and returns a single value.\n\nFunctions you may not be familiar with (and that we will see in more detail in coming weeks) include: \\[\\mbox{sd} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\\] \\[\\mbox{IQR} = 75\\% - 25\\%\\]\nWe can also filter based on multiple criteria. Suppose we are interested in flights headed to San Francisco (SFO) in February:\n\nsfo_feb_flights <- flights %>%\n  filter(dest == \"SFO\", month == 2)\n\nNote that we can separate the conditions using commas if we want flights that are both headed to SFO and in February. If we are interested in either flights headed to SFO or in February we can use the | instead of the comma.\n\nPractice\nCreate a new data frame that includes flights headed to SFO in February, and save this data frame as sfo_feb_flights. How many flights meet these criteria?\n\nsfo_feb_flights %>%\n  summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  <int>\n1   791\n\n\nDescribe the distribution of the arrival delays of these flights using summary and/or appropriate summary statistics.\n\nsfo_feb_flights %>%\n  summarize(arrdelmean = mean(arr_delay, na.rm=TRUE), \n            arrdelsd = sd(arr_delay, na.rm=TRUE),\n            arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE))\n\n# A tibble: 1 × 4\n  arrdelmean arrdelsd arrdelmed arrdeliqr\n       <dbl>    <dbl>     <dbl>     <dbl>\n1      -9.14     31.4       -13        29\n\n\n\n\n\ngroup_by (group before summarizing)\nAnother useful technique is quickly calculating summary statistics for various groups in your data frame. For example, we can modify the above command using the group_by() function to get the same summary stats for each origin airport:\n\nsfo_feb_flights %>%\n  group_by(origin) %>%\n  summarize(median_dd = median(dep_delay, na.rm=TRUE), \n            iqr_dd = IQR(dep_delay, na.rm=TRUE), n_flights = n())\n\n# A tibble: 2 × 4\n  origin median_dd iqr_dd n_flights\n  <chr>      <dbl>  <dbl>     <int>\n1 EWR            0      9       194\n2 JFK           -2      8       597\n\n\nHere, we first grouped the data by origin, and then calculated the summary statistics.\n\nPractice\nCalculate the median and interquartile range for arr_delay of flights in in the sfo_feb_flights data frame, grouped by carrier. Which carrier has the most variable arrival delays (as measured by IQR)?\n\nsfo_feb_flights %>% \n  group_by(carrier) %>%\n  summarize(arrdelmed = median(arr_delay, na.rm=TRUE), \n            arrdeliqr = IQR(arr_delay, na.rm=TRUE)) %>%\n  arrange(desc(arrdeliqr))\n\n# A tibble: 5 × 3\n  carrier arrdelmed arrdeliqr\n  <chr>       <dbl>     <dbl>\n1 AA             -7      35  \n2 DL            -24      27.5\n3 UA             -9      27  \n4 B6            -11      25.5\n5 VX            -20      23  \n\n\n\n\n\n\narrange() departure delays over months\n\nPractice\nWhich month would you expect to have the highest average delay departing from an NYC airport?\nLet’s think about how we would answer this question:\n\nFirst, calculate monthly averages for departure delays. With the new language we are learning, we need to\n\ngroup_by() months, then\nsummarize() mean departure delays.\n\nThen, we need to arrange() these average delays in desc()ending order\n\n\nflights %>%\n  group_by(month) %>%\n  summarize(mean_dd = mean(dep_delay, na.rm=TRUE)) %>%\n  arrange(desc(mean_dd))\n\n# A tibble: 12 × 2\n   month mean_dd\n   <int>   <dbl>\n 1     7   21.7 \n 2     6   20.8 \n 3    12   16.6 \n 4     4   13.9 \n 5     3   13.2 \n 6     5   13.0 \n 7     8   12.6 \n 8     2   10.8 \n 9     1   10.0 \n10     9    6.72\n11    10    6.24\n12    11    5.44\n\n\n\n\nOn time departure rate for NYC airports\nSuppose you will be flying out of NYC and want to know which of the three major NYC airports has the best on time departure rate of departing flights. Suppose also that for you a flight that is delayed for less than 5 minutes is basically “on time”. You consider any flight delayed for 5 minutes of more to be “delayed”.\nIn order to determine which airport has the best on time departure rate, we need to\n\nfirst classify each flight as “on time” or “delayed”,\nthen group flights by origin airport,\nthen calculate on time departure rates for each origin airport,\nand finally arrange the airports in descending order for on time departure percentage.\n\n\n\nmutate() (create a new variable)\n\nPractice\nLet’s start with classifying each flight as “on time” or “delayed” by creating a new variable with the mutate() function.\n\nflights <- flights %>%\n  mutate(dep_type = ifelse(dep_delay < 5, \"on time\", \"delayed\"))\n\nThe first argument in the mutate() function is the name of the new variable we want to create, in this case dep_type. Then if dep_delay < 5 we classify the flight as \"on time\" and \"delayed\" if not, i.e. if the flight is delayed for 5 or more minutes.\nNote that we are also overwriting the flights data frame with the new version of this data frame that includes the new dep_type variable.\nWe can handle all the remaining steps in one code chunk:\n\nflights %>%\n  group_by(origin) %>%\n  summarize(ot_dep_rate = mean(dep_type == \"on time\", na.rm=TRUE)) %>%\n  arrange(desc(ot_dep_rate))\n\n# A tibble: 3 × 2\n  origin ot_dep_rate\n  <chr>        <dbl>\n1 LGA          0.728\n2 JFK          0.691\n3 EWR          0.639\n\n\n\n\nPractice\nIf you were selecting an airport (of the three NYC airports in the dataset) simply based on on time departure percentage, which NYC airport would you choose to fly out of? (How did you define “on time”? 0 min? 5 min? Something else?)\nLGA seems to have the highest on time departure percentage, so that’s the airport I would choose."
  },
  {
    "objectID": "handout/HW2_m150_s23.html",
    "href": "handout/HW2_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 2",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nRun a least square regression model, try different transformations on the explanatory and response variables to find a model for which the technical conditions hold.\nAnalyze two different datasets using a simulation method (you will need the infer package) as well as Fisher’s Exact Test\nFor plotting and infer code, see the class notes describing the Botox study:\nclick here to link for boxplots and click here to link for infer for simulating\n\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Hippel-Lindau disease\nEisenhofer et al. (1999) investigated the use of plasma normetanephrine and metanephrine for detecting pheochromocytoma in patients with von Hippel-Lindau disease and multiple endocrine neoplasia type 2. The data set (vonHippelLindau.csv, posted online) contains data from this study on 26 patients with von Hippel-Lindau disease and nine patients with multiple endocrineneoplasia. The variables in the data set are (problem from Dupont, chp 2.22, PubMed article at [http://www.ncbi.nlm.nih.gov/pubmed/10369850]):\nNote: the goal is to model p_ne (the response variable) from tumorvol (the explanatory variable).\n\n\n\nvariable\nunits\n\n\n\n\ndisease\n0: patient has von Hippel-Lindau disease\n\n\n\n1: patient has multiple endocrine neoplasia type 2\n\n\np_ne\nplasma norepinephrine (pg/ml)\n\n\ntumorvol\ntumor volume (ml)\n\n\n\nNote: the data this week is imported from the internet, so everyone can use the same link! The directories below do not go to my own computer, they go to a URL pointing to a dataset in the cloud.\n\ntumor <- readr::read_csv(\"http://pages.pomona.edu/~jsh04747/courses/math150/vonHippelLindau.csv\")\nhead(tumor, 3)\n\n# A tibble: 3 × 4\n  disease    id  p_ne tumorvol\n    <dbl> <dbl> <dbl>    <dbl>\n1       0     2  1845      336\n2       0     3  1734      216\n3       0     4   739      128\n\n\n\nRegress plasma norepinephrine against tumor volume. Draw a scatter plot of norepinephrine against tumor volume together with the estimated linear regression curve. What is the slope estimate for this regression? What proportion of the total variation in norepinephrine levels is explained by the regression?\n\n\nR hints:\nIf the linear model is piped into tidy(), the output will be important information on a per parameter basis. For example, coefficients, standard errors, etc.\nIf the linear model is piped into glance(), the output will be important information on a per model basis. For example, \\(R^2\\), overall model p-value, model degrees of freedom, etc.\nIf the linear model is piped into augment(), the output will be be important information on a per observation basis. For example, residuals (.resid), fitted values / predicted values (.fitted), etc.\nTo make a plot in R you want to add a series of layers. The code below is meant as an example, although the variables are totally wrong. Work through the lines of code below and see if you can follow. If you don’t follow the lines, ask me!\n\ntumor %>%                                 # which dataset?\n  ggplot(aes(x = id, y = tumorvol)) +     # set up the plot\n  geom_point() +                          # add the points\n  geom_smooth(method = \"lm\", se = FALSE)  # add the line a linear model without error bounds\n\n\n\n\n\n\n\n\n\nExperiment with different transformations of norepinephrine and tumor volume. Find transformations that provide a good fit to a linear model. Report your new linear model. What is your new \\(R^2\\)? Does the \\(R^2\\) matter in choosing your transformation? Explain.\n\n\n\nR hints:\nFirst transform one or both of your variables (see pg 49 in your text), then re-plot the data. Below is an example, but it turns out that I made a bad choice of transformation because the plot is terrible. Why (what makes the plot look bad)?\n\ntumor %>%\n  ggplot(aes(x = 1/tumorvol, y = p_ne)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nQ3. Regression Conditions\nWhich of the following conditions are required to test hypotheses using simple linear regression? If the condition isn’t valid, explain why not.\n\nThe random variable \\(Y\\) (not conditional on \\(X\\)) is normally distributed.\nThe variance of \\(Y\\) depends on \\(X\\).\nThe random variable \\(Y\\) is normally distributed at each value of \\(X\\).\nThe mean of \\(Y\\) (given \\(X\\)) is a linear function of \\(X\\).\nThe random variable \\(X\\) is randomly distributed on some scale.\n\n\n\nQ4. Chp 6, E1: Cancer and Smoking: Fisher’s Exact Test and Simulations Studies\nAnswer the following questions for the data displayed below. Hint: see the class notes for help with the R code. And ask lots of questions!\n\n\n\n\nlung cancer\nhealthy\n\n\n\n\n\nsmoker\n41\n28\n69\n\n\nnon-smoker\n19\n32\n51\n\n\n\n60\n60\n120\n\n\n\n\nsmokecancer <- data.frame(act = c(rep(\"non-smoker\", 51), rep(\"smoker\", 69)),\n                     outcome = c(rep(\"lung_cancer\", 19), rep(\"healthy\", 32), \n                                 rep(\"lung_cancer\", 41), rep(\"healthy\", 28)))\nsmokecancer %>% table()\n\n            outcome\nact          healthy lung_cancer\n  non-smoker      32          19\n  smoker          28          41\n\n\n\nWas either the explanatory variable (row) or the response (column) variable fixed before the study was conducted?\nIs this an example of an experiment or an observational study?\nIs this a cross-classification, cohort, or case-control study? Explain.\nCreated a segmented bar chart for the data.\nCreate a simulation study to test the one-sided hypothesis that smokers are more likely to have lung cancer. Provide a p-value and state your conclusions.\nUse Fisher’s exact test to test the one-sided hypothesis that smokers are more likely to have lung cancer. Provide a p-value and state your conclusions.\n\n\npraise()\n\n[1] \"You are laudable!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW3_m150_s23.html",
    "href": "handout/HW3_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 3",
    "section": "",
    "text": "knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=4, fig.width=6.5, \n                      fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(praise)\n\n\nAssignment Summary (Goals)\n\nUnderstanding null hypotheses with respect to odds and proportions\nTesting via z-stat, Fisher, Chi-sq (note: if you did all for all scenarios, you’d almost always end up with the same conclusion each time!)\nMaking conclusions about different study types\n(Decided against the Chi Square test, you are not responsible for it.)\n\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 6, A23\nShow that the null hypothesis \\(H_0: p_1 = p_2\\) is mathematically equivalent to the null hypothesis \\(H_0: \\theta_1 / \\theta_2 = 1\\) where \\(p\\) represents the proportion successful and \\(\\theta\\) represents the odds of success for any two groups (labeled 1 and 2).\n\n\nQ3. Chp 6, E7 Cancer Cells: Testing for Homogeneity of Odds\nUse the data from Table 6.1 and define a benign cell as a success. Conduct a hypothesis test for the homogeneity of odds.\n\nEnter the data, tabulate it, and create a barplot (see the R code in the class notes).\nState the null and alternative hypotheses.\nCalculate the odds ratio and the test statistic (the Z statistic!). See pgs 191-192 in your book.\nProvide the p-value and state your conclusions within the context of the study.\n\n\n\nQ4. Chp 6, E12 The Pill Scare: understanding relative risk reduction\nIn October 1995, the United Kingdom Committee on Safety of Medicines (CSM) issued a warning to 190,000 general practitioners, pharmacists, and directors of public health about oral contraceptive pills containing gestodene or desogestrel. The warning, based on three unpublished epidemiological research studies, stated > “It is well known that the pill may rarely produce thrombosis (blood clots) involving veins of the legs. New evidence has become available indicating that the chance of thrombosis occurring in a vein increases about two-fold for some types of pills compared to others.”\nTable 6.15 provides data from one of the studies.\nSince the occurrence of venous thrombosis is very rare (1 in 7000 for people using the second generation pill), 259 subjects were selected who had thrombosis and 651 similar subjects (from hospitals and community) who did not have thrombosis. Then these subjects were classified by the type of contraceptive they used.\n\nWas either the explanatory (row) or the response (column) variable fixed before the study was conducted?\nIs this an example of an experiment or an observational study?\nIs this a cross-classification, cohort, or case-control study?\nCreate a segmented bar chart for the data.\nUse a two-sided hypothesis and Fisher’s exact test to determine if the type of contraceptive impacts the likelihood of thrombosis. Do you expect the researchers took care to collect a simple random sample of subjects? What conclusions can be drawn?\n\nThe warning contained no numerical information other than the fact that the chance of blood clots was likely to double when birth control pills contained gestodene or desogestrel. This warning was widely publicized throughout the press, and evidence suggests that, as a result of this warning, many women ceased contraception altogether. Evidence shows a strong association between the warning and an increase in the number of unintended pregnancies and abortions (especially in women younger than 20 years old). This resulted in an estimated increase in cost of \\(\\textsterling\\) 21 million for maternity care and \\(\\textsterling\\) 4 to \\(\\textsterling\\) 6 million for abortion provision.\n\nRemember that the actual occurrence of venous thrombosis is only 1 in 7000.\nIf third generation pills double the chances of venous thrombosis, the likelihood of occurrence is still only 2 in 7000. Explain the difference between absolute risk reduction and relative risk reduction in this study.\nDeath from venous thrombosis related to third generation pills is estimated to be 1 in 11 million, much lower than the probability of death resulting from pregnancy. In 2005, the lifetime risk of maternal death in developed countries was 1 in 7300. The CSM warning did suggest that patients see a doctor before altering their contraceptives; however, it appears that many women simply stopped taking any contraceptives. Write a brief statement (just 1-2 sentences!) to the press, general practitioners, pharmacists, and directors of public health about this study.\n\n\npraise()\n\n[1] \"You are wonderful!\"\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  },
  {
    "objectID": "handout/HW4_m150_s23.html",
    "href": "handout/HW4_m150_s23.html",
    "title": "Math 150 - Methods in Biostatistics - Homework 4",
    "section": "",
    "text": "Assignment Summary (Goals)\n\nfluent use of the logistic model for prediction and for coefficient interpretation\npractice using ggplot() so that visualizations can inform the larger analysis\n\nNote that if you don’t know the R code either check my notes or ask me!!! Happy to scaffold, debug, send resources, etc. Don’t go down a rabbit hole trying to figure out an R function or syntax.\nAlso, note that you’ll need to get the data from Sakai and use it for this analysis. Look back to your own HW1 file to see the line of code you used to import the games1.csv dataset. Ask me if it isn’t obvious to you after you look at your own HW1.\n\nQ1. Collaborative Learning\nDescribe one thing you learned from someone (a fellow student or mentor) in our class this week (it could be: content, logistical help, background material, R information, etc.) 1-3 sentences.\n\n\nQ2. Chp 7, A1\nBased on the description of the Challenger disaster O-ring concerns, identify which variable in the Shuttle data set in Table 7.1 should be the explanatory variable and which should be the response variable.\n\n\nQ3. Chp 7, A2\nImagine you were an engineer working for Thiokol Corporation prior to January 1986. Create a few graphs of the data in Table 7.1. Is it obvious that temperature is related to the success of the O-rings? Submit any charts or graphs you have created that show a potential relationship between temperature and O-ring damage.\nnote: the data is coded with missing values represented by *. You may need to account for that. See how I did it below using na=\"*\". Again, ask me if you are having trouble!\nnote on graphs: if you tell me the type of graph you want, and you don’t know how to make it, ask me and I’ll send you code! Remember, your response is binary and your explanatory variable is continuous.\nnote on data: in order to get the assignment to work, you’ll need the data. Import it into the folder where the HW .Rmd file lives. Try not to use your downloads for everything!!\n\nshuttle <- read_csv(\"~/Dropbox/teaching/MA150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Shuttle.csv\",\n                     na=\"*\")\n\n# new names that make the data easier to work with:\n# mine loads with an empty 5th column\n# so I had to give the 5th column a name, also.\nnames(shuttle) <- c(\"flight\", \"date\", \"temp\", \"launch\", \"X5\")  \n\n# remove the row that has a missing value for launch\n# also create a character variable for success\nshuttle <- shuttle %>% \n  filter(!is.na(launch)) %>%\n  mutate(launchsucc = as.factor(ifelse(launch == 1, \"success\", \"failure\")))\n\n\n\nQ4. Chp 7, A3\nUse the data in Table 7.1 to create a scatterplot with a least squares regression line for the space shuttle data. Calculate the predicted response values (\\(\\hat{y} = b_0 + b_1 x\\)) when the temperature is 60F and when the temperature is 85F.\n\n\nQ5. Chp 7, A4\nSolve Equation (7.5) for \\(\\pi_i\\) to show that Equation (7.6) is true. Note that your text uses \\(\\pi_i\\) to represent the true model (akin to \\(p_i\\) that has been used in class). The difference is only in notation, not in meaning.\n\n\nQ6. Chp 7, A5\nUse Equation (7.6) to create twelve graphs: In each graph plot the explanatory variable (x) versus the expected probability of success (\\(p_i\\)) using the following values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n\n\n\n\n\\(\\beta_0\\)\n-10\n-10\n-10\n-5\n-5\n-5\n10\n10\n10\n5\n5\n5\n\n\n\\(\\beta_1\\)\n0.5\n1\n1.5\n0.5\n1\n1.5\n-0.5\n-1\n-1.5\n-0.5\n-1\n-1.5\n\n\n\n\nDo not submit the graphs, but explain the impact of changing \\(\\beta_0\\) and \\(\\beta_1\\).\nFor all of the graphs, at what value of \\(\\pi\\) does there appear to be the steepest slope?\n\nI wrote the R code for you (hopefully you can follow along with what it is doing). All you need to do for this problem is change the parameter values and look at the graph. Do not include all the graphs in your assignment, just answer the questions based on your observations.\n\n#set the parameters\nbeta0 <- -10\nbeta1 <- 0.5\nvaluesofX <- seq(0, 40, by=0.01)  # create a vector of X values\n\nprobfunc <- function(b0, b1, ex){\n  exp(b0 + b1*ex) / (1 + exp(b0 + b1*ex))\n}\n\nvaluesofY <- probfunc(beta0, beta1, valuesofX)\n\ndatatoplot <- data.frame(explan = valuesofX, prob = valuesofY)\n\nggplot(datatoplot) + \n  geom_line(aes(x = explan, y = prob))\n\n\n\n\n\n\n\n\n\n\nQ7. Chp 7, A6\n[For the shuttle data:] Use statistical software to calculate the maximum likelihood estimates of \\(\\beta_0\\) and \\(\\beta_1\\). Compare the maximum likelihood estimates to the least squares estimates in A3. Use glm(response ~ explanatory, family = \"binomial\", data = yourdataset) %>% tidy().\n\n\nQ8. Chp 7, A7\nUse Equation (7.9) to predict the probability that a launch has no O-ring damage when the temperature is 31F, 50F, and 75F.\n\n\nQ9. Chp 7, A8\nCalculate the odds of a launch with no O-ring damage when the temperature is 60F and when the temperature is 70F.\n\n\nQ10. Chp 7, A9\nFor the shuttle model above, when \\(x_i\\) increases by 10, state in terms of \\(e^{b_1}\\) how much you would expect the odds to change. (Here you are calculating the odds ratio for an increase in 10 degrees.)\n\n\nQ11. Chp 7, A10\nThe difference between the odds of success at 60F and 59F is about 0.3285 - 0.2605 = 0.068. Would you expect the difference between the odds at 52F and 51F to also be about 0.068? Explain why or why not.\n\n\nQ12. Chp 7, A11\nCreate a plot of two prediction models (one logistic, one linear). Plot temperature versus the estimated probability using maximum likelihood estimates from A6, and plot temperature versus the estimated probability using the least squares estimates from A3.\n\nR code:\nStep1. Look up at probfunc() above. Write a very similar function that is linear instead. Give it a different name.\nStep2. Using the two sets of coefficients (one from the linear and one from the logistic), predict the “y” value for both models for a vector of possible explanatory variables (e.g., valuesofX <- seq(50,85,by=0.01)). You should have two different vectors of predictions (and the vector of X, the explanatory variable).\nStep3. Create a data.frame() with three columns. Let’s say you call it mypredictons. The ggplot code will look like this. Have fun with coloring the plot or changing the line types or something!\nggplot(shuttle) +\n   geom_point(aes(x = temp, y = launch)) + \n   geom_line(data = mypredictions, aes(x = valuesofX, y = yourlinearpreds)) +\n   geom_line(data = mypredictions, aes(x = valuesofX, y = yourlogisticpreds))\n\npraise()\n\n[1] \"You are unreal!\"\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/"
  }
]