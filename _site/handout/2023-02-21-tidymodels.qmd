---
title: "Tidymodels"
subtitle: "Math 150 - Spring 2023"
author: "Jo Hardin (from Mine Ã‡etinkaya-Rundel)"
footer: "[https://m150-method-biostat.netlify.app/](https://m150-method-biostat.netlify.app/)"
logo: "../images/st47s.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: true
    transition: fade
    slide-number: true
    incremental: true 
    chalkboard: true
execute:
  freeze: auto
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

## Agenda

::: nonincremental
-   Workflow to help us think about model building
-   Breaking up data for independent assessment
-   Assessment metrics
:::

```{r}
# load packages
library(tidyverse)
library(tidymodels)
library(gghighlight)
library(knitr)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

# Comparing Models

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("figs/blackbox.jpeg")
```

<br>

1.  Data Modeling: The analysis in this culture starts with assuming a stochastic data model for the inside of the black box...The values of the parameters are estimated from the data and the model then used for information and/or prediction

2.  Algorithmic Modeling: The analysis in this culture considers the inside of the box complex and unknown. \[The\] approach is to find a function f(x) --- an algorithm that operates on x to predict the responses y.

Reference: Leo Breiman (2001)

# Data & goal

-   Data: <a href = "https://feederwatch.org/explore/raw-dataset-requests/" target = "_blank">Project FeederWatch</a>#TidyTuesday dataset on Project FeederWatch, a citizen science project for bird science, by way of <a href = "https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-01-10" target = "_blank">TidyTuesday</a>.

-   Can the characteristics of the bird feeder site like the surrounding yard and habitat predict whether a bird feeder site will be used by squirrels?

```{r}
site_data <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv') %>%
  mutate(squirrels = as.factor(ifelse(squirrels, "squirrels", "no squirrels")))

site_data <- site_data %>%
  filter(!is.na(squirrels)) %>%
  select(where(~!all(is.na(.x)))) %>%
  select(-loc_id, -proj_period_id, -fed_yr_round) %>%
  select(squirrels, everything()) 

glimpse(site_data)
```

## Feeders

How are other characteristics related to the presence of squirrels?

```{r}
site_data %>%
  filter(!is.na(squirrels)) %>%
  group_by(squirrels) %>%
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))
```

## Other variables

What about some of the variables describing the habitat?

```{r}
site_data %>%
  filter(!is.na(squirrels)) %>%
  group_by(squirrels) %>%
  summarise(across(contains("hab"), mean, na.rm = TRUE)) %>%
  pivot_longer(contains("hab")) %>%
  mutate(name = str_remove(name, "hab_")) %>%
  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "% of locations", y = NULL, fill = NULL)
```

# Modeling

## Train / test

Create an initial split:

```{r}
#| echo: true

set.seed(470)
feeder_split <- site_data %>%
  initial_split(strata = squirrels) # prop = 3/4 in each group, by default
```

Save training data

```{r}
#| echo: true

feeder_train <- training(feeder_split)
dim(feeder_train)
```

Save testing data

```{r}
#| echo: true

feeder_test  <- testing(feeder_split)
dim(feeder_test)
```

## Training data

```{r}
#| echo: true

feeder_train
```

## Feature engineering

-   We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity

-   Variables that go into the model and how they are represented are just as critical to success of the model

-   **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)

## Modeling workflow

-   Create a **recipe** for feature engineering steps to be applied to the training data

-   Fit the model to the training data after these steps have been applied

-   Using the model estimates from the training data, predict outcomes for the test data

-   Evaluate the performance of the model on the test data

# Building recipes

## Initiate a recipe

```{r}
#| echo: true
#| code-line-numbers: "|2|3"

feeder_rec <- recipe(
  squirrels ~ .,    # formula
  data = feeder_train 
  )

feeder_rec
```

## Working with recipes {.smaller}

-   When building recipes you in a pipeline, you don't get to see the effect of the recipe on your data, which can be unsettling
-   You can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model
-   This requires two functions: `prep()` to train the recipe and `bake()` to apply it to your data

. . .

::: callout-note
Using `prep()` and `bake()` are shown here for demonstrative purposes. They do not need to be a part of your pipeline. I do find them assuring, however, so that I can see the effects of the recipe steps as the recipe is built.
:::

## Impute missing values & Remove zero variance predictors {.smaller}

Impute missing values (replace with mean).  
Remove all predictors that contain only a single value

```{r}
#| echo: true
#| code-line-numbers: "|2|3"

feeder_rec <- feeder_rec %>%
  step_impute_mean(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors())

feeder_rec
```

## Prep and bake {.smaller}

```{r}
#| echo: true

feeder_rec_trained <- prep(feeder_rec)

bake(feeder_rec_trained, feeder_train) %>%
  glimpse()
```

# Building workflows

## Specify model

```{r}
#| echo: true

feeder_spec <- logistic_reg() %>%
  set_engine("glm")

feeder_spec
```

## Build workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.

```{r}
#| echo: true

feeder_wflow <- workflow() %>%
  add_model(feeder_spec) %>%
  add_recipe(feeder_rec)
```

<br>

*See next slide for workflow...*

## View workflow

```{r}
#| echo: true

feeder_wflow
```

## Fit model to training data {.smaller}

```{r}
#| echo: true

feeder_fit <- feeder_wflow %>%
  fit(data = feeder_train)

feeder_fit %>% tidy()
```

<br>

. . .

*So many predictors!*

## Model fit summary

```{r}
#| echo: true

feeder_fit %>% tidy() 
```

# Evaluate model

> sensitivty = number of squirrels that are accurately predicted to be squirrels (true positive rate)

> specificity = number of non-squirrels that are accurately predicted to be non-squirrels (true negative rate)

## Make predictions for training data

```{r}
#| echo: true
feeder_train_pred <- predict(feeder_fit, feeder_train, type = "prob") %>%
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) %>%
  bind_cols(feeder_train %>% select(squirrels))

feeder_train_pred
```

## Accuracy

```{r}
#| echo: true

rbind(accuracy(feeder_train_pred, truth = squirrels, estimate = .pred_class),
      sensitivity(feeder_train_pred, truth = squirrels, estimate = .pred_class),
      specificity(feeder_train_pred, truth = squirrels, estimate = .pred_class))
```

## Visualizing accuracy

```{r}
#| echo: true

feeder_train_pred %>%
  select(squirrels, .pred_class) %>%
  yardstick::conf_mat(squirrels, .pred_class) %>%
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

. . .

## But, really...

*who cares about predictions on **training** data?*

## Make predictions for testing data

```{r}
#| echo: true

feeder_test_pred <- predict(feeder_fit, feeder_test, type = "prob") %>%
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) %>%
  bind_cols(feeder_test %>% select(squirrels))

feeder_test_pred
```

## Evaluate testing data

```{r}
#| echo: true

rbind(accuracy(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      sensitivity(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      specificity(feeder_test_pred, truth = squirrels, estimate = .pred_class))
```

## Evaluate testing data

```{r}
#| echo: true

feeder_test_pred %>%
  select(squirrels, .pred_class) %>%
  yardstick::conf_mat(squirrels, .pred_class) %>%
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

## Training vs. testing

```{r}
rbind(accuracy(feeder_train_pred, truth = squirrels, estimate = .pred_class),
      sensitivity(feeder_train_pred, truth = squirrels, estimate = .pred_class),
      specificity(feeder_train_pred, truth = squirrels, estimate = .pred_class))

rbind(accuracy(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      sensitivity(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      specificity(feeder_test_pred, truth = squirrels, estimate = .pred_class))

```

## What's up with training data? {.smaller}

-   The training set does not have the capacity to be a good arbiter of performance.

-   It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

-   Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.
