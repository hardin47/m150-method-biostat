{
  "hash": "bf6df6bb2487e4dcc3ce9e65c15774dc",
  "result": {
    "markdown": "---\ntitle: \"Tidymodels\"\nsubtitle: \"Math 150 - Spring 2023\"\nauthor: \"Jo Hardin (from Mine Çetinkaya-Rundel)\"\nfooter: \"[https://m150-method-biostat.netlify.app/](https://m150-method-biostat.netlify.app/)\"\nlogo: \"../images/st47s.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: true\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\nexecute:\n  freeze: auto\nbibliography: book.bib\n---\n\n\n\n\n## Agenda\n\n::: nonincremental\n-   Workflow to help us think about model building\n-   Breaking up data for independent assessment\n-   Assessment metrics\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n# Comparing Models\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](figs/blackbox.jpeg){fig-align='center' width=90%}\n:::\n:::\n\n\n\n1.  Data Modeling: The analysis in this culture starts with assuming a stochastic data model for the inside of the black box...The values of the parameters are estimated from the data and the model then used for information and/or prediction\n\n2.  Algorithmic Modeling: The analysis in this culture considers the inside of the box complex and unknown. The approach is to find a function f(x) --- an algorithm that operates on x to predict the responses y.\n\nReference: Leo Breiman (2001)\n\n# Data & goal\n\n-   Data: <a href = \"https://feederwatch.org/explore/raw-dataset-requests/\" target = \"_blank\">Project FeederWatch</a>#TidyTuesday dataset on Project FeederWatch, a citizen science project for bird science, by way of <a href = \"https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-01-10\" target = \"_blank\">TidyTuesday</a>.\n\n-   Can the characteristics of the bird feeder site like the surrounding yard and habitat predict whether a bird feeder site will be used by squirrels?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 235,685\nColumns: 59\n$ squirrels                    <fct> no squirrels, no squirrels, no squirrels,…\n$ yard_type_pavement           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_garden             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_landsca            <dbl> 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ yard_type_woods              <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yard_type_desert             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hab_dcid_woods               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, NA, NA,…\n$ hab_evgr_woods               <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, NA,…\n$ hab_mixed_woods              <dbl> 1, 1, 1, 1, 1, 1, NA, NA, NA, NA, 1, 1, 1…\n$ hab_orchard                  <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, NA,…\n$ hab_park                     <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, 1, …\n$ hab_water_fresh              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hab_water_salt               <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, 1, …\n$ hab_residential              <dbl> 1, 1, 1, 1, 1, 1, NA, NA, NA, NA, 1, 1, 1…\n$ hab_industrial               <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, 1, …\n$ hab_agricultural             <dbl> 1, 1, 1, 1, 1, 1, NA, NA, NA, NA, 1, 1, 1…\n$ hab_desert_scrub             <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, NA,…\n$ hab_young_woods              <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, NA,…\n$ hab_swamp                    <dbl> NA, NA, NA, NA, 0, 0, NA, NA, NA, NA, NA,…\n$ hab_marsh                    <dbl> 1, 1, 1, 1, 1, 1, NA, NA, NA, NA, 1, 1, 1…\n$ evgr_trees_atleast           <dbl> 11, 11, 11, 11, 11, 11, 0, 0, 0, 4, 1, 1,…\n$ evgr_shrbs_atleast           <dbl> 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4,…\n$ dcid_trees_atleast           <dbl> 11, 11, 11, 11, 1, 1, 11, 11, 11, 11, 4, …\n$ dcid_shrbs_atleast           <dbl> 4, 4, 4, 4, 4, 4, 11, 11, 11, 11, 4, 4, 4…\n$ fru_trees_atleast            <dbl> 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ cacti_atleast                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ brsh_piles_atleast           <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ water_srcs_atleast           <dbl> 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ bird_baths_atleast           <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,…\n$ nearby_feeders               <dbl> 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,…\n$ cats                         <dbl> 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,…\n$ dogs                         <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,…\n$ humans                       <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,…\n$ housing_density              <dbl> 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2,…\n$ fed_in_jan                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, N…\n$ fed_in_feb                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, N…\n$ fed_in_mar                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, N…\n$ fed_in_apr                   <dbl> 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, NA, 0, N…\n$ fed_in_may                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 0, N…\n$ fed_in_jun                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 0, N…\n$ fed_in_jul                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 0, N…\n$ fed_in_aug                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 0, N…\n$ fed_in_sep                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 0, N…\n$ fed_in_oct                   <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, NA, 1, N…\n$ fed_in_nov                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, N…\n$ fed_in_dec                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, NA, 1, N…\n$ numfeeders_suet              <dbl> 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 2,…\n$ numfeeders_ground            <dbl> NA, 0, 0, 0, NA, NA, 1, 1, 1, 1, 3, 3, 3,…\n$ numfeeders_hanging           <dbl> 1, 1, 1, 3, NA, NA, 2, 2, 2, 2, 2, 2, 1, …\n$ numfeeders_platfrm           <dbl> 1, 1, 1, 0, NA, NA, 1, 1, 1, 2, 1, 1, 1, …\n$ numfeeders_humming           <dbl> NA, 0, 0, 0, NA, NA, 1, 1, 1, 1, NA, 0, 0…\n$ numfeeders_water             <dbl> 1, 1, 1, 1, NA, NA, 2, 2, 2, 2, 1, 1, 1, …\n$ numfeeders_thistle           <dbl> NA, 0, 0, 0, NA, NA, 1, 1, 1, 2, 1, 1, 1,…\n$ numfeeders_fruit             <dbl> NA, 0, 0, 0, NA, NA, 1, 1, 1, 1, NA, 0, 0…\n$ numfeeders_hopper            <dbl> NA, NA, NA, NA, 1, 1, NA, NA, NA, NA, NA,…\n$ numfeeders_tube              <dbl> NA, NA, NA, NA, 1, 1, NA, NA, NA, NA, NA,…\n$ numfeeders_other             <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ population_atleast           <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5001, 5001,…\n$ count_area_size_sq_m_atleast <dbl> 1.01, 1.01, 1.01, 1.01, 1.01, 1.01, 375.0…\n```\n:::\n:::\n\n\n## Feeders\n\nHow are other characteristics related to the presence of squirrels?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  squirrels    nearby_feeders\n  <fct>                 <dbl>\n1 no squirrels          0.344\n2 squirrels             0.456\n```\n:::\n:::\n\n\n## Other variables\n\nWhat about some of the variables describing the habitat?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](2023-02-21-tidymodels_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n# Modeling\n\n## Train / test\n\nCreate an initial split:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(470)\nfeeder_split <- site_data %>%\n  initial_split(strata = squirrels) # prop = 3/4 in each group, by default\n```\n:::\n\n\nSave training data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_train <- training(feeder_split)\ndim(feeder_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 176763     59\n```\n:::\n:::\n\n\nSave testing data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_test  <- testing(feeder_split)\ndim(feeder_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 58922    59\n```\n:::\n:::\n\n\n## Training data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_train\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 176,763 × 59\n   squirrels    yard_t…¹ yard_…² yard_…³ yard_…⁴ yard_…⁵ hab_d…⁶ hab_e…⁷ hab_m…⁸\n   <fct>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 no squirrels        0       0       1       0       0       1      NA       1\n 2 no squirrels        0       0       1       0       0       1      NA       1\n 3 no squirrels        0       0       1       0       0       1       0       1\n 4 no squirrels        0       0       1       0       0       1      NA      NA\n 5 no squirrels        0       0       1       1       0       1       0       0\n 6 no squirrels        0       0       1       0       0       1      NA       1\n 7 no squirrels        0       0       0       1       0       0       0       1\n 8 no squirrels        0       0       1       1       0      NA      NA      NA\n 9 no squirrels        0       0       1       0       0       1       1      NA\n10 no squirrels        0       0       1       1       0       0       0       1\n# … with 176,753 more rows, 50 more variables: hab_orchard <dbl>,\n#   hab_park <dbl>, hab_water_fresh <dbl>, hab_water_salt <dbl>,\n#   hab_residential <dbl>, hab_industrial <dbl>, hab_agricultural <dbl>,\n#   hab_desert_scrub <dbl>, hab_young_woods <dbl>, hab_swamp <dbl>,\n#   hab_marsh <dbl>, evgr_trees_atleast <dbl>, evgr_shrbs_atleast <dbl>,\n#   dcid_trees_atleast <dbl>, dcid_shrbs_atleast <dbl>,\n#   fru_trees_atleast <dbl>, cacti_atleast <dbl>, brsh_piles_atleast <dbl>, …\n```\n:::\n:::\n\n\n## Feature engineering\n\n-   We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\n\n-   Variables that go into the model and how they are represented are just as critical to success of the model\n\n-   **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)\n\n## Modeling workflow\n\n-   Create a **recipe** for feature engineering steps to be applied to the training data\n\n-   Fit the model to the training data after these steps have been applied\n\n-   Using the model estimates from the training data, predict outcomes for the test data\n\n-   Evaluate the performance of the model on the test data\n\n# Building recipes\n\n## Initiate a recipe\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|2|3\"}\nfeeder_rec <- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  )\n\nfeeder_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n```\n:::\n:::\n\n\n## Working with recipes {.smaller}\n\n-   When building recipes you in a pipeline, you don't get to see the effect of the recipe on your data, which can be unsettling\n-   You can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\n-   This requires two functions: `prep()` to train the recipe and `bake()` to apply it to your data\n\n. . .\n\n::: callout-note\nUsing `prep()` and `bake()` are shown here for demonstrative purposes. They do not need to be a part of your pipeline. I do find them assuring, however, so that I can see the effects of the recipe steps as the recipe is built.\n:::\n\n## Impute missing values & Remove zero variance predictors {.smaller}\n\nImpute missing values (replace with mean).  \nRemove all predictors that contain only a single value\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"|2|3\"}\nfeeder_rec <- feeder_rec %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n\nOperations:\n\nMean imputation for all_numeric_predictors()\nSparse, unbalanced variable filter on all_numeric_predictors()\n```\n:::\n:::\n\n\n## Prep and bake {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec_trained <- prep(feeder_rec)\n\nbake(feeder_rec_trained, feeder_train) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 176,763\nColumns: 59\n$ yard_type_pavement           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_garden             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ yard_type_landsca            <dbl> 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ yard_type_woods              <dbl> 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,…\n$ yard_type_desert             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ hab_dcid_woods               <dbl> 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_evgr_woods               <dbl> 0.223341, 0.223341, 0.000000, 0.223341, 0…\n$ hab_mixed_woods              <dbl> 1.000000, 1.000000, 1.000000, 0.655411, 0…\n$ hab_orchard                  <dbl> 0.09324933, 0.09324933, 0.00000000, 0.093…\n$ hab_park                     <dbl> 0.4502185, 0.4502185, 0.0000000, 0.450218…\n$ hab_water_fresh              <dbl> 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_water_salt               <dbl> 0.05016023, 0.05016023, 0.00000000, 0.050…\n$ hab_residential              <dbl> 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ hab_industrial               <dbl> 0.2215792, 0.2215792, 0.0000000, 0.221579…\n$ hab_agricultural             <dbl> 1.0000000, 1.0000000, 1.0000000, 0.409835…\n$ hab_desert_scrub             <dbl> 0.09257371, 0.09257371, 0.00000000, 0.092…\n$ hab_young_woods              <dbl> 0.349782, 0.349782, 0.000000, 0.349782, 0…\n$ hab_swamp                    <dbl> 0.2917369, 0.2917369, 0.0000000, 0.291736…\n$ hab_marsh                    <dbl> 1.0000000, 1.0000000, 1.0000000, 0.175361…\n$ evgr_trees_atleast           <dbl> 11, 11, 11, 0, 11, 1, 4, 4, 4, 1, 1, 1, 1…\n$ evgr_shrbs_atleast           <dbl> 4, 4, 1, 4, 0, 1, 4, 4, 4, 1, 0, 4, 4, 11…\n$ dcid_trees_atleast           <dbl> 11, 11, 1, 4, 1, 4, 11, 11, 4, 1, 1, 1, 1…\n$ dcid_shrbs_atleast           <dbl> 4, 4, 4, 1, 1, 4, 11, 11, 1, 1, 0, 4, 4, …\n$ fru_trees_atleast            <dbl> 4.00000, 4.00000, 1.00000, 1.00000, 0.000…\n$ cacti_atleast                <dbl> 0.0000000, 0.0000000, 0.0000000, 0.000000…\n$ brsh_piles_atleast           <dbl> 0, 0, 0, 1, 0, 0, 1, 4, 1, 0, 0, 1, 1, 1,…\n$ water_srcs_atleast           <dbl> 1.0000000, 1.0000000, 1.0000000, 0.000000…\n$ bird_baths_atleast           <dbl> 0, 0, 0, 1, 0, 0, 1, 1, 4, 0, 0, 0, 0, 1,…\n$ nearby_feeders               <dbl> 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,…\n$ cats                         <dbl> 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,…\n$ dogs                         <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,…\n$ humans                       <dbl> 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,…\n$ housing_density              <dbl> 2, 2, 2, 2, 2, 3, 1, 1, 1, 1, 2, 1, 2, 2,…\n$ fed_in_jan                   <dbl> 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ fed_in_feb                   <dbl> 1.000000, 1.000000, 1.000000, 1.000000, 1…\n$ fed_in_mar                   <dbl> 1.0000000, 1.0000000, 1.0000000, 1.000000…\n$ fed_in_apr                   <dbl> 1.0000000, 1.0000000, 0.0000000, 1.000000…\n$ fed_in_may                   <dbl> 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_jun                   <dbl> 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_jul                   <dbl> 0.000000, 0.000000, 0.000000, 1.000000, 1…\n$ fed_in_aug                   <dbl> 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_sep                   <dbl> 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_oct                   <dbl> 0.0000000, 0.0000000, 0.0000000, 1.000000…\n$ fed_in_nov                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ fed_in_dec                   <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ numfeeders_suet              <dbl> 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 2, 3, 4, 1,…\n$ numfeeders_ground            <dbl> 0.000000, 0.000000, 1.270686, 1.000000, 0…\n$ numfeeders_hanging           <dbl> 1.000000, 1.000000, 2.707598, 2.000000, 2…\n$ numfeeders_platfrm           <dbl> 1.000000, 1.000000, 1.033629, 1.033629, 0…\n$ numfeeders_humming           <dbl> 0.0000000, 0.0000000, 0.4989854, 0.498985…\n$ numfeeders_water             <dbl> 1.0000000, 1.0000000, 0.7887147, 0.788714…\n$ numfeeders_thistle           <dbl> 0.000000, 0.000000, 1.007624, 1.000000, 1…\n$ numfeeders_fruit             <dbl> 0.0000000, 0.0000000, 0.1442454, 0.144245…\n$ numfeeders_hopper            <dbl> 1.385205, 1.385205, 1.000000, 1.385205, 0…\n$ numfeeders_tube              <dbl> 2.162308, 2.162308, 1.000000, 2.162308, 2…\n$ numfeeders_other             <dbl> 0.6037683, 0.6037683, 0.6037683, 0.603768…\n$ population_atleast           <dbl> 1, 1, 1, 25001, 5001, 25001, 1, 1, 1, 1, …\n$ count_area_size_sq_m_atleast <dbl> 1.01, 1.01, 1.01, 1.01, 1.01, 1.01, 375.0…\n$ squirrels                    <fct> no squirrels, no squirrels, no squirrels,…\n```\n:::\n:::\n\n\n# Building workflows\n\n## Specify model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_log <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nfeeder_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## Build workflow\n\n**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow <- workflow() %>%\n  add_recipe(feeder_rec) %>%\n  add_model(feeder_log) \n```\n:::\n\n\n<br>\n\n*See next slide for workflow...*\n\n## View workflow\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## Fit model to training data {.smaller}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit <- feeder_wflow %>%\n  fit(data = feeder_train)\n\nfeeder_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# … with 49 more rows\n```\n:::\n:::\n\n\n<br>\n\n. . .\n\n*So many predictors!*\n\n## Model fit summary\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit %>% tidy() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# … with 49 more rows\n```\n:::\n:::\n\n\n## All together\n\n::: {.panel-tabset}\n\n## recipe\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec <- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n\nOperations:\n\nMean imputation for all_numeric_predictors()\nSparse, unbalanced variable filter on all_numeric_predictors()\n```\n:::\n:::\n\n\n## model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_log <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nfeeder_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n## workflow\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow <- workflow() %>%\n  add_recipe(feeder_rec) %>%\n  add_model(feeder_log) \nfeeder_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit <- feeder_wflow %>%\n  fit(data = feeder_train)\n\nfeeder_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# … with 49 more rows\n```\n:::\n:::\n\n## predict\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_train_pred <- predict(feeder_fit, feeder_train, type = \"prob\") %>%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %>%\n  bind_cols(feeder_train %>% select(squirrels))\n\nfeeder_train_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  <dbl>           <dbl> <fct>       <fct>       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# … with 176,753 more rows\n```\n:::\n:::\n\n\n\n:::\n\n\n\n# Evaluate model\n\n> sensitivty = number of squirrels that are accurately predicted to be squirrels (true positive rate)\n\n> specificity = number of non-squirrels that are accurately predicted to be non-squirrels (true negative rate)\n\n## Make predictions for training data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_train_pred <- predict(feeder_fit, feeder_train, type = \"prob\") %>%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %>%\n  bind_cols(feeder_train %>% select(squirrels))\n\nfeeder_train_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  <dbl>           <dbl> <fct>       <fct>       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# … with 176,753 more rows\n```\n:::\n:::\n\n\n## Accuracy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbind(accuracy(feeder_train_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_train_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101\n```\n:::\n:::\n\n\n## Visualizing accuracy\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_train_pred %>%\n  select(squirrels, .pred_class) %>%\n  yardstick::conf_mat(squirrels, .pred_class) %>%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n```\n\n::: {.cell-output-display}\n![](2023-02-21-tidymodels_files/figure-revealjs/unnamed-chunk-26-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n. . .\n\n## But, really...\n\n*who cares about predictions on **training** data?*\n\n## Make predictions for testing data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_test_pred <- predict(feeder_fit, feeder_test, type = \"prob\") %>%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %>%\n  bind_cols(feeder_test %>% select(squirrels))\n\nfeeder_test_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 58,922 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  <dbl>           <dbl> <fct>       <fct>       \n 1               0.201            0.799 squirrels   no squirrels\n 2               0.156            0.844 squirrels   no squirrels\n 3               0.352            0.648 squirrels   no squirrels\n 4               0.197            0.803 squirrels   squirrels   \n 5               0.121            0.879 squirrels   squirrels   \n 6               0.272            0.728 squirrels   squirrels   \n 7               0.0459           0.954 squirrels   squirrels   \n 8               0.0341           0.966 squirrels   squirrels   \n 9               0.0631           0.937 squirrels   squirrels   \n10               0.0312           0.969 squirrels   squirrels   \n# … with 58,912 more rows\n```\n:::\n:::\n\n\n## Evaluate testing data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbind(accuracy(feeder_test_pred, truth = squirrels, \n               estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"),\n      specificity(feeder_test_pred, truth = squirrels, \n                  estimate = .pred_class, event_level = \"second\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n```\n:::\n:::\n\n\n## Evaluate testing data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_test_pred %>%\n  select(squirrels, .pred_class) %>%\n  yardstick::conf_mat(squirrels, .pred_class) %>%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n```\n\n::: {.cell-output-display}\n![](2023-02-21-tidymodels_files/figure-revealjs/unnamed-chunk-29-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Training vs. testing\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.815\n2 sensitivity binary         0.985\n3 specificity binary         0.101\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.985 \n3 specificity binary        0.0988\n```\n:::\n:::\n\n\n## What's up with training data? {.smaller}\n\n-   The training set does not have the capacity to be a good arbiter of performance.\n\n-   It is not an independent piece of information; predicting the training set can only reflect what the model already knows.\n\n-   Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.\n\n\n## All together\n\n::: {.panel-tabset}\n\n## recipe\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec <- recipe(\n  squirrels ~ .,    # formula\n  data = feeder_train \n  ) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n\nOperations:\n\nMean imputation for all_numeric_predictors()\nSparse, unbalanced variable filter on all_numeric_predictors()\n```\n:::\n:::\n\n\n## model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_log <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nfeeder_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n## workflow\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow <- workflow() %>%\n  add_recipe(feeder_rec) %>%\n  add_model(feeder_log) \n\nfeeder_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit <- feeder_wflow %>%\n  fit(data = feeder_train)\n\nfeeder_fit %>% tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 59 × 5\n   term               estimate std.error statistic   p.value\n   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n 1 (Intercept)         -1.42      0.0505    -28.2  2.26e-174\n 2 yard_type_pavement  -0.854     0.149      -5.74 9.23e-  9\n 3 yard_type_garden    -0.175     0.0392     -4.47 7.89e-  6\n 4 yard_type_landsca    0.168     0.0219      7.69 1.45e- 14\n 5 yard_type_woods      0.309     0.0170     18.1  1.59e- 73\n 6 yard_type_desert    -0.297     0.0789     -3.76 1.68e-  4\n 7 hab_dcid_woods       0.336     0.0161     20.9  6.55e- 97\n 8 hab_evgr_woods      -0.0797    0.0192     -4.15 3.36e-  5\n 9 hab_mixed_woods      0.420     0.0158     26.5  3.17e-155\n10 hab_orchard         -0.307     0.0252    -12.2  4.00e- 34\n# … with 49 more rows\n```\n:::\n:::\n\n## predict\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_test_pred <- predict(feeder_fit, feeder_test, type = \"prob\") %>%\n  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,\n                                        \"squirrels\", \"no squirrels\"))) %>%\n  bind_cols(feeder_test %>% select(squirrels))\n\nfeeder_train_pred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 176,763 × 4\n   `.pred_no squirrels` .pred_squirrels .pred_class squirrels   \n                  <dbl>           <dbl> <fct>       <fct>       \n 1                0.168           0.832 squirrels   no squirrels\n 2                0.168           0.832 squirrels   no squirrels\n 3                0.358           0.642 squirrels   no squirrels\n 4                0.166           0.834 squirrels   no squirrels\n 5                0.274           0.726 squirrels   no squirrels\n 6                0.191           0.809 squirrels   no squirrels\n 7                0.288           0.712 squirrels   no squirrels\n 8                0.184           0.816 squirrels   no squirrels\n 9                0.227           0.773 squirrels   no squirrels\n10                0.350           0.650 squirrels   no squirrels\n# … with 176,753 more rows\n```\n:::\n:::\n\n\n## assess\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrbind(accuracy(feeder_test_pred, truth = squirrels, estimate = .pred_class),\n      sensitivity(feeder_test_pred, truth = squirrels, estimate = .pred_class),\n      specificity(feeder_test_pred, truth = squirrels, estimate = .pred_class))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary        0.814 \n2 sensitivity binary        0.0988\n3 specificity binary        0.985 \n```\n:::\n:::\n\n\n:::\n\n\n\n# Cross Validation\n\n::: nonincremental\n-   Cross validation for model comparison (which model is best?)\n-   Cross validation for model evaluation (what is the accuracy rate of the model?)\n:::\n\n## Competing models\n\n*  we use the test data to assess how the model does.  But we haven't yet thought about how to use the data to **build** a particular model.\n\n* compare two different models to predict whether or not there are squirrels\n    - Model 1: removes the information about the habitat and about the trees and shrubs\n    - Model 2: removes the information about feeding the birds\n    \n    \n## Compare recipes\n\n::: {.panel-tabset}\n\n## recipe 1\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec1 <- recipe(squirrels ~ ., data = feeder_train) %>%\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) %>%\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n```\n:::\n\n\n## recipe 2\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec2 <- recipe(squirrels ~ ., data = feeder_train) %>%\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) %>%\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n```\n:::\n\n\n:::\n\n\n## How can we decide?\n\n* ~~measure which model does better on the test data~~\n* ~~measure which model does better on the training data~~\n* measure which model does better on the cross validated data\n\n## How does cross validation work?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![4-fold CV is depicted.  Notice that the holdout group is never used as part of the coefficient estimation process.](figs/CV.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Cross validation\n\nMore specifically, **v-fold cross validation**:\n\n-   Randomly partition the **training** **data** into v group\n-   Use v-1 groups to build the model (calculate MLEs); use 1 group for prediction / assessment\n-   Repeat v times, updating which group is used for assessment each time\n\n## Cross validation, step 1\n\nConsider the example below where the **training** **data** are randomly split into 3 partitions:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Splitting the data into a partition of v=3 groups. Source: [@tidymodelingR]](figs/three-CV.svg){fig-align='center' fig-alt='Thirty observations are seen where three colors are used to demonstrate that the observations can be partitioned into three groups.' width=100%}\n:::\n:::\n\n\n## Cross validation, steps 2 and 3 \n\n-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis\n-   Repeat v times, updating which partition is used for assessment each time\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![With the data split into three groups, we can see how 2/3 of the observations are used to fit the model and 1/3 of the observations are used to estimate the performance of the model. Source: [@tidymodelingR]](figs/three-CV-iter.svg){fig-align='center' fig-alt='Three iterations of model fitting are shown, each time using only 2/3 of the observations.  The remaining 1/3 of the observations are used to estimate the performance of the model.' width=100%}\n:::\n:::\n\n\n\n## Cross validation using tidymodels\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(4747)\nfolds <- vfold_cv(feeder_train, v = 3, strata = squirrels)\nfolds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#  3-fold cross-validation using stratification \n# A tibble: 3 × 2\n  splits                 id   \n  <list>                 <chr>\n1 <split [117841/58922]> Fold1\n2 <split [117842/58921]> Fold2\n3 <split [117843/58920]> Fold3\n```\n:::\n:::\n\n\n\n## Fit the model separately to each fold\n\n\n::: {.panel-tabset}\n\n## recipe\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec1 <- recipe(squirrels ~ ., data = feeder_train) %>%\n  # delete the habitat variables\n  step_rm(contains(\"hab\")) %>%\n  # delete the tree/shrub info\n  step_rm(contains(\"atleast\")) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n\nOperations:\n\nVariables removed contains(\"hab\")\nVariables removed contains(\"atleast\")\nMean imputation for all_numeric_predictors()\nSparse, unbalanced variable filter on all_numeric_predictors()\n```\n:::\n:::\n\n\n## model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_log <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nfeeder_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n## workflow\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow1 <- workflow() %>%\n  add_recipe(feeder_rec1) %>%\n  add_model(feeder_log) \n\nfeeder_wflow1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmetrics_interest <- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs1 <- feeder_wflow1 %>%\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n```\n:::\n\n## predict\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit_rs1 %>% augment() %>%\n  select(squirrels, .pred_class) %>%\n  yardstick::conf_mat(squirrels, .pred_class) %>%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n```\n\n::: {.cell-output-display}\n![](2023-02-21-tidymodels_files/figure-revealjs/unnamed-chunk-47-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## assess\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(feeder_fit_rs1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1\n```\n:::\n:::\n\n:::\n\n## Repeat the CV analysis for the second model\n\n\n::: {.panel-tabset}\n\n## recipe\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_rec2 <- recipe(squirrels ~ ., data = feeder_train) %>%\n  # delete the variables on when the birds were fed\n  step_rm(contains(\"fed\")) %>%\n  # delete the variables about the bird feeders\n  step_rm(contains(\"feed\")) %>%\n  step_impute_mean(all_numeric_predictors()) %>%\n  step_nzv(all_numeric_predictors())\n\nfeeder_rec2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         58\n\nOperations:\n\nVariables removed contains(\"fed\")\nVariables removed contains(\"feed\")\nMean imputation for all_numeric_predictors()\nSparse, unbalanced variable filter on all_numeric_predictors()\n```\n:::\n:::\n\n\n## model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_log <- logistic_reg() %>%\n  set_engine(\"glm\")\n\nfeeder_log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n## workflow\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_wflow2 <- workflow() %>%\n  add_recipe(feeder_rec2) %>%\n  add_model(feeder_log) \n\nfeeder_wflow2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n4 Recipe Steps\n\n• step_rm()\n• step_rm()\n• step_impute_mean()\n• step_nzv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n```\n:::\n:::\n\n\n## fit\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmetrics_interest <- metric_set(accuracy, roc_auc, \n                              sensitivity, specificity)\n\nfeeder_fit_rs2 <- feeder_wflow2 %>%\n  fit_resamples(resamples = folds,\n                metrics = metrics_interest,\n                control = control_resamples(save_pred = TRUE,\n                                            event_level = \"second\"))\n```\n:::\n\n## predict\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfeeder_fit_rs2 %>% augment() %>%\n  select(squirrels, .pred_class) %>%\n  yardstick::conf_mat(squirrels, .pred_class) %>%\n  autoplot(type = \"heatmap\") + \n  scale_fill_gradient(low=\"#D6EAF8\", high=\"#2E86C1\") \n```\n\n::: {.cell-output-display}\n![](2023-02-21-tidymodels_files/figure-revealjs/unnamed-chunk-53-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## assess\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(feeder_fit_rs2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1\n```\n:::\n:::\n\n:::\n\n## Compare two models\n\n#### Model 1\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(feeder_fit_rs1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n1 accuracy    binary     0.809      3 0.000247 Preprocessor1_Model1\n2 roc_auc     binary     0.663      3 0.00180  Preprocessor1_Model1\n3 sensitivity binary     0.996      3 0.000190 Preprocessor1_Model1\n4 specificity binary     0.0265     3 0.000545 Preprocessor1_Model1\n```\n:::\n:::\n\n\n#### Model 2\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(feeder_fit_rs2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 6\n  .metric     .estimator   mean     n  std_err .config             \n  <chr>       <chr>       <dbl> <int>    <dbl> <chr>               \n1 accuracy    binary     0.813      3 0.000325 Preprocessor1_Model1\n2 roc_auc     binary     0.698      3 0.00228  Preprocessor1_Model1\n3 sensitivity binary     0.990      3 0.000424 Preprocessor1_Model1\n4 specificity binary     0.0693     3 0.000291 Preprocessor1_Model1\n```\n:::\n:::\n\n\n## Cross validation then test data\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Nested cross-validation: two cross-validation loops are run one inside the other.  [@CVpaper]](figs/CV.jpg){fig-align='center' width=100%}\n:::\n:::\n\n\n## Bias-variance trade-off\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  [@ESL]](figs/varbias.png){fig-align='center' width=100%}\n:::\n:::\n\n\n---\n\n",
    "supporting": [
      "2023-02-21-tidymodels_files/figure-revealjs"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}