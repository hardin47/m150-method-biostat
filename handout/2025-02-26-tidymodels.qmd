---
title: "Tidymodels"
subtitle: "Math 150 - Spring 2025"
author: "Jo Hardin"
format:
  revealjs:
    incremental: false
    scrollable: true
    slide-number: true
    show-slide-number: all
    df-print: default
execute:
  echo: true
  warning: false
  message: false
bibliography: book.bib
embed-resources: true
---


```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

## Agenda

::: nonincremental
-   Workflow to help us think about model building
-   Breaking up data for independent assessment
-   Assessment metrics
:::

```{r}
#| echo: false

# load packages
library(tidyverse)
library(tidymodels)
library(gghighlight)
library(knitr)
options(pillar.width = 70)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))
```

## Comparing Models

<div>
  <center><img src="figs/blackbox.png" width="70%"></center>
</div>


<div class = "fragment">
1.  Data Modeling: The analysis in this culture starts with assuming a stochastic data model for the inside of the black box...The values of the parameters are estimated from the data and the model then used for information and/or prediction

2.  Algorithmic Modeling: The analysis in this culture considers the inside of the box complex and unknown. The approach is to find a function f(x) --- an algorithm that operates on x to predict the responses y.

Reference: Leo Breiman (2001)
</div>

## Data & goal

-   Data: <a href = "https://feederwatch.org/explore/raw-dataset-requests/" target = "_blank">Project FeederWatch</a>#TidyTuesday dataset on Project FeederWatch, a citizen science project for bird science, by way of <a href = "https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-01-10" target = "_blank">TidyTuesday</a>.

-   Can the characteristics of the bird feeder site like the surrounding yard and habitat predict whether a bird feeder site will be used by squirrels?

::: {.panel-tabset}

## data
```{r}
#| echo: false
site_data <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv') |>
  mutate(squirrels = as.factor(ifelse(squirrels, "squirrels", "no squirrels"))) |> 
  filter(!is.na(squirrels))

site_data <- site_data |>
  select(where(~!all(is.na(.x)))) |>
  select(-loc_id, -proj_period_id, -fed_yr_round) |>
  select(squirrels, everything()) 

site_data
```

## code
```{r}
#| eval: false
site_data <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv') |>
  mutate(squirrels = as.factor(ifelse(squirrels, "squirrels", "no squirrels"))) |> 
  filter(!is.na(squirrels))

site_data <- site_data |>
  select(where(~!all(is.na(.x)))) |>
  select(-loc_id, -proj_period_id, -fed_yr_round) |>
  select(squirrels, everything()) 

site_data
```
:::

## How many squirrels?

```{r}
site_data |>
  group_by(squirrels) |>
  summarise(number_squirrels = n())
```


## Feeders

How are other characteristics related to the presence of squirrels?

```{r}
site_data |>
  group_by(squirrels) |>
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))
```

For those observations that saw squirrels, 45% had feeders nearby; for those observations that did not see squirrels, 34% had feeders nearby.

## Other variables

What about some of the variables describing the habitat?

::: {.panel-tabset}

## viz

```{r}
#| echo: false
site_data |>
  group_by(squirrels) |>
  summarise(across(contains("hab"), mean, na.rm = TRUE)) |>
  pivot_longer(contains("hab")) |>
  mutate(name = str_remove(name, "hab_")) |>
  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "% of locations", y = NULL, fill = NULL)
```

## code
```{r}
#| eval: false
site_data |>
  group_by(squirrels) |>
  summarise(across(contains("hab"), mean, na.rm = TRUE)) |>
  pivot_longer(contains("hab")) |>
  mutate(name = str_remove(name, "hab_")) |>
  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "% of locations", y = NULL, fill = NULL)
```

:::

# Modeling

## Train / test

Create an initial split:

```{r}
#| echo: true

set.seed(470)
feeder_split <- site_data |>
  initial_split(strata = squirrels) # default prop = 3/4 of each group
```

Save training data

```{r}
#| echo: true

feeder_train <- training(feeder_split)
dim(feeder_train)
```

Save testing data

```{r}
#| echo: true

feeder_test  <- testing(feeder_split)
dim(feeder_test)
```

## Training data

```{r}
#| echo: true

feeder_train
```

## Feature engineering

-   We prefer simple models when possible, but **parsimony** does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity

-   Variables that go into the model and how they are represented are just as critical to success of the model

-   **Feature engineering** allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)

-   Feature engineering is the act of creating new variables from the ones that already exist.

## Feature engineering - examples

- date: change to weekend / weekday
- categorical: reduce # levels, one-hot encoding
- numerical: transform using square, ln, normalize, scale
- bin numerical data to make it categorical
- missing data: its own level? impute?
- adjust outliers

## Modeling workflow

-   Create a **recipe** for feature engineering steps to be applied to the training data

-   Fit the model to the training data after these steps have been applied

-   Using the model estimates from the training data, predict outcomes for the test data

-   Evaluate the performance of the model on the test data

# Building recipes

## Initiate a recipe

```{r}
#| echo: true
#| code-line-numbers: "|2|3"

feeder_rec <- recipe(
  squirrels ~ .,    # formula
  data = feeder_train 
  )

feeder_rec
```

## Working with recipes {.smaller}

-   When building recipes you in a pipeline, you don't get to see the effect of the recipe on your data, which can be unsettling
-   You can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model
-   This requires two functions: `prep()` to train the recipe and `bake()` to apply it to your data

. . .

::: callout-note
Using `prep()` and `bake()` are shown here for demonstrative purposes. They do not need to be a part of your pipeline. I do find them assuring, however, so that I can see the effects of the recipe steps as the recipe is built.
:::

## Impute missing values & Remove zero variance predictors {.smaller}

Impute missing values (replace with mean).  
Remove all predictors that contain only a single value

```{r}
#| echo: true
#| code-line-numbers: "|2|3"

feeder_rec <- feeder_rec |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())

feeder_rec
```

## Prep and bake {.smaller}

```{r}
#| echo: true

feeder_rec_trained <- prep(feeder_rec)

bake(feeder_rec_trained, feeder_train)
```

# Building workflows

## Specify model

```{r}
#| echo: true

feeder_log <- logistic_reg() |>
  set_engine("glm")

feeder_log
```

## Build workflow

**Workflows** bring together models and recipes so that they can be easily applied to both the training and test data.

```{r}
#| echo: true

feeder_wflow <- workflow() |>
  add_recipe(feeder_rec) |>
  add_model(feeder_log) 
```

<br>

*See next slide for workflow...*

## View workflow

```{r}
#| echo: true

feeder_wflow
```

## Fit model to training data {.smaller}

```{r}
#| echo: true

feeder_fit <- feeder_wflow |>
  fit(data = feeder_train)

feeder_fit |> tidy()
```

<br>

. . .

*So many predictors!*

## Model fit summary

```{r}
#| echo: true

feeder_fit |> tidy() 
```

## All together

::: {.panel-tabset}

## recipe

```{r echo = TRUE}
feeder_rec <- recipe(
  squirrels ~ .,    # formula
  data = feeder_train 
  ) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())

feeder_rec
```

## model

```{r echo = TRUE}
feeder_log <- logistic_reg() |>
  set_engine("glm")

feeder_log
```
## workflow

```{r echo = TRUE}
feeder_wflow <- workflow() |>
  add_recipe(feeder_rec) |>
  add_model(feeder_log) 
feeder_wflow
```

## fit

```{r echo = TRUE}
feeder_fit <- feeder_wflow |>
  fit(data = feeder_train)

feeder_fit |> tidy()
```
## predict

```{r echo = TRUE}
feeder_train_pred <- predict(feeder_fit, feeder_train, type = "prob") |>
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) |>
  bind_cols(feeder_train |> select(squirrels))

feeder_train_pred
```


:::



# Evaluate model

> sensitivty = number of squirrels that are accurately predicted to be squirrels (true positive rate)

> specificity = number of non-squirrels that are accurately predicted to be non-squirrels (true negative rate)

## Make predictions for training data

```{r}
#| echo: true
feeder_train_pred <- predict(feeder_fit, feeder_train, type = "prob") |>
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) |>
  bind_cols(feeder_train |> select(squirrels))

feeder_train_pred
```

## Accuracy

```{r}
#| echo: true

rbind(accuracy(feeder_train_pred, truth = squirrels, 
               estimate = .pred_class),
      sensitivity(feeder_train_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"),
      specificity(feeder_train_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"))
```

## Visualizing accuracy

```{r}
#| echo: true

feeder_train_pred |>
  select(squirrels, .pred_class) |>
  yardstick::conf_mat(squirrels, .pred_class) |>
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

## Different predictions for training data

Note that you now have to be predicted 0.75 to be a squirrel!

```{r}
#| echo: true
feeder_train_pred_0.75 <- predict(feeder_fit, feeder_train, type = "prob") |>
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.75,
                                        "squirrels", "no squirrels"))) |>
  bind_cols(feeder_train |> select(squirrels))

feeder_train_pred
```


```{r}
#| echo: true

rbind(accuracy(feeder_train_pred_0.75, truth = squirrels, 
               estimate = .pred_class),
      sensitivity(feeder_train_pred_0.75, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"),
      specificity(feeder_train_pred_0.75, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"))
```


## But, really...

*who cares about predictions on **training** data?*

## Make predictions for testing data

```{r}
#| echo: true

feeder_test_pred <- predict(feeder_fit, feeder_test, type = "prob") |>
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) |>
  bind_cols(feeder_test |> select(squirrels))

feeder_test_pred
```

## Evaluate testing data

```{r}
#| echo: true

rbind(accuracy(feeder_test_pred, truth = squirrels, 
               estimate = .pred_class),
      sensitivity(feeder_test_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"),
      specificity(feeder_test_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"))
```

## Evaluate testing data

```{r}
#| echo: true

feeder_test_pred |>
  select(squirrels, .pred_class) |>
  yardstick::conf_mat(squirrels, .pred_class) |>
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

## Training vs. testing

```{r}
rbind(accuracy(feeder_train_pred, truth = squirrels, 
               estimate = .pred_class),
      sensitivity(feeder_train_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"),
      specificity(feeder_train_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"))

rbind(accuracy(feeder_test_pred, truth = squirrels, 
               estimate = .pred_class),
      sensitivity(feeder_test_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"),
      specificity(feeder_test_pred, truth = squirrels, 
                  estimate = .pred_class, event_level = "second"))

```

## What's up with training data? {.smaller}

-   The training set does not have the capacity to be a good arbiter of performance.

-   It is not an independent piece of information; predicting the training set can only reflect what the model already knows.

-   Suppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test.


## All together

::: {.panel-tabset}

## recipe

```{r echo = TRUE}
feeder_rec <- recipe(
  squirrels ~ .,    # formula
  data = feeder_train 
  ) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())

feeder_rec
```

## model

```{r echo = TRUE}
feeder_log <- logistic_reg() |>
  set_engine("glm")

feeder_log
```
## workflow

```{r echo = TRUE}
feeder_wflow <- workflow() |>
  add_recipe(feeder_rec) |>
  add_model(feeder_log) 

feeder_wflow
```

## fit

```{r echo = TRUE}
feeder_fit <- feeder_wflow |>
  fit(data = feeder_train)

feeder_fit |> tidy()
```
## predict

```{r echo = TRUE}
feeder_test_pred <- predict(feeder_fit, feeder_test, type = "prob") |>
  mutate(.pred_class = as.factor(ifelse(.pred_squirrels >=0.5,
                                        "squirrels", "no squirrels"))) |>
  bind_cols(feeder_test |> select(squirrels))

feeder_train_pred
```

## assess

```{r echo = TRUE}
rbind(accuracy(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      sensitivity(feeder_test_pred, truth = squirrels, estimate = .pred_class),
      specificity(feeder_test_pred, truth = squirrels, estimate = .pred_class))
```

:::



# Cross Validation

::: nonincremental
-   Cross validation for model comparison (which model is best?)
-   Cross validation for model evaluation (what is the accuracy rate of the model?)
:::

## Competing models

*  we use the test data to assess how the model does.  But we haven't yet thought about how to use the data to **build** a particular model.

* compare two different models to predict whether or not there are squirrels
    - Model 1: removes the information about the habitat and about the trees and shrubs
    - Model 2: removes the information about feeding the birds
    
    
## Compare recipes

::: {.panel-tabset}

## recipe 1
```{r echo = TRUE}
feeder_rec1 <- recipe(squirrels ~ ., data = feeder_train) |>
  # delete the habitat variables
  step_rm(contains("hab")) |>
  # delete the tree/shrub info
  step_rm(contains("atleast")) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())
```

## recipe 2
```{r echo = TRUE}
feeder_rec2 <- recipe(squirrels ~ ., data = feeder_train) |>
  # delete the variables on when the birds were fed
  step_rm(contains("fed")) |>
  # delete the variables about the bird feeders
  step_rm(contains("feed")) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())
```

:::


## How can we decide?

* ~~measure which model does better on the test data~~
* ~~measure which model does better on the training data~~
* measure which model does better on the cross validated data

## How does cross validation work?

```{r fig.cap = "4-fold CV is depicted.  Notice that the holdout group is never used as part of the coefficient estimation process.", out.width = "90%", fig.align='center', echo=FALSE}
knitr::include_graphics("figs/CV.png")
```

## Cross validation

More specifically, **v-fold cross validation**:

-   Randomly partition the **training** **data** into v group
-   Use v-1 groups to build the model (calculate MLEs); use 1 group for prediction / assessment
-   Repeat v times, updating which group is used for assessment each time

## Cross validation, step 1

Consider the example below where the **training** **data** are randomly split into 3 partitions:

```{r fig.cap = "Splitting the data into a partition of v=3 groups. Source: [@tidymodelingR]", fig.alt = "Thirty observations are seen where three colors are used to demonstrate that the observations can be partitioned into three groups.", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("figs/three-CV.svg")
```

## Cross validation, steps 2 and 3 

-   Use 1 partition for assessment, and the remaining v-1 partitions for analysis
-   Repeat v times, updating which partition is used for assessment each time

```{r fig.cap = "With the data split into three groups, we can see how 2/3 of the observations are used to fit the model and 1/3 of the observations are used to estimate the performance of the model. Source: [@tidymodelingR]", fig.alt = "Three iterations of model fitting are shown, each time using only 2/3 of the observations.  The remaining 1/3 of the observations are used to estimate the performance of the model.", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("figs/three-CV-iter.svg")
```


## Cross validation using tidymodels

```{r echo = TRUE}
set.seed(4747)
folds <- vfold_cv(feeder_train, v = 3, strata = squirrels)
folds
```


## Fit the model separately to each fold


::: {.panel-tabset}

## recipe

```{r echo = TRUE}
feeder_rec1 <- recipe(squirrels ~ ., data = feeder_train) |>
  # delete the habitat variables
  step_rm(contains("hab")) |>
  # delete the tree/shrub info
  step_rm(contains("atleast")) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())

feeder_rec1
```

## model

```{r echo = TRUE}
feeder_log <- logistic_reg() |>
  set_engine("glm")

feeder_log
```
## workflow

```{r echo = TRUE}
feeder_wflow1 <- workflow() |>
  add_recipe(feeder_rec1) |>
  add_model(feeder_log) 

feeder_wflow1
```

## fit

```{r echo = TRUE}
metrics_interest <- metric_set(accuracy, roc_auc, 
                              sensitivity, specificity)

feeder_fit_rs1 <- feeder_wflow1 |>
  fit_resamples(resamples = folds,
                metrics = metrics_interest,
                control = control_resamples(save_pred = TRUE,
                                            event_level = "second"))
```
## predict

```{r echo = TRUE}
feeder_fit_rs1 |> augment() |>
  select(squirrels, .pred_class) |>
  yardstick::conf_mat(squirrels, .pred_class) |>
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

## assess

```{r echo = TRUE}
collect_metrics(feeder_fit_rs1)
```
:::

## Repeat the CV analysis for the second model


::: {.panel-tabset}

## recipe

```{r echo = TRUE}
feeder_rec2 <- recipe(squirrels ~ ., data = feeder_train) |>
  # delete the variables on when the birds were fed
  step_rm(contains("fed")) |>
  # delete the variables about the bird feeders
  step_rm(contains("feed")) |>
  step_impute_mean(all_numeric_predictors()) |>
  step_nzv(all_numeric_predictors())

feeder_rec2
```

## model

```{r echo = TRUE}
feeder_log <- logistic_reg() |>
  set_engine("glm")

feeder_log
```
## workflow

```{r echo = TRUE}
feeder_wflow2 <- workflow() |>
  add_recipe(feeder_rec2) |>
  add_model(feeder_log) 

feeder_wflow2
```

## fit

```{r echo = TRUE}
metrics_interest <- metric_set(accuracy, roc_auc, 
                              sensitivity, specificity)

feeder_fit_rs2 <- feeder_wflow2 |>
  fit_resamples(resamples = folds,
                metrics = metrics_interest,
                control = control_resamples(save_pred = TRUE,
                                            event_level = "second"))
```
## predict

```{r echo = TRUE}
feeder_fit_rs2 |> augment() |>
  select(squirrels, .pred_class) |>
  yardstick::conf_mat(squirrels, .pred_class) |>
  autoplot(type = "heatmap") + 
  scale_fill_gradient(low="#D6EAF8", high="#2E86C1") 
```

## assess

```{r echo = TRUE}
collect_metrics(feeder_fit_rs2)
```
:::

## Compare two models

#### Model 1

```{r echo = TRUE}
collect_metrics(feeder_fit_rs1)
```

#### Model 2

```{r echo = TRUE}
collect_metrics(feeder_fit_rs2)
```

## Cross validation then test data

```{r fig.cap = "Nested cross-validation: two cross-validation loops are run one inside the other.  [@CVpaper]", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("figs/CV.jpg")
```

## Bias-variance trade-off

```{r fig.cap = "Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  [@ESL]", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("figs/varbias.png")
```

---

