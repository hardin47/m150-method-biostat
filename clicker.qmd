---
---
<!-- the two formats are html and revealjs -->

# Clicker Q

to go with **Practicing Statistics** by Kuiper & Sklar.  Math 150 - Methods in Biostatistics.

```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```

```{r}
#| echo: false
#| message: false
#| warning: false

# figure options
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(tidyverse)
library(plotROC)
library(survival)
```

---

In terms of the prerequisite for Math 150, Methods in Biostatistics, you should know at least a little bit (hopefully a lotta bit) about the following topics.

1. Hypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-square test.[^1]

   (a) Never heard of it
   (b) Heard of it, but don't know anything about it
   (c) Know a little about it (or did once)
   (d) Know something about it
   (e) Confident about it

[^1]: preferably d or e.  maybe c on some of them.

---

In terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics

2. Interaction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.[^2]

   (a) Never heard of it
   (b) Heard of it, but don't know anything about it
   (c) Know a little about it (or did once)
   (d) Know something about it
   (e) Confident about it

[^2]: these are the topics we will be covering.  Would be nice if you have heard of them.

---

3. The Central Limit Theorem (CLT) says:[^3]
   a. The sample average (statistic) converges to the true average (parameter)
   b. The sample average (statistic) converges to some point
   c. The distribution of the sample average (statistic) converges to a normal distribution
   d. The distribution of the sample average (statistic) converges to some distribution
   e. I have no idea what the CLT says
   

[^3]: c. The distribution of the sample average (statistic) converges to a normal distribution

---

4. The p-value is the probability:[^4]
   a. that the null hypothesis is true given the observed data.
   b. of data as or more extreme than the observed data given that the null hypothesis is true.

[^4]: b. of data as or more extreme than the observed data given that the null hypothesis is true.

---

5. Why do we use a t distribution (instead of a z / normal distribution) in the t-test?[^5]
   a. the technical conditions don't hold
   b. the means are quite variable
   c. we like the letter t
   d. we have two samples
   e. we don't know the true standard deviation parameter

[^5]:  e. we don't know the true standard deviation parameter

---


6. What happens if a t-test is used but isn't appropriate (technical conditions don't hold)?[^6]
   (a) the p-value isn't actually the probability of our data or more extreme if H0 is true.
   (b) the software won't give a p-value as output
   (c) the rejection region needs to be calculated in the opposite direction
   (d) the world blows up

[^6]: a. the p-value isn't actually the probability of our data or more extreme if H0 is true.

---

7. We use linear regression to run a test of means 
($x_i = 0$ for controls, group 1; $x_i = 1$ for cases, group 2)
What is: $\sum_i x_i$?[^7]
   (a) $n$
   (b) $n_1$
   (c) $n_2$
   (d) $n_1 \cdot \overline{y}_1$
   (e) $n_2 \cdot \overline{y}_2$

[^7]: c. $n_2$

---

8. We use linear regression to run a test of means 
($x_i = 0$ for controls, group 1; $x_i = 1$ for cases, group 2)
What is: $\sum_i x_iy_i$?[^8]
   (a) $n$
   (b) $n_1$
   (c) $n_2$
   (d) $n_1 \cdot \overline{y}_1$
   (e) $n_2 \cdot \overline{y}_2$

[^8]: e. $n_2 \cdot \overline{y}_2$

---

9. The regression technical conditions include:[^9]
    (a) The Y variable is normally distributed
    (b) The X variable is normally distributed
    (c) The residuals are normally distributed
    (d) The slope coefficient is normally distributed
    (e) The intercept coefficient is normally distributed

[^9]: d. The residuals are normally distributed (which induces a., d., and e.).  There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).

---

10. We need the technical conditions to hold in order to calculate $b_0$ and $b_1.$[^10]

    (a) TRUE
    (b) FALSE
    (c) It depends
    
[^10]: FALSE.  We can always minimize the sums of squares, regardless of whether or not the model is any good.

---

11. Why do we check technical conditions?[^11]
    (a) so that the inference is valid
    (b) so that the estimates are valid
    (c) so that the p-value is more likely to be small
    (d) so that the confidence level is right
    (e) for fun

[^11]: a. so that the inference is valid (and also for fun).  Note that d. so that the confidence level is right is also a correct answer because confidence intervals are all part of the "inference" paradigm.

---

12. When writing the regression equation, why is there a hat ( ^) on the response variable?[^12]
    (a) because the prediction is an estimate
    (b) because the prediction is an average
    (c) because the prediction may be due to extrapolation
    (d) a & b
    (e) all of the above

[^12]: d. due to estimation and average

---

13. With a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?[^13]
    (a) happiness causes longer lives
    (b) longer lives cause happiness
    (c) happiness and longer life are correlated
    (d) happiness and longer life are perfectly predictive
    (e) happiness and longer life are unrelated

[^13]: c. happiness and longer life are correlated

---

14. If there is no relationship in the population (true correlation = 0), then r = 0.[^14]
    (a) TRUE
    (b) FALSE

[^14]: b. FALSE, there is no reason that the statistic will equal the parameter.

---

15. If there is no relationship in the population (true slope $\beta_1 = 0$), then $b_1=0$.[^15]
    (a) TRUE
    (b) FALSE

[^15]: b. FALSE, there is no reason that the statistic will equal the parameter.



---

16. Smaller variability around the regression line ($\sigma$):[^16]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn't necessarily change the variability of $b_1$.

[^16]: b. decreases the variability of $b_1$.

---

17. Smaller variability in the explanatory variable (SD(X) = $s_X$):[^17]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn't necessarily change the variability of $b_1$.

[^17]: a. increases the variability of $b_1$.

---

18. A smaller sample size ($n$):[^18]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn't necessarily change the variability of $b_1$.

[^18]: a. increases the variability of $b_1$.

---

19. We transform our variables...[^19]
    (a) ... to find the highest $r^2$ value.
    (b) ... when the X variable is not normally distributed.
    (c) ... to make the model easier to interpret.
    (d) ... so that the technical conditions are met.

[^19]: d. so that the technical conditions are met.


---

20. In the Botox and Pain Relief example, the p-value is calculated.  What does "probability" refer to?[^20]
    (a) random allocation
    (b) random sample

[^20]: a. random allocation

---

p-value = probability of the observed data or more extreme given the null hypothesis is true.

---

21. "Observed data or more extreme" is:[^21]
    (a) fewer than 9
    (b) 9 or fewer
    (c) 9 or more
    (d) more than 9

[^21]: c. 9 or more

---


22. What is the mean value of the null sampling distribution for the number of Botox therapy who showed pain reduction?[^22]
    (a) 0
    (b) 9
    (c) 5.3
    (d) 11
    (e) 15

[^22]: c. 5.3 because (15/31)*11 = 5.3

---

23. What conclusion would you draw from the Back Pain and Botox study?[^23]
    (a) Not enough evidence to conclude that Botox is more effective than the placebo.
    (b) Strong evidence that Botox is equally as effective as the placebo.
    (c) Strong evidence that Botox is more effective than the placebo.
    
[^23]: c. Strong evidence that Botox is more effective than the placebo.

---

24. If we consider those in the study with back pain to be representative of all people with back pain, what would you conclude about the percentage of people who will have reduced back pain if they use Botox?[^24]
    (a) Substantially greater than 50%
    (b) Substantially less than 50%
    (c) Close to 50%

[^24]: c. Close to 50%  (the point estimate is 0.6)

---

25. Material check-in
    (a) So far, so good
    (b) Concepts are good, R is confusing
    (c) R is good, concepts are confusing
    (d) Everything is confusing
    
---

26. People check-in
    (a) So far, so good
    (b) I can go to office hours / mentor sessions, but I didn't happen to this week.
    (c) I can't make the scheduled office hours / mentor sessions
    (d) I'm looking for someone to study with
    
---

See Canvas front page for **anonymous** survey / feedback for the class.  Also, if you are looking for people to work with, you could contact me directly (non-anonymously!) so that I can connect you to people.

---
    
27. Sample 1,000,000 people who are over 6' tall and 1,000,000 people who are under 6' tall.  Record if the person is in the NBA.  What is measurable?[^27]
    (a) P(NBA if 6' tall)
    (b) P(6' tall if in the NBA)
    (c) both
    (d) neither  

[^27]: a. P(NBA if 6' tall) (cohort: cannot measure the probability of the explanatory variable given the response)

---

28. Sample 100 people who are in the NBA and 100 people who are not in the NBA. Record if the person is over 6' tall. What is measurable?[^28]
    (a) P(NBA if 6' tall)
    (b) P(6' tall if in the NBA)
    (c) both
    (d) neither  

[^28]: b. P(6' tall if in the NBA) (case-control:  cannot measure the probability of the response variable given a level of the explanatory variable)

---

29. Sample 10,000,000 people.  Record their height and whether or not they are in the NBA. What is measurable?[^29]
    (a) P(NBA if 6' tall)
    (b) P(6' tall if in the NBA)
    (c) both
    (d) neither  

[^29]: c. both (cross-classification: can measure all the probabilities)

---

30. Calcium channel blockers have recently been reported to be associated with increased mortality. Cardiac patients who recently died of their heart disease were compared to control cardiac patients with similar disease who survive. Assume such a study had found that 40% of the recent cardiac deaths were taking calcium channel blockers at the time of death, as compared to 25% of the controls.[^30]
    (a) Case-control
    (b) Cohort
    (c) Cross-classification

[^30]: a. case-control (they selected based on people who had died or not)

---

31. It is well known that the use of urinary catheters conveys a substantial risk of urinary tract infection (UTI). A group of physicians believe that, in an intensive care setting, use of one particular type of urinary catheter is more likely to encourage infection than use of other types. They therefore review medical records over a recent period for all uses of urinary catheters in an ICU. They find that 200 new UTIs occurred during 1000 ICU patient-days of catheterization with the suspect type of catheter, as compared to 100 new UTIs during 5000 ICU-patient days of catheterization with all other types. Noting the increased frequency of new UTIs when the suspect catheter type is used, they regard their hypothesis as confirmed. To reduce nosocomial UTIs, they recommend discontinuing use of that type of catheter in the ICU.[^31]
    (a) Case-control
    (b) Cohort
    (c) Cross-classification

[^31]: b. cross-classification (they selected all uses of catheters)

---

32. When we select individuals based on the explanatory variable, we cannot accurately measure[^32]
    (a) the proportion of people in the population in each explanatory category
    (b) the proportion of people in the population in each response group
    (c) anything about the population
    (d) confounding variables

[^32]: a. the proportion of people in the population in each explanatory category (tbh, we can't measure b either, but we can measure the proportion of people in each response group, separated by the explanatory variable)

---

33. Relative Risk is[^33]
    (a) the difference of two proportions
    (b) the ratio of two proportions
    (c) the log of the ratio of two proportions
    (d) the log of the difference of two proportions

[^33]: b. the ratio of two proportions

---

34. The odds ratio is "invariant to which variable is explanatory and which is response" means:[^34]
    (a) we always put the bigger odds in the numerator
    (b) we must collect data so that we can estimate the response in the population
    (c) which variable is called the explanatory changes the value of the OR
    (d) which variable is called the explanatory does not change the value of the OR

[^34]: d. which variable is called the explanatory does not change the value of the OR

---

35. In finding a CI for RR = p1/p2, why is it okay to exponentiate the end points of the interval for ln(p1/p2)?[^35]
    (a) Because if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.
    (b) Because taking the natural log of the RR makes the  distribution approximately normal.
    (c) Because the natural log compresses values that are  bigger than 1 and spreads values that are smaller than 1.
    (d) Because we can get exact p-values using Fisher's Exact Test.

[^35]: a. Because if ln(p1/p2) is in the original interval, p1/p2 will be in the exponentiated interval.

---

36. In order to find a CI for the true OR, our steps are:[^36]
 i. find $\widehat{\ln(\mbox{OR})}$
 ii. add $\pm \ z^* \sqrt{\frac{1}{n_1 \hat{p}_1 (1-\hat{p}_1)} + \frac{1}{n_2 \hat{p}_2 (1-\hat{p}_2)}}$
 iii. take exp of the endpoints 
 
   (a) because the sampling distribution of $\widehat{\mbox{OR}}$ is normal
   (b) because OR is typically greater than 1
   (c) because the $\ln$ transformation makes the sampling distribution almost normal
   (d) because OR is invariant to the choice of explanatory or response variable

[^36]: c. because the $\ln$ transformation makes the sampling distribution almost normal

---

37. I know where to find: the solutions to the warm-ups, the clicker questions (with solutions), and the HW solutions[^37]
     (a) TRUE
     (b) FALSE
     
[^37]: The warm-up solutions and clicker questions are on the main course website.  The HW solutions are on Canvas under Files.


---


38. At the value $x = -\beta_0 / \beta_1$, the probability of success is:[^38]
    (a) 0
    (b) 0.5
    (c) 1
    (d) depends on $\beta_0$
    (e) depends on $\beta_1$

[^38]: b. 0.5

---

39. The logistic model gives probability of failure:[^39]
    (a) $\frac{e^{\beta_0+ \beta_1 x}}{1+ e^{\beta_0+ \beta_1 x}}$
    (b) $\frac{1}{1+ e^{\beta_0+ \beta_1 x}}$
    (c) $e^{\beta_0+ \beta_1 x}$
    (d) $e^{-(\beta_0+ \beta_1 x)}$
    (e) $\beta_0+ \beta_1 x$

[^39]: b. $\frac{1}{1+ e^{\beta_0+ \beta_1 x}}$

---

40. The logistic model gives odds of success:[^40]
    (a) $\frac{e^{\beta_0+ \beta_1 x}}{1+ e^{\beta_0+ \beta_1 x}}$
    (b) $\frac{1}{1+ e^{\beta_0+ \beta_1 x}}$
    (c) $e^{\beta_0+ \beta_1 x}$
    (d) $e^{-(\beta_0+ \beta_1 x)}$
    (e) $\beta_0+ \beta_1 x$

[^40]: c. $e^{\beta_0+ \beta_1 x}$

---

41. The logistic model gives odds of failure:[^41]
    (a) $\frac{e^{\beta_0+ \beta_1 x}}{1+ e^{\beta_0+ \beta_1 x}}$
    (b) $\frac{1}{1+ e^{\beta_0+ \beta_1 x}}$
    (c) $e^{\beta_0+ \beta_1 x}$
    (d) $e^{-(\beta_0+ \beta_1 x)}$
    (e) $\beta_0+ \beta_1 x$

[^41]: d. $e^{-(\beta_0+ \beta_1 x)}$

---

42. With a logistic regression model, the relative risk of success (for a one unit increase in X) is:[^42]
    (a) $- \beta_0/\beta_1$
    (b) $\beta_0+ \beta_1 x$
    (c) $e^{\beta_0+ \beta_1 x}$
    (d) a non-linear function of X (which depends on X )

[^42]: d. a non-linear function of X (which depends on X )

---

43. If we want the relative risk of survival (for a one unit increase in X) to be independent of X, we should use which link:[^43]
    (a) linear
    (b) logistic
    (c) complementary log-log
    (d) log-linear

[^43]: d. log-linear

---

44.  You take a sample of size 4 from a binary population and get: FSFF. 
(failure, success, failure, failure) What is your guess for p = P(success)?[^44]
     (a) 0.05
     (b) 0.15
     (c) 0.25
     (d) 0.5
     (e) 0.75
 
[^44]: c. 0.25

---

45.  In a logistic regression model, the variability is given by[^45]
     (a) Normal Y given X
     (b) Binomial Y given X
     (c) Bernoulli Y given X
     (d) Poisson Y given X

[^45]: c. Bernoulli Y given X

---

46.  When trying to find estimates for $\beta_0$  and $\beta_1$, we maximize the likelihood.  $$\prod_{i=1}^n \bigg(\frac{e^{\beta_0+ \beta_1 x_i}}{1+ e^{\beta_0+ \beta_1 x_i}}\bigg)^{y_i}\bigg(\frac{1}{1+ e^{\beta_0+ \beta_1 x_i}}\bigg)^{1 - y_i}$$
Take the derivative with respect to which variable(s):[^46]
     (a) X
     (b) Y
     (c) $\beta_0$
     (d) $\beta_1$
     (e) $\beta_0$ and $\beta_1$

[^46]: e. $\beta_0$ and $\beta_1$

---

47. Maximum likelihood estimation seeks to:[^47]
    (a) Find the data which are most likely under the model.
    (b) Find the parameters which are most likely under the model.
    (c) Find the parameters which make the data most likely under the model.
    (d) Find the data which make the parameters most likely under the model.

[^47]: c. Find the parameters which make the data most likely under the model.

---

48.  We use maximum likelihood estimation because:[^48]
     (a) It gives an principled approach for estimating the parameters.
     (b) The estimates are asymptotically normally distributed.
     (c) The estimates are always easy to compute.
     (d) All of the above.
     (e) Some of the above.

[^48]: e. Some of the above (a. It gives an principled approach for estimating the parameters. and b. The estimates are asymptotically normally distributed.)

---

49. We know that for a given data set (with MLEs of $b_0$,$b_1$):[^49]
    (a) $L(b_0,b_1)< L(b_0,\beta_1=0)$ always
    (b) $L(b_0,b_1)> L(b_0,\beta_1=0)$ always
    (c) $L(b_0,b_1) \leq L(b_0,\beta_1=0)$ always
    (d) $L(b_0,b_1) \geq L(b_0,\beta_1=0)$ always

[^49]: d. $L(b_0,b_1) \geq L(b_0,\beta_1=0)$ always


---

50. How many parameters did we estimate in the HERS warm-up with the additive model?[^50]
     a. 1
     b. 3
     c. 4
     d. 2757
     e. 2761

[^50]: c. 4 parameter estimates: $b_0, b_1, b_2, b_3$

---

51. How many parameters did we estimate in the HERS warm-up with the interaction model?[^51]
     a. 3
     b. 4
     c. 6
     d. 7
     e. 12

[^51]: d. 7 parameter estimates: $b_0, b_1, b_2, b_3, b_4, b_5, b_6$

---

52. What are the df for the LRT addressing whether interaction is needed in the HERS warm-up?[^52]
     a. 2
     b. 3
     c. 2760
     d. 2754
     e. 2757

[^52]: b. 3 (7 - 4 = 3)

---

53. (Bird nest example) How many parameters do we estimate when considering Length as a categorical variable?  (the only variable)[^53]
    (a) 0
    (b) 1
    (c) 2
    (d) 33
    (e) 34

[^53]: e. 34

---

54. (Bird nest example) How many df for the LRT addressing whether Length (as a categorical variable) belongs in the model?[^54]
    (a) 0
    (b) 1
    (c) 2
    (d) 33
    (e) 34

[^54]: d. 33 (34 - 1 = 33)

---

55. (Bird nest example) How many df for the LRT addressing whether Incubate and Color belong in the model (given Length is determined to be in the model)?[^55]
    (a) 0
    (b) 1
    (c) 2
    (d) 3
    (e) 4 

[^55]: c. 2 (4 - 2 = 2)

---

56. An interaction term in a multiple logistic regression model may be used when:[^56]
    (a) the model fit is poor.
    (b) there is a quadratic relationship between the response and explanatory variables.
    (c) neither one of two explanatory variables contribute significantly to the regression model.
    (d) the relationship between X1 and P(success) changes for differing values of X2.

[^56]: d. the relationship between X1 and P(success) changes for differing values of X2.

---

57. The interpretations of the main effects (on their own) make sense only when the interaction component is not significant.[^57]
    (a)  TRUE
    (b)	 FALSE

[^57]: a. TRUE

---

58. If the interaction is significant but the main effects aren't:[^58]
    (a) report on the significance of the main effects
    (b) remove the main effects from the model
    (c) avoid talking about main effects on their own
    (d) test whether the main effects are significant without interaction in the model

[^58]: c. avoid talking about main effects on their own

---

59. With two variables of interest, what should you test first?[^59]
    (a) Variable 1.
    (b) Variable 2.
    (c) The interaction between variables 1 and 2.
    (d) None of the above.

[^59]: c. The interaction between variables 1 and 2. (probably...  although there are many schools of thought on how to build models)

---

60. Consider variable 1 is continuous and variable 2 has 4 levels.  How many degrees of freedom are associated with the drop in deviance test (LRT) of their overall interaction?[^60]
    (a) 1
    (b) 2
    (c) 3
    (d) 4
    (e) 5

[^60]: c. 3 (1 * (4-1) = 3)

---

61. When selecting variables, it is important that[^61]
    (a) The model predicts training data well
    (b) The model predicts test data well
    (c) The coefficients on the variables are all significant
    (d) The relationships between the variables make sense

[^61]: b. The model predicts test data well

---

62. To get a sense of the true accuracy of the model, the test data should be assessed (for accuracy)[^62]
     a. on the first model only.
     b. on the last model only.
     c. on every model in the process.

[^62]: b. on the last model only.

---

63. If I am using all features of my dataset and I achieve 100% accuracy on my training set, but ~70% on testing set, what should I look out for?[^63]
    (a) Underfitting
    (b) Nothing, the model is perfect
    (c) Overfitting

[^63]: c. overfitting

---

64. If I am picking and choosing between features of my dataset and I achieve 30% accuracy on my training set, and ~30% on testing set, what should I look out for?[^64]
    (a) Underfitting
    (b) Nothing, the model is perfect
    (c) Overfitting

[^64]: a. underfitting

---

65. Cross validating will guarantee that the model does not overfit.[^65]
    (a)	TRUE
    (b)	FALSE

[^65]: b. FALSE.  CV reduces the effect of overfitting, but at the end of the day, you are still building a model on the dataset at hand, and it is possible that you will overfit that dataet.

---

66. Suppose we want to compute 10-Fold Cross-Validation error on 200 training examples. We need to compute a model error rate N1 times, and the Cross-Validation error is the average of the errors. To compute each error, we need to train a model with data of size N2, and test the model on the data of size N3.
What are the numbers for N1, N2, N3?[^66]
    (a) N1 = 1, N2 = 180, N3 = 20
    (b) N1 = 10, N2 = 180, N3 = 20
    (c) N1 = 10, N2 = 200, N3 = 20
    (d) N1 = 10, N2 = 200, N3 = 200
    (e) N1 = 20, N2 = 180, N3 = 20

[^66]: b. N1 = 10, N2 = 180, N3 = 20

---

67. You are reviewing papers for Fancy Conference, and you see submissions with the following claims. Which ones would you consider accepting?[^67]
     (a) My method achieves a training error lower than all previous  methods!
     (b) My method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min test error.)
     (c) My method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)
     (d) My method achieves a CV error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)

[^67]: c. My method achieves a test error lower than all previous methods! (Footnote: When variables are chosen so as to min CV error.)

```{r}
#| echo: false
#| eval: false

68. From the burn data, consider:  
$$P(\mbox{surv if } X = 0.4) = 0.3$$
$$P(\mbox{surv if } X = 0.2) = 0.9$$
The two points are concordant if[^68]
    (a)	X = 0.4 survives and X = 0.2 dies
    (b)	X = 0.4 dies and X= 0.2 survives
    (c)	They both survive
    (d)	They both die

[^68]: b. X = 0.4 dies and X= 0.2 survives
```

---

69. Which model is better (according to ROC)?[^69]
    (a) pink because it goes closer to (1,1)
    (b) pink because it is closer to y=x
    (c) blue because it is farther from y=x
    (d) blue because it is steeper
    (e) neither

```{r}
#| echo: false

set.seed(47)
D.ex <- rbinom(50, 1, .5)
rocdata <- data.frame(D = c(D.ex, D.ex), 
                   M = c(rnorm(50, mean = D.ex/4, sd = .1), rnorm(50, mean = (D.ex+1)/4, sd = .2)), 
                   Z = c(rep("A", 50), rep("B", 50)))

ggplot(rocdata, aes(m = M, d = D, color = Z)) + geom_roc() +
  theme(legend.title = element_blank()) +
  scale_color_manual(values = c("blue", "pink")) + 
  geom_abline(slope = 1, intercept = 0)

```

[^69]: c. blue because it is farther from the line y=x

---

70. In ROC curve, the x-axis measures[^70]
    (a) True Pos Rate which we want high
    (b) False Pos Rate which we want low
    (c) True Neg Rate which we want high
    (d) False Neg Rate which we want low

[^70]: b. False Pos Rate which we want low

---

71. Quiz on 11 topics (you know nothing).  Your friends know topics:  
A: {1, 2, 3, 4, 5, 6, 7}  
B: {8, 9, 10}  
C: {1, 2, 3, 4, 8, 10}  
D: {5, 6, 7, 9, 11}  
Who should you choose to help you answer the questions?[^71]
    (a) A
    (b) B
    (c) C
    (d) D
    (e) can't tell

[^71]: a. A

---

72.  Who do you want to choose next?[^72]  
A: {1, 2, 3, 4, 5, 6, 7}  
B: {8, 9, 10}  
C: {1, 2, 3, 4, 8, 10}  
D: {5, 6, 7, 9, 11}  
     (a) A
     (b) B
     (c) C
     (d) D
     (e) can't tell

[^72]: b. B

---

73.  If you can pick two people, who do you pick?[^73]  
A: {1, 2, 3, 4, 5, 6, 7}   
B: {8, 9, 10}   
C: {1, 2, 3, 4, 8, 10}  
D: {5, 6, 7, 9, 11}   
     (a) A, B
     (b) A, C
     (c) A, D
     (d) C, B
     (e) C, D

[^73]:  e. C and D

---

74. The variables in the k-variable model identified by **forward** selection are a subset of the variables in the (k+1)-variable model identified by **forward** selection.[^74]
    (a)	TRUE (always TRUE)
    (b)	FALSE (not always TRUE)

[^74]: a. TRUE

---

75. The variables in the k-variable model identified by **backward** selection are a subset of the variables in the (k+1)-variable model identified by **backward** selection.[^75]
    (a)	TRUE (always TRUE)
    (b)	FALSE (not always TRUE)

[^75]: a. TRUE

---

76. The variables in the k-variable model identified by **backward** selection are a subset of the variables in the (k+1)-variable model identified by **forward** selection.[^76]
    (a)	TRUE (always TRUE)
    (b)	FALSE (not always TRUE)

[^76]: b. FALSE

---

77. The variables in the k-variable model identified by **forward** selection are a subset of the variables in the (k+1)-variable model identified by **backward** selection.[^77]
    (a)	TRUE (always TRUE)
    (b)	FALSE (not always TRUE)

[^77]: b. FALSE

---

78. The variables in the k-variable model identified by **best-subsets** selection are a subset of the variables in the (k+1)-variable model identified by **best-subsets** selection.[^78]
    (a)	TRUE (always TRUE)
    (b)	FALSE (not always TRUE)

[^78]: b. FALSE

---

79. In a drop-in-deviance test (LRT), the reduced model corresponds to the null hypothesis being true.[^79]
    (a) TRUE
    (b) FALSE

[^79]: a. TRUE (the coefficient values are forced to be zero)

---

80.  In a drop-in-deviance test (LRT), the full model corresponds to the alternative hypothesis being true.[^80]
     (a) TRUE
     (b) FALSE

[^80]: b. FALSE (the null model can exist within the full model because there is flexibility in the values of the coefficients)

---

81. With model building:[^81]
    (a)	There are many ways to find a good model.
    (b)	There is always one right answer.
    (c)	There is no end to the fun.
    (d)	Can we take a pure math class yet?

[^81]: a. There are many ways to find a good model.  Also, c. there is no end to the fun.

---

82. When probability of being able to buy a candy bar is modeled as a function of the number of coins, the coefficient on number of coins is:[^82]
     (a) positive
     (b) negative
     (c) zero
     (d) no intuition exists for being able to answer this question

[^82]: a. positive

---

83.  When probability of being able to buy a candy bar is modeled as a function of the number of low coins, the coefficient on number of low coins is:[^83]
     (a) positive
     (b) negative
     (c) zero
     (d) no intuition exists for being able to answer this question 

[^83]: a. positive

---

84.  When probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of coins is:[^84]
     (a) positive
     (b) negative
     (c) zero
     (d) no intuition exists for being able to answer this question

[^84]: a. positive

---

85. When probability of being able to buy a candy bar is modeled as a function of the number of coins and number of low coins, the coefficient on number of low coins is:[^85]
     (a) positive
     (b) negative
     (c) zero
     (d) no intuition exists for being able to answer this question

[^85]: b. negative

---

86. If we consider the censored times to be event times, the empirical survival curve will (on average)[^86]
    (a) underestimate the parameter
    (b) overestimate the parameter
    (c) sometimes under and sometimes overestimate the parameter

[^86]: a. underestimate the parameter

---

87. $n_i - d_i = n_{i+1}$ when:[^87]
    (a) there are no deaths at time $t_i$
    (b) there is no censoring at time $t_i$
    (c) there are no deaths at time $t_{i+1}$
    (d) there is no censoring at time $t_{i+1}$
    (e) there is no censoring at time $t_{i-1}$

[^87]: b. there is no censoring at time $t_i$

---

88. $\frac{(n_i - d_i)}{n_i} = 1$ when:[^88]
    (a) there are no deaths at time $t_i$
    (b) there is no censoring at time $t_i$
    (c) there are no deaths at time $t_{i+1}$
    (d) there is no censoring at time $t_{i+1}$
    (e) there is no censoring at time $t_{i-1}$

[^88]: a. there are no deaths at time $t_i$

---


89. Prop survive > 50 days, `treated` (turquoise line)[^89]
    (a) ~0.65
    (b) ~0.35
    (c) ~0.45
    (d) we only know it's bigger than red
    (e) we only know it's smaller than red
    
```{r}
#| echo: false

surv_data <- data.frame(time = c(45, 35, 48, 24, 42, 52, 55, 47, 42, 74, 62, 32, 41, 69, 17),
censor = c(0,1,0,1,1,1,0,1,1,1,0,0,0,1,0), group = c("control", "control", "control", "control", "control", "control","treated", "treated", "treated", "treated","treated", "treated", "treated", "treated", "treated"))

survfit(Surv(time, censor) ~ group, data = surv_data) %>%
survminer::ggsurvplot(conf.int = FALSE)
```

[^89]: a. ~0.65


---

```{r}
#| echo: false

surv_data <- data.frame(time = c(45, 35, 48, 24, 42, 52, 55, 47, 42, 74, 62, 32, 41, 69, 17),
censor = c(0,1,0,1,1,1,0,1,1,1,0,0,0,1,0), group = c("control", "control", "control", "control", "control", "control","treated", "treated", "treated", "treated","treated", "treated", "treated", "treated", "treated"))

survfit(Surv(time, censor) ~ group, data = surv_data) %>%
survminer::ggsurvplot(conf.int = FALSE)
```

---

90. Kaplan Meier curves (Log-Rank p-value),[^90]
    (a) blue is clearly better
    (b) red is clearly better
    (c) can't tell because they cross
    (d) can't tell because the p-value is big
    (e) can't tell because the p-value is small
    
```{r fig.height = 4}
#| echo: false

surv_data <- data.frame(time = c(45, 35, 48, 64, 42, 52, 55, 37, 42, 74, 32, 32, 41, 49, 17),
censor = c(0,1,0,1,1,1,0,1,1,1,0,0,0,1,0), group = c("control", "control", "control", "control", "control", "control","treated", "treated", "treated", "treated","treated", "treated", "treated", "treated", "treated"))

survfit(Surv(time, censor) ~ group, data = surv_data) %>%
survminer::ggsurvplot(conf.int = FALSE, pval=TRUE)
```

[^90]: c. can't tell because they cross (and also because d. the p-value is big)

---

```{r}
#| echo: false

surv_data <- data.frame(time = c(45, 35, 48, 64, 42, 52, 55, 37, 42, 74, 32, 32, 41, 49, 17),
censor = c(0,1,0,1,1,1,0,1,1,1,0,0,0,1,0), group = c("control", "control", "control", "control", "control", "control","treated", "treated", "treated", "treated","treated", "treated", "treated", "treated", "treated"))

survfit(Surv(time, censor) ~ group, data = surv_data) %>%
survminer::ggsurvplot(conf.int = FALSE, pval=TRUE)
```
---

91. The hazard at time $t$ represents:[^91]
    (a) the probability of the event
    (b) the instantaneous rate of the event
    (c) the relative risk of the event
    (d) the odds ratio of the event
    
[^91]: b. the instantaneous rate of the event

---

92. The last entry in the table for the h(t) column is `NA` because:[^92]
    (a) the last observation was a death
    (b) the last observation was censored
    (c) the time interval is too big
    (d) the time interval is too small
    
```{r fig.cap = "Table 9.6  [@KuiperSklar]", out.width='100%', fig.align='center', echo=FALSE}
knitr::include_graphics("handout/figs/hazard_NA.jpg")
```

[^92]: b. the last observation was censored

---


93. Censored observations are$\ldots$?[^93]
    (a) More important than non-censored ones in survival analysis
    (b) Are assumed to be normally distributed over time
    (c) Are assumed to have the same survival chances as uncensored observations
    (d) Are essential to allow calculation of the Kaplan Meier plot
    (e) Are allocated to the baseline survival curve

[^93]: c. Are assumed to have the same survival chances as uncensored observations





---

