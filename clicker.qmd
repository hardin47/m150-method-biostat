---
---
<!-- the two formats are html and revealjs -->

# Clicker Q

to go with **Practicing Statistics** by Kuiper & Sklar.  Math 150 - Methods in Biostatistics.

```{=html}
<style>
.reveal ol ol {
   list-style-type: lower-alpha;
}
</style>
```


---

In terms of the prerequisite for Math 150, Methods in Biostatisitcs, you should know at least a little bit (hopefully a lotta bit) about the following topics.

1. Hypothesis test, confidence interval, sample mean, central limit theorem, standard deviation, standard error of a statistics, p-value, t-test, chi-square test.[^1]

   (a) Never heard of it
   (b) Heard of it, but don’t know anything about it
   (c) Know a little about it (or did once)
   (d) Know something about it
   (e) Confident about it

[^1]: preferably d or e.  maybe c on some of them.

---

In terms of the prerequisite for Math 150, Methods in Biostatisitcs, you do not need to know the following topics

2. Interaction, simple linear regression, multiple linear regression, logistic regression, survival analysis, R.[^2]

   (a) Never heard of it
   (b) Heard of it, but don’t know anything about it
   (c) Know a little about it (or did once)
   (d) Know something about it
   (e) Confident about it

[^2]: these are the topics we will be covering.  Would be nice if you have heard of them.

---

3. The Central Limit Theorem (CLT) says:[^3]
   a. The sample average (statistic) converges to the true average (parameter)
   b. The sample average (statistic) converges to some point
   c. The distribution of the sample average (statistic) converges to a normal distribution
   d. The distribution of the sample average (statistic) converges to some distribution
   e. I have no idea what the CLT says
   

[^3]: c. The distribution of the sample average (statistic) converges to a normal distribution

---

4. The p-value is the probability:[^4]
   a. that the null hypothesis is true given the observed data.
   b. of data as or more extreme than the observed data given that the null hypothesis is true.

[^4]: b. of data as or more extreme than the observed data given that the null hypothesis is true.

---

5. Why do we use a t distribution (instead of a z / normal distribution) in the t-test?[^5]
   a. the technical conditions don’t hold
   b. the means are quite variable
   c. we like the letter t
   d. we have two samples
   e. we don’t know the true standard deviation parameter

[^5]:  e. we don’t know the true standard deviation parameter

---


6. What happens if a t-test is used but isn’t appropriate (technical conditions don’t hold)?[^5]
   (a) the p-value isn’t actually the probability of our data or more extreme if H0 is true.
   (b) the software won’t give a p-value as output
   (c) the rejection region needs to be calculated in the opposite direction
   (d) the world blows up

[^5]: a. the p-value isn’t actually the probability of our data or more extreme if H0 is true.

---

7. We use linear regression to run a test of means 
($x_i = 0$ for controls, group 1; $x_i = 1$ for cases, group 2)
What is: $\sum_i x_i$?[^6]
   (a) $n$
   (b) $n_1$
   (c) $n_2$
   (d) $n_1 \cdot \overline{y}_1$
   (e) $n_2 \cdot \overline{y}_2$

[^6]: c. $n_2$

---

8. We use linear regression to run a test of means 
($x_i = 0$ for controls, group 1; $x_i = 1$ for cases, group 2)
What is: $\sum_i x_iy_i$?[^7]
   (a) $n$
   (b) $n_1$
   (c) $n_2$
   (d) $n_1 \cdot \overline{y}_1$
   (e) $n_2 \cdot \overline{y}_2$

[^7]: e. $n_2 \cdot \overline{y}_2$

---

9. With a strong correlation and very small p-value, what can we conclude about happiness and life expectancy?[^8]
   (a) happiness causes longer lives
   (b) longer lives cause happiness
   (c) happiness and longer life are correlated
   (d) happiness and longer life are perfectly predictive
   (e) happiness and longer life are unrelated

[^8]: c. happiness and longer life are correlated

---

10. Why do we check technical conditions?[^9]
    (a) so that the inference is valid
    (b) so that the estimates are valid
    (c) so that the p-value is more likely to be small
    (d) so that the confidence level is right
    (e) for fun

[^9]: a. so that the inference is valid (and also for fun)

---

11. When writing the regression equation, why is there a hat ( ^) on the response variable?[^10]
    (a) because the prediction is an estimate
    (b) because the prediction is an average
    (c) because the prediction may be due to extrapolation
    (d) a & b
    (e) all of the above

[^10]: d. due to estimation and average

---

12. If there is no relationship in the population (true correlation = 0), then r = 0.[^11]
    (a) TRUE
    (b) FALSE

[^11]: b. FALSE, there is no reason that the statistic will equal the parameter.

---

13. If there is no relationship in the population (true slope $\beta_1 = 0$), then $b_1=0$.[^12]
    (a) TRUE
    (b) FALSE

[^12]: b. FALSE, there is no reason that the statistic will equal the parameter.

---

14. The regression technical conditions include:[^13]
    (a) The Y variable is normally distributed
    (b) The X variable is normally distributed
    (c) The residuals are normally distributed
    (d) The slope coefficient is normally distributed
    (e) The intercept coefficient is normally distributed

[^13]: d. The residuals are normally distributed (which induces a., d., and e.).  There is nothing in the technical conditions about the distribution of X (remember, X can be binary!).

---

15. Smaller variability around the regression line ($\sigma$):[^14]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn’t necessarily change the variability of $b_1$.

[^14]: b. decreases the variability of $b_1$.

---

16. Smaller variability in the explanatory variable (SD(X) = $s_X$):[^15]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn’t necessarily change the variability of $b_1$.

[^15]: a. increases the variability of $b_1$.

---

17. A smaller sample size ($n$):[^16]
    (a) increases the variability of $b_1$.
    (b) decreases the variability of $b_1$.
    (c) doesn’t necessarily change the variability of $b_1$.

[^16]: b. decreases the variability of $b_1$.

---

18. We transform our variables...[^17]
    (a) ... to find the highest $r^2$ value.
    (b) ... when the X variable is not normally distributed.
    (c) ... to make the model easier to interpret.
    (d) ... so that the technical conditions are met.

[^17]: d. so that the technical conditions are met.

---

19. What does "probability" (in calculating the p-value) mean here?[^18]
    (a) random allocation
    (b) random sample

[^18]: not sure

---
